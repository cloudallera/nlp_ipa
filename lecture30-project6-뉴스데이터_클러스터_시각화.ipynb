{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#프로젝트-:-뉴스-기사-클러스터,-시각화\" data-toc-modified-id=\"프로젝트-:-뉴스-기사-클러스터,-시각화-1\">프로젝트 : 뉴스 기사 클러스터, 시각화</a></span><ul class=\"toc-item\"><li><span><a href=\"#과제\" data-toc-modified-id=\"과제-1.1\">과제</a></span></li><li><span><a href=\"#풀이\" data-toc-modified-id=\"풀이-1.2\">풀이</a></span><ul class=\"toc-item\"><li><span><a href=\"#뉴스-기사-읽어오기,-전처리\" data-toc-modified-id=\"뉴스-기사-읽어오기,-전처리-1.2.1\">뉴스 기사 읽어오기, 전처리</a></span></li><li><span><a href=\"#EM\" data-toc-modified-id=\"EM-1.2.2\">EM</a></span></li><li><span><a href=\"#DTM,-TDM,-TWM\" data-toc-modified-id=\"DTM,-TDM,-TWM-1.2.3\">DTM, TDM, TWM</a></span></li><li><span><a href=\"#어휘-클러스터\" data-toc-modified-id=\"어휘-클러스터-1.2.4\">어휘 클러스터</a></span></li><li><span><a href=\"#시각화\" data-toc-modified-id=\"시각화-1.2.5\">시각화</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 : 뉴스 기사 클러스터, 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뉴스 데이터에 적용하기 (iter=10)\n",
    "\n",
    "\n",
    "- WordCloud 만들기(명사만) => SSE 확인\n",
    "\n",
    "\n",
    "- K=2가 아닌, < 10까지 변화 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뉴스 기사 읽어오기, 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.naver import NewsScraping\n",
    "\n",
    "from functions.info_retrieval import max_tf, raw_idf\n",
    "from functions.info_retrieval import smoothig_idf\n",
    "from functions.info_retrieval import clean_collection\n",
    "from functions.info_retrieval import extend_lexicon\n",
    "from functions.info_retrieval import inverted_index_with_tf\n",
    "from functions.info_retrieval import get_tdm_from_dtm\n",
    "from functions.info_retrieval import tdm2twm\n",
    "from functions.info_retrieval import evaluate_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "    '정치': '00',\n",
    "    '경제': '01',\n",
    "    '사회': '02',\n",
    "    '생활문화': '03',\n",
    "    '세계': '04',\n",
    "    'IT과학': '05'\n",
    "}\n",
    "\n",
    "inverse_category = {\n",
    "    '00': '정치',\n",
    "    '01': '경제',\n",
    "    '02': '사회',\n",
    "    '03': '생활문화',\n",
    "    '04': '세계',\n",
    "    '05': 'IT과학'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = NewsScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = news.get_filenames(all_folder=True)\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = []   # tuple(filename, content)들의 list\n",
    "\n",
    "for filename in filenames:\n",
    "    collection.append((filename.split(\"/\")[-1], news.get_content(filename)))\n",
    "    \n",
    "len(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_collection = clean_collection(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 개 documents 중 30 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 60 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 90 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 120 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 150 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 180 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 210 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 240 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 270 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 300 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 330 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 360 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 390 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 420 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 450 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 480 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 510 개 documents의 lexicon 추출 완료\n",
      "540 개 documents 중 540 개 documents의 lexicon 추출 완료\n",
      "540 개 documents의 lexicon 추출 완료\n"
     ]
    }
   ],
   "source": [
    "extended_collection = []\n",
    "corpus_count = len(cleaned_collection)\n",
    "\n",
    "for idx, (filename, corpus) in enumerate(cleaned_collection):\n",
    "    extended_lexicon = extend_lexicon(corpus)\n",
    "    extended_collection.append((filename, extended_lexicon))\n",
    "    if (idx + 1) % 30 == 0:\n",
    "        print(corpus_count, \"개 documents 중\", idx + 1, \"개 documents의 lexicon 추출 완료\")\n",
    "\n",
    "print(idx + 1, \"개 documents의 lexicon 추출 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity = Euclidean Distance\n",
    "from math import sqrt\n",
    "\n",
    "def euclidean(x1, x2):\n",
    "    _sum = 0.0\n",
    "    for i in range(len(x1)):\n",
    "        _sum += (x1[i] - x2[i]) ** 2\n",
    "        \n",
    "    return sqrt(_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x1, x2):\n",
    "    _sum = 0.0\n",
    "    x1_length = 0.0\n",
    "    x2_length = 0.0\n",
    "    \n",
    "    for i in range(len(x1)):\n",
    "        _sum += x1[i]*x2[i]\n",
    "    \n",
    "    return _sum / (euclidean(x1, [0 for _ in range(len(x1))]) * euclidean(x2, [0 for _ in range(len(x1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개 이상을 처리하는 expectation\n",
    "def expectation(data, clusters, k=3, cos=True):\n",
    "    distance = list()\n",
    "    metric = cosine if cos else euclidean\n",
    "    rvalue = max if cos else min\n",
    "    \n",
    "    distance = list()\n",
    "\n",
    "    for i in range(k):\n",
    "        distance.append(metric(data, clusters[i]))\n",
    "        \n",
    "    return distance.index(rvalue(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개 이상을 처리하는 maximization\n",
    "def maximization(data):\n",
    "    N = len(data)\n",
    "    _sum = list(0 for _ in range(len(data[0])))\n",
    "    \n",
    "    for _ in data:\n",
    "        for i in range(len(_)):\n",
    "            _sum[i] += _[i]\n",
    "\n",
    "    return tuple(_/N for _ in _sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(data, centroid):   # sum squared error\n",
    "    _sum = 0.0\n",
    "    \n",
    "    for _ in data:\n",
    "        _sum += euclidean(_, centroid)\n",
    "        \n",
    "    return _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Randomly K=3, N=100, Centroids=Clusters => rnk\n",
    "# iterCount = 6\n",
    "# sseList = list()\n",
    "\n",
    "# cluster = sample(data, K)    # 실행 할 때 마다 초기화 필요\n",
    "\n",
    "# for _ in range(iterCount):\n",
    "#     rnk = list(list(0 for _ in range(K)) for _ in range(N))\n",
    "\n",
    "#     for i in range(N):\n",
    "#         k = expectation(data[i], cluster)\n",
    "#         rnk[i][k] = 1\n",
    "\n",
    "#     _sum = 0.0\n",
    "#     oldCluster = list()\n",
    "    \n",
    "#     for i in range(K):\n",
    "#         dataset = [data[j] for j in range(N) if rnk[j][i]]\n",
    "#         _sum += sse(dataset, cluster[i])\n",
    "#         oldCluster.append(cluster[i])\n",
    "#         cluster[i] = maximization(dataset)\n",
    "        \n",
    "#     print(\"Iteration: {0} / SSE:{1}\".format(_+1, _sum))\n",
    "\n",
    "#     for i in range(N):\n",
    "#         plt.scatter(data[i][0], data[i][1], alpha=0.3, color=colormap[rnk[i].index(max(rnk[i]))])\n",
    "\n",
    "#     for i in range(K):\n",
    "#         plt.plot((oldCluster[i][0], cluster[i][0]), (oldCluster[i][1], cluster[i][1]), \"r-\")\n",
    "#         plt.scatter(cluster[i][0], cluster[i][1], color=colormap[i], linewidths=4, edgecolors=colormap[-1])\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "#     sseList.append(_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Randomly K=3, N=100, Centroids=Clusters => rnk\n",
    "# iterCount = 6\n",
    "# sseList = list()\n",
    "\n",
    "# # https://matplotlib.org/examples/color/named_colors.html\n",
    "# colormap = (\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"gray\", \"gold\", \"navy\", \"ivory\", \"violet\", \"pink\", \"brown\", \"k\")\n",
    "\n",
    "# for k in range(2, 10):\n",
    "#     cluster = sample(data, k)    # 실행 할 때 마다 초기화 필요\n",
    "\n",
    "#     for _ in range(iterCount):\n",
    "#         rnk = list(list(0 for _ in range(k)) for _ in range(N))\n",
    "\n",
    "#         for i in range(N):\n",
    "#             j = expectation(data[i], cluster, k=k, cos=True)\n",
    "#             rnk[i][j] = 1\n",
    "\n",
    "#         _sum = 0.0\n",
    "#         oldCluster = list()\n",
    "\n",
    "#         for i in range(k):\n",
    "#             dataset = [data[j] for j in range(N) if rnk[j][i]]\n",
    "#             _sum += sse(dataset, cluster[i])\n",
    "#             oldCluster.append(cluster[i])\n",
    "#             cluster[i] = maximization(dataset)\n",
    "\n",
    "#     sseList.append(_sum)\n",
    "        \n",
    "#     print(\"Iteration: {0} / SSE:{1}\".format(k, _sum))\n",
    "\n",
    "#     for i in range(N):\n",
    "#         plt.scatter(data[i][0], data[i][1], alpha=0.3, color=colormap[rnk[i].index(max(rnk[i]))])\n",
    "\n",
    "#     for i in range(k):\n",
    "#         plt.plot((oldCluster[i][0], cluster[i][0]), (oldCluster[i][1], cluster[i][1]), \"r-\")\n",
    "#         plt.scatter(cluster[i][0], cluster[i][1], color=colormap[i], linewidths=4, edgecolors=colormap[-1])\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# plt.plot(range(2, 10), sseList, \"r-\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTM, TDM, TWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92098, 379911, 540, 540)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_lexicon, global_posting, global_document, dtm = inverted_index_with_tf(extended_collection)\n",
    "len(global_lexicon), len(global_posting), len(global_document), len(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = get_tdm_from_dtm(dtm)\n",
    "twm, dtw = tdm2twm(tdm, global_document)\n",
    "global_lexicon_idf, global_document_weight = evaluate_idf(global_lexicon, global_posting, global_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 문서를 길이가 동일한 다차원 벡터로 만들기 (크기는 전체 Vocabulary 크기)\n",
    "N = len(dtm)   # 전체 document 수\n",
    "V = len(tdm)   # term의 전체 차원 수(vocabulary 수)\n",
    "\n",
    "cluster = list()\n",
    "docVector = list(list(0.0 for _ in range(V)) for _ in range(N))\n",
    "\n",
    "for i in range(N):\n",
    "    for j, t in enumerate(tdm.keys()):\n",
    "        docVector[i][j] = twm[t][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(docVector)\n",
    "# print(docVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어휘 클러스터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 92098)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "K = 3\n",
    "\n",
    "cluster = sample(docVector, K)\n",
    "len(cluster), len(docVector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-ef7b22d234cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocVector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrnk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaximization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msseList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-25515585b0e2>\u001b[0m in \u001b[0;36mmaximization\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmaximization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# 여러개의 문서를 처리할 수 있도록 수정 필요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sseList = list()\n",
    "\n",
    "for _ in range(10):\n",
    "    rnk = list(list(0.0 for _ in range(K)) for _ in range(N))\n",
    "\n",
    "    for i, d in enumerate(docVector):\n",
    "        c = expectation(d, clusters=cluster, k=K, cos=False)\n",
    "        rnk[i][c] = 1\n",
    "        \n",
    "    _sum = 0.0\n",
    "    \n",
    "    for j, c in enumerate(cluster):\n",
    "        dataset = [docVector[i] for i in range(N) if rnk[i][j]]\n",
    "        _sum += sse(dataset, c)\n",
    "        cluster[j] = maximization(dataset)\n",
    "        \n",
    "    sseList.append(_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sseList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), sseList, \"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterDataset = list()\n",
    "\n",
    "for j, c in enumerate(cluster):\n",
    "    dataset = [i for i in range(N) if rnk[i][j]]\n",
    "    clusterDataset.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = list(twm.keys())\n",
    "topic = list()\n",
    "\n",
    "for c in cluster:\n",
    "    wordList = defaultdict(float)\n",
    "    \n",
    "    for i, w in enumerate(c):\n",
    "        wordList[voca[i]] = w\n",
    "        \n",
    "    topic.append(dict(sorted(wordList.items(), key=lambda x: x[1], reverse=True)[:5]))    # {word, value}\n",
    "    print(sorted(wordList, key=wordList.get, reverse=True)[:5])      # [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import  WordCloud\n",
    "fontPath = \"Libray/Fonts/AppleGothic.ttf\"\n",
    "wc = WordCloud(font_path=fontPath, background_color=\"white\")\n",
    "wc.generate_from_frequencies(topic[0])\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
