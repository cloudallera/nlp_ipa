{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Normalization\" data-toc-modified-id=\"Text-Normalization-1\">Text Normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#불필요한-구두점-지우기\" data-toc-modified-id=\"불필요한-구두점-지우기-1.1\">불필요한 구두점 지우기</a></span></li><li><span><a href=\"#stopwords\" data-toc-modified-id=\"stopwords-1.2\">stopwords</a></span></li><li><span><a href=\"#한글에-불용어,-stropwords-적용\" data-toc-modified-id=\"한글에-불용어,-stropwords-적용-1.3\">한글에 불용어, stropwords 적용</a></span></li><li><span><a href=\"#불용어-사전-관련-추가-고려사항\" data-toc-modified-id=\"불용어-사전-관련-추가-고려사항-1.4\">불용어 사전 관련 추가 고려사항</a></span><ul class=\"toc-item\"><li><span><a href=\"#짧은-어휘-처리\" data-toc-modified-id=\"짧은-어휘-처리-1.4.1\">짧은 어휘 처리</a></span></li><li><span><a href=\"#빈도수-처리\" data-toc-modified-id=\"빈도수-처리-1.4.2\">빈도수 처리</a></span></li></ul></li></ul></li><li><span><a href=\"#비속어-사전\" data-toc-modified-id=\"비속어-사전-2\">비속어 사전</a></span><ul class=\"toc-item\"><li><span><a href=\"#ngramEojeol()\" data-toc-modified-id=\"ngramEojeol()-2.1\">ngramEojeol()</a></span></li><li><span><a href=\"#ngramUmjeol()\" data-toc-modified-id=\"ngramUmjeol()-2.2\">ngramUmjeol()</a></span></li><li><span><a href=\"#findNgram()\" data-toc-modified-id=\"findNgram()-2.3\">findNgram()</a></span></li><li><span><a href=\"#splitTerms()\" data-toc-modified-id=\"splitTerms()-2.4\">splitTerms()</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Segmenting/tokenizing words\n",
    "- Normalizing word formats\n",
    "- Segmenting sentences\n",
    "\n",
    "Normalization Issue(영어)\n",
    "\n",
    "![normalizationissue](./images/normalization_issue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불필요한 구두점 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[!\"\\#\\$%\\&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~]',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more somthing.\"\n",
    "pattern = re.compile(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'d\", 'like', 'to', 'learn', 'more', 'somthing', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize:  ['I', \"'d\", 'like', 'to', 'learn', 'more', 'somthing', '.']\n",
      "normalization:  ['I', 'd', 'like', 'to', 'learn', 'more', 'somthing']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "print(\"word_tokenize: \", word_tokenize(sentence))\n",
    "\n",
    "for term in word_tokenize(sentence):\n",
    "    # re.sub(pattern, \"\", term)\n",
    "    new = pattern.sub(\"\", term)\n",
    "    if new:\n",
    "        tokens.append(new)\n",
    "\n",
    "print(\"normalization: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize:  ['파이썬', ',', '자연어처리', ',', '그리고', '어쩌고']\n",
      "normalization:  ['파이썬', '자연어처리', '그리고', '어쩌고']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"파이썬, 자연어처리, 그리고 어쩌고\"\n",
    "pattern = re.compile(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "\n",
    "tokens = []\n",
    "\n",
    "print(\"word_tokenize: \", word_tokenize(sentence))\n",
    "\n",
    "for term in word_tokenize(sentence):\n",
    "    # re.sub(pattern, \"\", term)\n",
    "    new = pattern.sub(\"\", term)\n",
    "    if new:\n",
    "        tokens.append(new)\n",
    "\n",
    "print(\"normalization: \", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i\\nme\\nmy\\nmyself\\nwe\\nour\\nours\\nourselves\\nyou\\nyou're\\nyou've\\nyou'll\\nyou'd\\nyour\\nyours\\nyourself\\nyourselves\\nhe\\nhim\\nhis\\nhimself\\nshe\\nshe's\\nher\\nhers\\nherself\\nit\\nit's\\nits\\nitself\\nthey\\nthem\\ntheir\\ntheirs\\nthemselves\\nwhat\\nwhich\\nwho\\nwhom\\nthis\\nthat\\nthat'll\\nthese\\nthose\\nam\\nis\\nare\\nwas\\nwere\\nbe\\nbeen\\nbeing\\nhave\\nhas\\nhad\\nhaving\\ndo\\ndoes\\ndid\\ndoing\\na\\nan\\nthe\\nand\\nbut\\nif\\nor\\nbecause\\nas\\nuntil\\nwhile\\nof\\nat\\nby\\nfor\\nwith\\nabout\\nagainst\\nbetween\\ninto\\nthrough\\nduring\\nbefore\\nafter\\nabove\\nbelow\\nto\\nfrom\\nup\\ndown\\nin\\nout\\non\\noff\\nover\\nunder\\nagain\\nfurther\\nthen\\nonce\\nhere\\nthere\\nwhen\\nwhere\\nwhy\\nhow\\nall\\nany\\nboth\\neach\\nfew\\nmore\\nmost\\nother\\nsome\\nsuch\\nno\\nnor\\nnot\\nonly\\nown\\nsame\\nso\\nthan\\ntoo\\nvery\\ns\\nt\\ncan\\nwill\\njust\\ndon\\ndon't\\nshould\\nshould've\\nnow\\nd\\nll\\nm\\no\\nre\\nve\\ny\\nain\\naren\\naren't\\ncouldn\\ncouldn't\\ndidn\\ndidn't\\ndoesn\\ndoesn't\\nhadn\\nhadn't\\nhasn\\nhasn't\\nhaven\\nhaven't\\nisn\\nisn't\\nma\\nmightn\\nmightn't\\nmustn\\nmustn't\\nneedn\\nneedn't\\nshan\\nshan't\\nshouldn\\nshouldn't\\nwasn\\nwasn't\\nweren\\nweren't\\nwon\\nwon't\\nwouldn\\nwouldn't\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.open(\"english\").read()\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  ['Beautiful', 'is', 'better', 'than', 'ugly.']\n",
      "word_tokenize:  ['Beautiful', 'is', 'better', 'than', 'ugly', '.']\n",
      "clean_stopwords:  ['Beautiful', 'better', 'ugly']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Beautiful is better than ugly.\"\n",
    "print(\"split: \", sentence.split())\n",
    "print(\"word_tokenize: \", word_tokenize(sentence))\n",
    "\n",
    "pattern = re.compile(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "sentence = pattern.sub(\"\", sentence)\n",
    "\n",
    "terms = word_tokenize(sentence)\n",
    "clearnTerms = []\n",
    "\n",
    "for term in terms:\n",
    "    if term not in stop:\n",
    "        clearnTerms.append(term)\n",
    "\n",
    "print(\"clean_stopwords: \", clearnTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글에 불용어, stropwords 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  ['어머님', '은', '자장면', '이,', '싫다', '고', '하셨', '어.']\n",
      "word_tokenize:  ['어머님', '은', '자장면', '이', ',', '싫다', '고', '하셨', '어', '.']\n",
      "clean_stopwords:  ['어머님', '자장면', '싫다', '하셨', '어']\n"
     ]
    }
   ],
   "source": [
    "# punctuation 제거 -> word_tokenize -> stopwords 제거\n",
    "sentence = \"어머님 은 자장면 이, 싫다 고 하셨 어.\"\n",
    "ko_stop = [\"은\", \"는\", \"이\", \"가\", \"고\", \"께\", \"을\", \"를\", \"께서\", \"게\", \"에게\"]   # stopwords 정의\n",
    "print(\"split: \", sentence.split())\n",
    "print(\"word_tokenize: \", word_tokenize(sentence))\n",
    "\n",
    "pattern = re.compile(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "sentence = pattern.sub(\"\", sentence)\n",
    "\n",
    "terms = word_tokenize(sentence)\n",
    "clearnTerms = []\n",
    "\n",
    "for term in terms:\n",
    "    if term not in ko_stop:\n",
    "        clearnTerms.append(term)\n",
    "\n",
    "print(\"clean_stopwords: \", clearnTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 사전 관련 추가 고려사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영어의 경우 대소문자 : 소문자로 바꿔서 처리  => 한글 문장 내에도 영어가 있을 수 있음\n",
    "    - 축약어 : 대소문자의 의미가 있음 ==> 도메인에 따라 다르게 정의할 수도 있음\n",
    "- 짧은 어휘 : 너무 짧은 글자는 제거 (또는 너무 긴 단어도 제거)\n",
    "- 저빈도 어휘 : 너무 드문 단어 제거\n",
    "- 정규 표현식을 사용\n",
    "- 구어체에서는 신조어를 중요하게 생각해야 할 수도 있음(추가 해야 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **한글 불용어와 비속어**\n",
    "\n",
    "![불용어](./images/한글-불용어_비속어.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 짧은 어휘 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd\n",
      "like\n",
      "learn\n",
      "more\n",
      "somthing.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more somthing.\"\n",
    "\n",
    "threshold = 2 \n",
    "\n",
    "for token in sentence.split():\n",
    "    if len(token) > threshold:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "learn\n",
      "more\n",
      "somthing\n"
     ]
    }
   ],
   "source": [
    "for token in word_tokenize(sentence):\n",
    "    if len(token) > threshold:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to learn more somthing.\n",
      " '  like   learn more somthing.\n"
     ]
    }
   ],
   "source": [
    "# 정규식 표현\n",
    "# \\W\\b.+\\b\\W : \\W는 word가 아닌 것, \\w는 word인것, \\b는 경계(boundary)\n",
    "# {1, 2} : 1글자~2글자\n",
    "# {2, } : 2글자 이상\n",
    "# regular expression cheetsheet 참고\n",
    "\n",
    "pattern = re.compile(r\"\\b\\w{1,%d}\\b\" % threshold)\n",
    "print(sentence)\n",
    "print(pattern.sub(\" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빈도수 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped : I => 3\n",
      ">> 'd\n",
      ">> like\n",
      ">> to\n",
      ">> learn\n",
      ">> more\n",
      "Skipped : somthing => 1\n",
      ">> .\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "\n",
    "sentence = \"I'd like to learn more somthing. I'd like to learn more I.\"\n",
    "\n",
    "tokens = Text(word_tokenize(sentence))\n",
    "for k in tokens.vocab():\n",
    "    if tokens.count(k) > 2 or tokens.count(k) < 2:\n",
    "        print(\"Skipped : {0} => {1}\".format(k, tokens.count(k)))\n",
    "    else:\n",
    "        print(\">>\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비속어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** 짜증나 씨발짜증나'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = [\"시발\", \"씨발\"]\n",
    "sentence = \"시발 짜증나 씨발짜증나\"\n",
    "result = []\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append(\"*\" * len(term))\n",
    "    else:\n",
    "        result.append(term)\n",
    "\n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngramEojeol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['동해물과 백두산이', '백두산이 마르고', '마르고 닳도록']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어절 단위의 ngram (n=2)\n",
    "def ngramEojeol(sentence, n=2):\n",
    "    '''\n",
    "    입력: 단어1, 단어2, 단어3, 단어4 : 4\n",
    "    출력(2) : 단어12, 단어23, 단어34 : 3 - n + 1\n",
    "    출력(3) : 단어123, 단어234         : 2 - n + 1\n",
    "    '''\n",
    "    tokens = sentence.split()\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram.append(\" \".join(tokens[i:i+n]))    # 공백을 추가해줘야 단어 사이가 구분됨\n",
    "        \n",
    "    return ngram\n",
    "\n",
    "ngramEojeol(\"동해물과 백두산이 마르고 닳도록\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngramUmjeol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국민', '민의', '의국', '국민', '민을', '을국', '국민', '민에']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음절 단위의 ngram (n=2)\n",
    "def ngramUmjeol(term, n=2):\n",
    "    '''\n",
    "    입력: 음절1, 음절2, 음절3, 음절4 : 4\n",
    "    출력(2) : 음절12, 음절23, 음절34 : 3 - n + 1\n",
    "    출력(3) : 음절123, 음절234         : 2 - n + 1\n",
    "    '''\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        ngram.append(\"\".join(term[i:i+n]))    # 공백 없이 붙여서 단어를 만듦\n",
    "        \n",
    "    return ngram\n",
    "\n",
    "ngramUmjeol(\"국민의국민을국민에\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split 만으로는 음절을 정확히 처리하지 못했음(띄어쓰기 안하는 경우...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** 짜증나 ***** **'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = [\"시발\", \"씨발\"]\n",
    "sentence = \"시발 짜증나 씨발짜증나 씨발\"\n",
    "result = []\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append(\"*\" * len(term))\n",
    "    else:\n",
    "        flag = False\n",
    "        for token in ngramUmjeol(term):\n",
    "            if token in stop:\n",
    "                flag = True\n",
    "        \n",
    "        if flag == True:\n",
    "            result.append(\"*\" * len(term))\n",
    "        else:\n",
    "            result.append(term)\n",
    "\n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword 중간에 문자가 포함된 경우는 걸러내지 못함\n",
    "stop = [\"시발\", \"씨발\"]\n",
    "sentence = \"시~발 짜증나 씨발짜증나 씨1발\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시~발 짜증나 씨발짜증나 씨1발\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"\\B\\[0-9]+\\B\")\n",
    "sentence = pattern.sub(\"\", sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** 짜증나 ***** ** 시~발 짜증나 ***** 씨1발'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "        result.append(\"*\" * len(term))\n",
    "    else:\n",
    "        flag = False\n",
    "        for token in ngramUmjeol(term):\n",
    "            if token in stop:\n",
    "                flag = True\n",
    "        \n",
    "        if flag:\n",
    "            result.append(\"*\" * len(term))\n",
    "        else:\n",
    "            result.append(term)\n",
    "\n",
    "\" \".join(result)\n",
    "\n",
    "# ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### findNgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def findNgram(tokens, n=2):\n",
    "    result = defaultdict(int)    # {}  : dictionary 객체 생성\n",
    "    \n",
    "    for k, v in tokens.items():\n",
    "        term = k.split()\n",
    "        \n",
    "        for i in range(len(term) - n + 1):\n",
    "            ngram = (term[i], term[i+1])\n",
    "            # (l, o), (o, w), (w, </w>)\n",
    "            \n",
    "            if ngram in result.keys():\n",
    "                result[ngram] += v\n",
    "            else:\n",
    "                result[ngram] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitTerms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l o w e r </w> _ l o w e s t </w>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitTerms(term):\n",
    "    # t = term.replace(\" \", \"_\")  # 공백 치환\n",
    "    termLists = term.split()\n",
    "    result = []\n",
    "\n",
    "    for termList in termLists:\n",
    "        result.append(\" \".join(list(termList)+['</w>']))\n",
    "    return \" _ \".join(result)\n",
    "\n",
    "splitTerms(\"lower lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    splitTerms(\"시발\"):10,\n",
    "    splitTerms(\"시1발\"):2,\n",
    "    splitTerms(\"시!발\"):2,\n",
    "    splitTerms(\"시123발\"):2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('시', '발'): 10,\n",
       "             ('발', '</w>'): 16,\n",
       "             ('시', '1'): 4,\n",
       "             ('1', '발'): 2,\n",
       "             ('시', '!'): 2,\n",
       "             ('!', '발'): 2,\n",
       "             ('1', '2'): 2,\n",
       "             ('2', '3'): 2,\n",
       "             ('3', '발'): 2})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNgram(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'** ******** ****'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    pattern = findNgram(data)\n",
    "    maxKey = max(pattern, key=pattern.get)\n",
    "\n",
    "pattern = findNgram(data)\n",
    "maxKey = max(pattern, key=pattern.get)\n",
    "\n",
    "sentence = \"시발 시112312발 시!a발\"\n",
    "repattern = re.compile(maxKey[0] + \".*\"  + maxKey[1])\n",
    "\n",
    "result = []\n",
    "for token in sentence.split():\n",
    "    if repattern.search(\"<w>{0}</w>\".format(token)):\n",
    "        result.append(\"*\" * len(token))\n",
    "    else:\n",
    "        result.append(token)\n",
    "\n",
    "\" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
