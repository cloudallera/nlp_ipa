{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word-Embedding\" data-toc-modified-id=\"Word-Embedding-1\">Word Embedding</a></span><ul class=\"toc-item\"><li><span><a href=\"#introduction\" data-toc-modified-id=\"introduction-1.1\">introduction</a></span></li><li><span><a href=\"#국소표현-vs.-분산표현\" data-toc-modified-id=\"국소표현-vs.-분산표현-1.2\">국소표현 vs. 분산표현</a></span></li><li><span><a href=\"#분포가설\" data-toc-modified-id=\"분포가설-1.3\">분포가설</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-based-vs-Predictive-methods\" data-toc-modified-id=\"Count-based-vs-Predictive-methods-1.3.1\">Count-based vs Predictive methods</a></span></li><li><span><a href=\"#문맥(context)의-정의\" data-toc-modified-id=\"문맥(context)의-정의-1.3.2\">문맥(context)의 정의</a></span></li></ul></li><li><span><a href=\"#word_context_matrix\" data-toc-modified-id=\"word_context_matrix-1.4\">word_context_matrix</a></span></li><li><span><a href=\"#특이치분해(SVD)\" data-toc-modified-id=\"특이치분해(SVD)-1.5\">특이치분해(SVD)</a></span></li><li><span><a href=\"#Exam1\" data-toc-modified-id=\"Exam1-1.6\">Exam1</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVD\" data-toc-modified-id=\"SVD-1.6.1\">SVD</a></span></li><li><span><a href=\"#distance\" data-toc-modified-id=\"distance-1.6.2\">distance</a></span></li><li><span><a href=\"#similarity\" data-toc-modified-id=\"similarity-1.6.3\">similarity</a></span></li></ul></li><li><span><a href=\"#Exam2\" data-toc-modified-id=\"Exam2-1.7\">Exam2</a></span><ul class=\"toc-item\"><li><span><a href=\"#vocabulary,-matrix-만들기\" data-toc-modified-id=\"vocabulary,-matrix-만들기-1.7.1\">vocabulary, matrix 만들기</a></span></li><li><span><a href=\"#SVD\" data-toc-modified-id=\"SVD-1.7.2\">SVD</a></span></li><li><span><a href=\"#distance\" data-toc-modified-id=\"distance-1.7.3\">distance</a></span></li><li><span><a href=\"#그래프\" data-toc-modified-id=\"그래프-1.7.4\">그래프</a></span></li></ul></li><li><span><a href=\"#언어모델\" data-toc-modified-id=\"언어모델-1.8\">언어모델</a></span><ul class=\"toc-item\"><li><span><a href=\"#n-gram\" data-toc-modified-id=\"n-gram-1.8.1\">n-gram</a></span></li><li><span><a href=\"#분산표현-:-임베드행렬(embedding-matrix),-word2vec\" data-toc-modified-id=\"분산표현-:-임베드행렬(embedding-matrix),-word2vec-1.8.2\">분산표현 : 임베드행렬(embedding matrix), word2vec</a></span></li></ul></li><li><span><a href=\"#CBOW-Neural-Network\" data-toc-modified-id=\"CBOW-Neural-Network-1.9\">CBOW Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#one-hot-vector로-변환\" data-toc-modified-id=\"one-hot-vector로-변환-1.9.1\">one-hot vector로 변환</a></span></li><li><span><a href=\"#CBOW\" data-toc-modified-id=\"CBOW-1.9.2\">CBOW</a></span></li><li><span><a href=\"#tensorflow-NN\" data-toc-modified-id=\"tensorflow-NN-1.9.3\">tensorflow NN</a></span></li><li><span><a href=\"#keras-NN\" data-toc-modified-id=\"keras-NN-1.9.4\">keras NN</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 : https://ratsgo.github.io/natural%20language%20processing/2017/08/16/deepNLP/#a-단어-임베딩\n",
    "- 참고 : http://fbsight.com/t/nlp-natural-language-processing/2110 (머신러닝의 자연어 처리 기술, 김홍배)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분산표상으로 표현된 벡터(Distributional vectors) 또는 **단어 임베딩(Word Embedding)** 은 근본적으로는 **distributional hypothesis**를 전제로 한다. **이 가정은 비슷한 의미를 지닌 단어는 비슷한 문맥에 등장하는 경향이 있을 것이라는 내용이 핵심이다**. 따라서 이 벡터들은 이웃한 단어의 특징을 잡아내고자 한다. 분산표상 벡터의 주된 장점은 이 벡터들이 단어 간 유사성을 내포하고 있다는 점이다. 코사인 유사도 같은 지표를 사용함으로써 벡터간 유사성을 측정할 수 있다.\n",
    "\n",
    "- D차원 벡터로 표현된 단어 벡터 : V를 전체 단어 수라고 할 때, D는 V보다 훨씬 작다.\n",
    "![word_vector.png](./images/word_vector.png)\n",
    "\n",
    "**단어 임베딩은 딥러닝 모델의 첫번째 데이터 처리 계층에 자주 사용된다.** 일반적으로, 단어 임베딩은 레이블이 없는 방대한 말뭉치에서 ‘보조적인 목적함수(예컨대 이웃단어로 중심단어를 예측한다, 각 단어벡터는 일반적인 문법적, 의미적 정보를 내포한다)’를 최적화함으로써 사전 학습된다(Mikolov et al., 2013b, a). 단어 임베딩은 문맥 유사도를 잡아내는 데 효율적이라는 사실이 증명되었다. 또한 임베딩 벡터의 차원이 작은 덕분에 계산이 빠르고 효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 기반의 NLP 모델은 언제나 이러한 임베딩 벡터를 활용해 단어, 구, 문장을 표현한다. 이는 전통적인 단어 빈도수 기반의 모델과 딥러닝 기반의 모델 간의 가장 큰 차이점이다.\n",
    "\n",
    "단어 임베딩은 NLP 문제의 광범위한 범위에서 최첨단(state-of-the-art) 결과를 이끌어냈다(Weston et al., 2011; Socher et al., 2011a; Turney and Pantel, 2010). 예컨대 Glorot et al.(2011)은 도메인 특성에 맞는 감성 분류를 위한 stacked denoisiong autoencoder 모델에 단어 임베딩을 사용했다. Hermann and Blunsom(2013)은 단어 임베딩을 활용해 문장 합성을 학습하기 위한 combinatory categorial autoencoders를 제안했다. 이러한 광범위한 사용은 NLP 태스크를 수행하는 딥러닝 모델에서의 임베딩 효과와 중요성을 보여준다.\n",
    "\n",
    "워드 임베딩은 주로 **문맥(context)** 을 통해 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 국소표현 vs. 분산표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![국소표현](./images/국소표현.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![분산표현1](./images/분산표현1.png)\n",
    "\n",
    "![분산표현2](./images/분산표현2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분포가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**“You shall know a word by the company it keeps”**, J. R. Firth\n",
    "\n",
    "비슷한 문맥을 가진 단어는 비슷한 의미를 갖는다. (현대의 통계적 자연어 처리에서 획기적인 발상)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count-based vs Predictive methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분포가설에 기반한 방법은 크게 2종류로 나눈다.\n",
    "\n",
    "-  count-based methods\n",
    "    - 예: **SVD (LSA)**、HAL、etc.\n",
    "    - 단어, 문맥 출현횟수를 세는 방법\n",
    "    \n",
    "    \n",
    "- predictive methods\n",
    "    - 예:NPLM、**word2vec**、etc.\n",
    "    - 단어에서 문맥 또는 문맥에서 단어를 예측하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문맥(context)의 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신 이외의 OO중에 나타나는 단어\n",
    "\n",
    "- 문장\n",
    "- 단락\n",
    "- 문서\n",
    "\n",
    "크기 2k+1의 단어열에 대해 기준단어(April)주변의 단어가 문맥\n",
    "\n",
    "- context window\n",
    "![context_windows.png](./images/context_windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word_context_matrix1](./images/word_context_matrix1.png)\n",
    "\n",
    "![word_context_matrix2](./images/word_context_matrix2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특이치분해(SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**고차원 벡터의「가장 중요한 정보」를 유지하면서 저차원・조밀한 벡터로 압축하고 싶다.** (e.g. 수십만 차원 → 수백 차원)\n",
    "\n",
    "\n",
    "- 새로운 단어가 추가될 때마다, matrix를 다시 생성해야 라며, 데이터가 커지면 계산량이 너무 많아지는 문제가 있음\n",
    "\n",
    "\n",
    "- SVD 계산량은 n×m행렬의 경우, $O(mn^2), (n < m)$ → 즉 어휘수에 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVD](./images/SVD2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"I\", \"like\", \"enjoy\", \"deep\", \"learning\", \"NLP\", \"flying\", \".\"]\n",
    "\n",
    "X = np.array([\n",
    "    [0,2,1,0,0,0,0,0],\n",
    "    [2,0,0,1,0,1,0,0],\n",
    "    [1,0,0,0,0,0,1,0],\n",
    "    [0,1,0,0,1,0,0,0],\n",
    "    [0,0,0,1,0,0,0,1],\n",
    "    [0,1,0,0,0,0,0,1],\n",
    "    [0,0,1,0,0,0,0,1],\n",
    "    [0,0,0,0,1,1,1,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>like</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>deep</th>\n",
       "      <th>learning</th>\n",
       "      <th>NLP</th>\n",
       "      <th>flying</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I  like  enjoy  deep  learning  NLP  flying  .\n",
       "I         0     2      1     0         0    0       0  0\n",
       "like      2     0      0     1         0    1       0  0\n",
       "enjoy     1     0      0     0         0    0       1  0\n",
       "deep      0     1      0     0         1    0       0  0\n",
       "learning  0     0      0     1         0    0       0  1\n",
       "NLP       0     1      0     0         0    0       0  1\n",
       "flying    0     0      1     0         0    0       0  1\n",
       ".         0     0      0     0         1    1       1  0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X, columns=words, index=words)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, V = np.linalg.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.75726275, 0.        ],\n",
       "       [0.        , 2.678248  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = min(m, n)\n",
    "# 2차원에 분산 표현을 하고 싶다. => Sigma를 2차원으로 만든다. => S[:2]\n",
    "_Sigma = np.diag(Sigma[:2])\n",
    "_Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "US = U[:, :2].dot(_Sigma)    # U를 m x 2로 만든다.\n",
    "SV = _Sigma.dot(V[:2])    # V를 2 x n으로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44515015, -1.53425886],\n",
       "       [-1.63902195,  1.68761941],\n",
       "       [-0.70661477,  0.73388691],\n",
       "       [-0.78757738, -0.66397017],\n",
       "       [-0.53253583,  0.09065737],\n",
       "       [-0.8413365 , -0.78737543],\n",
       "       [-0.50317243, -0.4312723 ],\n",
       "       [-0.68076383,  0.42116725]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44515015, -1.63902195, -0.70661477, -0.78757738, -0.53253583,\n",
       "        -0.8413365 , -0.50317243, -0.68076383],\n",
       "       [ 1.53425886, -1.68761941, -0.73388691,  0.66397017, -0.09065737,\n",
       "         0.78737543,  0.4312723 , -0.42116725]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I [-1.44515015 -1.53425886]\n",
      "like [-1.63902195  1.68761941]\n",
      "enjoy [-0.70661477  0.73388691]\n",
      "deep [-0.78757738 -0.66397017]\n",
      "learning [-0.53253583  0.09065737]\n",
      "NLP [-0.8413365  -0.78737543]\n",
      "flying [-0.50317243 -0.4312723 ]\n",
      ". [-0.68076383  0.42116725]\n"
     ]
    }
   ],
   "source": [
    "for us, w in zip(US, words):\n",
    "    print(w, us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8413365 , -0.78737543])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = US[words.index(\"NLP\")]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96043   , 2.60036565, 1.52721611, 0.13460646, 0.93075208,\n",
       "       0.        , 0.4910849 , 1.21916323])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.linalg.norm(US - np.repeat(query, len(words)).reshape(2, -1).T, axis=1)    # euclidian distance\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.9604300020857386\n",
      "like 2.600365650423297\n",
      "enjoy 1.5272161083915696\n",
      "deep 0.1346064624142894\n",
      "learning 0.9307520814224423\n",
      "NLP 0.0\n",
      "flying 0.4910848961381424\n",
      ". 1.2191632307652123\n"
     ]
    }
   ],
   "source": [
    "for dist, w in zip(result, words):\n",
    "    print(w, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1523051515781995"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qNorm = np.linalg.norm(query)\n",
    "qNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.10770235, 2.35254165, 1.01877104, 1.03011384, 0.54019735,\n",
       "       1.15230515, 0.66270528, 0.80051312])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mNorm = np.linalg.norm(US, axis=1)\n",
    "mNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0.9980150045904097\n",
      "like 0.018510435474395726\n",
      "enjoy 0.01418841149085003\n",
      "deep 0.9986570480042438\n",
      "learning 0.6051040644311387\n",
      "NLP 1.0000000000000002\n",
      "flying 0.9990464464925266\n",
      ". 0.26141095433016925\n"
     ]
    }
   ],
   "source": [
    "result = US.dot(query) / (qNorm * mNorm)    # cosine similarity\n",
    "\n",
    "for dist, w in zip(result, words):\n",
    "    print(w, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"king is a strong man\",\n",
    "    \"queen is a wise woman\",\n",
    "    \"boy is a young man\",\n",
    "    \"girl is a young woman\",\n",
    "    \"prince is a young king\",\n",
    "    \"princess is a young queen\",\n",
    "    \"man is strong\",\n",
    "    \"woman is pretty\",\n",
    "    \"prince is a boy will be king\",\n",
    "    \"princess is a girl will be queen\"\n",
    "]\n",
    "\n",
    "stopwords = [\"is\", \"a\", \"will\", \"be\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary, matrix 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list()\n",
    "_documents = list()\n",
    "\n",
    "for document in documents:\n",
    "    termList = list()\n",
    "    \n",
    "    for term in document.lower().split():\n",
    "        if term not in stopwords:\n",
    "            termList.append(term)\n",
    "            vocabulary.append(term)\n",
    "            \n",
    "    _documents.append(termList)\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young',\n",
       " 'strong',\n",
       " 'boy',\n",
       " 'girl',\n",
       " 'pretty',\n",
       " 'princess',\n",
       " 'wise',\n",
       " 'king',\n",
       " 'prince',\n",
       " 'man',\n",
       " 'queen',\n",
       " 'woman']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'strong', 'man'],\n",
       " ['queen', 'wise', 'woman'],\n",
       " ['boy', 'young', 'man'],\n",
       " ['girl', 'young', 'woman'],\n",
       " ['prince', 'young', 'king'],\n",
       " ['princess', 'young', 'queen'],\n",
       " ['man', 'strong'],\n",
       " ['woman', 'pretty'],\n",
       " ['prince', 'boy', 'king'],\n",
       " ['princess', 'girl', 'queen']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "X = np.zeros((V, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "\n",
    "for document in _documents:\n",
    "    for v in range(len(document) - K):\n",
    "        i = vocabulary.index(document[v])\n",
    "        j = vocabulary.index(document[v+1])\n",
    "        \n",
    "        X[i][j] += 1    # 루프를 vocbulary 수 만큼 돌지 않고, 행과 열을 대각선 방향으로 동시에 만듦\n",
    "        X[j][i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>young</th>\n",
       "      <th>strong</th>\n",
       "      <th>boy</th>\n",
       "      <th>girl</th>\n",
       "      <th>pretty</th>\n",
       "      <th>princess</th>\n",
       "      <th>wise</th>\n",
       "      <th>king</th>\n",
       "      <th>prince</th>\n",
       "      <th>man</th>\n",
       "      <th>queen</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strong</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretty</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princess</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wise</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prince</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          young  strong  boy  girl  pretty  princess  wise  king  prince  man  \\\n",
       "young       0.0     0.0  1.0   1.0     0.0       1.0   0.0   1.0     1.0  1.0   \n",
       "strong      0.0     0.0  0.0   0.0     0.0       0.0   0.0   1.0     0.0  2.0   \n",
       "boy         1.0     0.0  0.0   0.0     0.0       0.0   0.0   1.0     1.0  0.0   \n",
       "girl        1.0     0.0  0.0   0.0     0.0       1.0   0.0   0.0     0.0  0.0   \n",
       "pretty      0.0     0.0  0.0   0.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "princess    1.0     0.0  0.0   1.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "wise        0.0     0.0  0.0   0.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "king        1.0     1.0  1.0   0.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "prince      1.0     0.0  1.0   0.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "man         1.0     2.0  0.0   0.0     0.0       0.0   0.0   0.0     0.0  0.0   \n",
       "queen       1.0     0.0  0.0   1.0     0.0       0.0   1.0   0.0     0.0  0.0   \n",
       "woman       1.0     0.0  0.0   0.0     1.0       0.0   1.0   0.0     0.0  0.0   \n",
       "\n",
       "          queen  woman  \n",
       "young       1.0    1.0  \n",
       "strong      0.0    0.0  \n",
       "boy         0.0    0.0  \n",
       "girl        1.0    0.0  \n",
       "pretty      0.0    1.0  \n",
       "princess    0.0    0.0  \n",
       "wise        1.0    1.0  \n",
       "king        0.0    0.0  \n",
       "prince      0.0    0.0  \n",
       "man         0.0    0.0  \n",
       "queen       0.0    0.0  \n",
       "woman       0.0    0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X, index=vocabulary, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, V = np.linalg.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.69381462, 0.        ],\n",
       "       [0.        , 3.01139938]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = min(m, n)\n",
    "# 2차원에 분산 표현을 하고 싶다. => Sigma를 2차원으로 만든다. => S[:2]\n",
    "_Sigma = np.diag(Sigma[:2])\n",
    "_Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "US = U[:, :2].dot(_Sigma)    # U를 m x 2로 만든다.\n",
    "SV = _Sigma.dot(V[:2])    # V를 2 x n으로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young [-2.14771411 -1.68108232]\n",
      "strong [-0.87555418 -1.22298025]\n",
      "boy [-1.12538134  0.06751978]\n",
      "girl [-1.09145195  0.17128992]\n",
      "pretty [-0.2078559  -0.26901575]\n",
      "princess [-0.87691625  0.50135907]\n",
      "wise [-0.48047132 -0.48947834]\n",
      "king [-1.12313423  0.9419351 ]\n",
      "prince [-0.88610171  0.53581818]\n",
      "man [-1.05550031  1.37047342]\n",
      "queen [-1.00699081  0.6639009 ]\n",
      "woman [-0.76778117  0.81011387]\n"
     ]
    }
   ],
   "source": [
    "for us, w in zip(US, vocabulary):\n",
    "    print(w, us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00699081,  0.6639009 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = US[vocabulary.index(\"queen\")]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.60771853, 1.89145342, 0.60801871, 0.49979922, 1.22839329,\n",
       "       0.20818078, 1.26787477, 0.30131763, 0.17612313, 0.70823577,\n",
       "       0.        , 0.280356  ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.linalg.norm(US - np.repeat(query, len(vocabulary)).reshape(2, -1).T, axis=1)    # euclidian distance\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young 2.60771852907508\n",
      "strong 1.8914534170554338\n",
      "boy 0.6080187065803463\n",
      "girl 0.4997992196870504\n",
      "pretty 1.2283932894413243\n",
      "princess 0.2081807761212903\n",
      "wise 1.2678747716930638\n",
      "king 0.3013176295346332\n",
      "prince 0.1761231290740206\n",
      "man 0.7082357713789706\n",
      "queen 0.0\n",
      "woman 0.2803559954243204\n"
     ]
    }
   ],
   "source": [
    "for dist, w in zip(result, vocabulary):\n",
    "    print(w, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for pair, w in zip(US, vocabulary):\n",
    "    plt.text(pair[0], pair[1], w)\n",
    "    \n",
    "plt.xlim(-2.2, .0)\n",
    "plt.ylim(-1.8, 1.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young 1.9629251313504406\n",
      "strong 1.120684451179322\n",
      "boy 0.22997785849019986\n",
      "girl 0.3013176295346332\n",
      "pretty 0.7844205262688358\n",
      "princess 0.6160119660694485\n",
      "wise 0.6255791067112819\n",
      "king 1.0590471685941836\n",
      "prince 0.6487252004892228\n",
      "man 1.479392744469623\n",
      "queen 0.7712961615262973\n",
      "woman 0.940051326041137\n"
     ]
    }
   ],
   "source": [
    "queen = US[vocabulary.index(\"queen\")]\n",
    "king = US[vocabulary.index(\"king\")]\n",
    "girl = US[vocabulary.index(\"girl\")]\n",
    "\n",
    "query = queen - king + girl\n",
    "\n",
    "result = np.linalg.norm(US - np.repeat(query, len(vocabulary)).reshape(2, -1).T, axis=1)    # euclidian distance\n",
    "\n",
    "for dist, w in zip(result, vocabulary):\n",
    "    print(w, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![n-gram](./images/n-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이와 같이 n-gram의 n을 증가시킨다.\n",
    "- n 을 증가시켜도, 데이터가 충분하면 성능은 좋아진다. 그러나 단어가 가질수 있는 조합이 $|V|^n$ 로 지수적으로 커진다. <br>\n",
    "   → 지수적으로 학습데이터가 필요해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분산표현 : 임베드행렬(embedding matrix), word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유사성을 고려할 수 있다면, 일반화 능력을 향상시킴 => 이것은 분산표현으로 할 수 있는 일\n",
    "\n",
    "\n",
    "- 단어 임베딩은 CBOW와 skip-gram 모델을 제안한 Mikolov에 의해 비약적인 발전을 이룸\n",
    "\n",
    "![embedding_matrix.png](./images/embedding_matrix.png)\n",
    "\n",
    "- CBOW는 k개만큼의 주변 단어가 주어졌을 때 중심 단어의 조건부확률을 계산한다.\n",
    "- skip-gram 모델은 CBOW와 정반대로, 중심단어가 주어졌을 때 주변단어를 예측한다.\n",
    "\n",
    "\n",
    "- CBOW는 이해하기 쉬움, skip-gram이 실제 많이 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CBOW1](./images/CBOW1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CBOW2](./images/CBOW2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip-gram-model.png](./images/skip-gram-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot vector로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones(len(vocabulary))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.diag(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[np.argwhere(X[0])[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind = lambda x: vocabulary.index(x)\n",
    "inx2word = lambda x: vocabulary[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[word2ind(\"princess\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePair(D, SIZE):    # SIZE: window size\n",
    "    pairList = list()\n",
    "    limit = len(D)\n",
    "    \n",
    "    for i, term in enumerate(D):\n",
    "        start = i - SIZE\n",
    "        end = i + SIZE + 1\n",
    "\n",
    "        for j in range(start, end):\n",
    "            if (-1 < j < limit) and (i != j):    # 단어 수는 0~limit 범위에 있음\n",
    "                pairList.append((termList[j], term))\n",
    "                    \n",
    "    return pairList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 'king'), ('princess', 'strong'), ('queen', 'strong'), ('girl', 'man')]\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 1\n",
    "\n",
    "for document in _documents:\n",
    "    print(generatePair(document, WINDOW))\n",
    "    break\n",
    "\n",
    "# (strong) king\n",
    "# (king, man) strong\n",
    "# (strong) man\n",
    "\n",
    "# king strong\n",
    "# strong king\n",
    "# strong man\n",
    "# man strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 2\n",
    "\n",
    "inputList = list()\n",
    "outputList = list()\n",
    "\n",
    "for document in _documents:\n",
    "    for pair in generatePair(document, WINDOW):\n",
    "        inputList.append(pair[0])\n",
    "        outputList.append(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 52)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputList), len(outputList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 'king'), ('queen', 'king'), ('princess', 'strong'), ('queen', 'strong'), ('princess', 'man'), ('girl', 'man'), ('girl', 'queen'), ('queen', 'queen'), ('princess', 'wise'), ('queen', 'wise'), ('princess', 'woman'), ('girl', 'woman'), ('girl', 'boy'), ('queen', 'boy'), ('princess', 'young'), ('queen', 'young'), ('princess', 'man'), ('girl', 'man'), ('girl', 'girl'), ('queen', 'girl'), ('princess', 'young'), ('queen', 'young'), ('princess', 'woman'), ('girl', 'woman'), ('girl', 'prince'), ('queen', 'prince'), ('princess', 'young'), ('queen', 'young'), ('princess', 'king'), ('girl', 'king'), ('girl', 'princess'), ('queen', 'princess'), ('princess', 'young'), ('queen', 'young'), ('princess', 'queen'), ('girl', 'queen'), ('girl', 'man'), ('princess', 'strong'), ('girl', 'woman'), ('princess', 'pretty'), ('girl', 'prince'), ('queen', 'prince'), ('princess', 'boy'), ('queen', 'boy'), ('princess', 'king'), ('girl', 'king'), ('girl', 'princess'), ('queen', 'princess'), ('princess', 'girl'), ('queen', 'girl'), ('princess', 'queen'), ('girl', 'queen')]\n"
     ]
    }
   ],
   "source": [
    "print([(t1, t2) for t1, t2 in zip(inputList, outputList)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 2\n",
    "\n",
    "inputList = list()\n",
    "outputList = list()\n",
    "\n",
    "for document in _documents:\n",
    "    for pair in generatePair(document, WINDOW):\n",
    "        inputList.append(X[word2ind(pair[0])])\n",
    "        outputList.append(X[word2ind(pair[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for t1, t2 in zip(inputList, outputList):\n",
    "    print(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 12)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputVector = np.array(inputList)\n",
    "inputVector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputVector = np.array(outputList)\n",
    "outputVector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_CBOW_model.png](./images/Word2Vec_CBOW_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "DIM = 2\n",
    "RATE = 1\n",
    "\n",
    "inputLayer = tf.placeholder(tf.float32, shape=(None, V))\n",
    "outputLayer = tf.placeholder(tf.float32, shape=(None, V))\n",
    "\n",
    "weight1 = tf.Variable(tf.random_normal([V, DIM]))\n",
    "bias1 = tf.Variable(tf.random_normal([DIM]))\n",
    "\n",
    "weight2 = tf.Variable(tf.random_normal([DIM, V]))\n",
    "bias2 = tf.Variable(tf.random_normal([V]))\n",
    "\n",
    "layer1 = tf.add(tf.matmul(inputLayer, weight1), bias1)\n",
    "\n",
    "layer2 = tf.nn.softmax(tf.add(tf.matmul(layer1, weight2), bias2))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(outputLayer * tf.log(layer2), axis=[1]))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 3.2520480155944824\n",
      "2000 - 2.0949220657348633\n",
      "4000 - 2.094527244567871\n",
      "6000 - 2.0943949222564697\n",
      "8000 - 2.0943267345428467\n",
      "10000 - 2.094285011291504\n",
      "12000 - 2.094256639480591\n",
      "14000 - 2.094235897064209\n",
      "16000 - 2.0942203998565674\n",
      "18000 - 2.094208002090454\n"
     ]
    }
   ],
   "source": [
    "iteration = 20000\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "session.run(init)\n",
    "\n",
    "for i in range(iteration):\n",
    "    session.run(optimizer, feed_dict={inputLayer:inputVector, outputLayer:outputVector})\n",
    "    \n",
    "    if i % 2000 == 0:\n",
    "        print(\"{0} - {1}\".format(i, session.run(loss, feed_dict={inputLayer:inputVector, outputLayer:outputVector})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseVector = session.run(weight1 + bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young [-1.5517378 -0.7063792]\n",
      "strong [-1.0170699 -0.7199438]\n",
      "boy [-0.11208797 -0.39703315]\n",
      "girl [ 3.7584598  -0.01047621]\n",
      "pretty [-0.7538073 -1.639951 ]\n",
      "princess [-1.4936304 -1.8388834]\n",
      "wise [-0.36034524  0.40879303]\n",
      "king [-0.49532327 -1.4323877 ]\n",
      "prince [0.26973972 0.98510134]\n",
      "man [-1.3012569  -0.03616961]\n",
      "queen [-0.46555355  3.3997931 ]\n",
      "woman [0.09877595 0.1379321 ]\n"
     ]
    }
   ],
   "source": [
    "for t, v in zip(vocabulary, denseVector):\n",
    "    print(t, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for w, v in zip(vocabulary, denseVector):\n",
    "    plt.text(v[0], v[1], w)\n",
    "    \n",
    "plt.xlim(-2, 3)\n",
    "plt.ylim(-2, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = Sequential()\n",
    "ANN.add(Dense(DIM, input_dim=V))\n",
    "ANN.add(Dense(V, activation=\"softmax\"))\n",
    "ANN.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 2)                 26        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12)                36        \n",
      "=================================================================\n",
      "Total params: 62\n",
      "Trainable params: 62\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "52/52 [==============================] - 0s 3ms/sample - loss: 2.5006 - acc: 0.0962\n",
      "Epoch 2/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.4995 - acc: 0.0962\n",
      "Epoch 3/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4983 - acc: 0.0962\n",
      "Epoch 4/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.4972 - acc: 0.0962\n",
      "Epoch 5/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.4961 - acc: 0.0962\n",
      "Epoch 6/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4950 - acc: 0.0962\n",
      "Epoch 7/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.4938 - acc: 0.0962\n",
      "Epoch 8/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.4927 - acc: 0.0962\n",
      "Epoch 9/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4916 - acc: 0.0962\n",
      "Epoch 10/1000\n",
      "52/52 [==============================] - 0s 69us/sample - loss: 2.4905 - acc: 0.0962\n",
      "Epoch 11/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4894 - acc: 0.0962\n",
      "Epoch 12/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.4884 - acc: 0.0962\n",
      "Epoch 13/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4873 - acc: 0.0962\n",
      "Epoch 14/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.4862 - acc: 0.0962\n",
      "Epoch 15/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4851 - acc: 0.0962\n",
      "Epoch 16/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4841 - acc: 0.0962\n",
      "Epoch 17/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4830 - acc: 0.0962\n",
      "Epoch 18/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.4820 - acc: 0.0962\n",
      "Epoch 19/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.4809 - acc: 0.0962\n",
      "Epoch 20/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4799 - acc: 0.0962\n",
      "Epoch 21/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.4789 - acc: 0.0962\n",
      "Epoch 22/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4779 - acc: 0.0962\n",
      "Epoch 23/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.4769 - acc: 0.0962\n",
      "Epoch 24/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4758 - acc: 0.0962\n",
      "Epoch 25/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4748 - acc: 0.0962\n",
      "Epoch 26/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4738 - acc: 0.0962\n",
      "Epoch 27/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4729 - acc: 0.0962\n",
      "Epoch 28/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4719 - acc: 0.0962\n",
      "Epoch 29/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.4709 - acc: 0.0962\n",
      "Epoch 30/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4699 - acc: 0.0962\n",
      "Epoch 31/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4690 - acc: 0.0962\n",
      "Epoch 32/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.4680 - acc: 0.0962\n",
      "Epoch 33/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4670 - acc: 0.0962\n",
      "Epoch 34/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.4661 - acc: 0.0962\n",
      "Epoch 35/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4651 - acc: 0.0962\n",
      "Epoch 36/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.4642 - acc: 0.0962\n",
      "Epoch 37/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.4633 - acc: 0.0962\n",
      "Epoch 38/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4623 - acc: 0.0962\n",
      "Epoch 39/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.4614 - acc: 0.0962\n",
      "Epoch 40/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.4605 - acc: 0.0962\n",
      "Epoch 41/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4596 - acc: 0.0962\n",
      "Epoch 42/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.4587 - acc: 0.0962\n",
      "Epoch 43/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.4578 - acc: 0.0962\n",
      "Epoch 44/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.4568 - acc: 0.0962\n",
      "Epoch 45/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.4559 - acc: 0.0962\n",
      "Epoch 46/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.4551 - acc: 0.0962\n",
      "Epoch 47/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.4542 - acc: 0.0962\n",
      "Epoch 48/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.4533 - acc: 0.0962\n",
      "Epoch 49/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.4524 - acc: 0.0962\n",
      "Epoch 50/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.4515 - acc: 0.0962\n",
      "Epoch 51/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4506 - acc: 0.0962\n",
      "Epoch 52/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.4498 - acc: 0.0962\n",
      "Epoch 53/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4489 - acc: 0.0962\n",
      "Epoch 54/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.4480 - acc: 0.0962\n",
      "Epoch 55/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.4472 - acc: 0.0962\n",
      "Epoch 56/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.4463 - acc: 0.0962\n",
      "Epoch 57/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.4455 - acc: 0.0962\n",
      "Epoch 58/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.4446 - acc: 0.0962\n",
      "Epoch 59/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.4438 - acc: 0.0962\n",
      "Epoch 60/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.4429 - acc: 0.0962\n",
      "Epoch 61/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.4421 - acc: 0.0962\n",
      "Epoch 62/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.4412 - acc: 0.0962\n",
      "Epoch 63/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.4404 - acc: 0.1154\n",
      "Epoch 64/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.4396 - acc: 0.1154\n",
      "Epoch 65/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4387 - acc: 0.1154\n",
      "Epoch 66/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.4379 - acc: 0.1154\n",
      "Epoch 67/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4371 - acc: 0.1154\n",
      "Epoch 68/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.4362 - acc: 0.1154\n",
      "Epoch 69/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4354 - acc: 0.1154\n",
      "Epoch 70/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.4346 - acc: 0.1154\n",
      "Epoch 71/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4338 - acc: 0.1154\n",
      "Epoch 72/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.4330 - acc: 0.1154\n",
      "Epoch 73/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4322 - acc: 0.1154\n",
      "Epoch 74/1000\n",
      "52/52 [==============================] - 0s 67us/sample - loss: 2.4313 - acc: 0.1154\n",
      "Epoch 75/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4305 - acc: 0.1154\n",
      "Epoch 76/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.4297 - acc: 0.1154\n",
      "Epoch 77/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.4289 - acc: 0.1154\n",
      "Epoch 78/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4281 - acc: 0.1154\n",
      "Epoch 79/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.4273 - acc: 0.1154\n",
      "Epoch 80/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.4265 - acc: 0.1154\n",
      "Epoch 81/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.4257 - acc: 0.1154\n",
      "Epoch 82/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.4249 - acc: 0.1154\n",
      "Epoch 83/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4241 - acc: 0.1154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.4233 - acc: 0.1154\n",
      "Epoch 85/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.4225 - acc: 0.1154\n",
      "Epoch 86/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.4217 - acc: 0.1154\n",
      "Epoch 87/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.4209 - acc: 0.1154\n",
      "Epoch 88/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.4201 - acc: 0.1154\n",
      "Epoch 89/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.4194 - acc: 0.1154\n",
      "Epoch 90/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.4186 - acc: 0.1154\n",
      "Epoch 91/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.4178 - acc: 0.1154\n",
      "Epoch 92/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4170 - acc: 0.1154\n",
      "Epoch 93/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.4162 - acc: 0.1154\n",
      "Epoch 94/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.4154 - acc: 0.1154\n",
      "Epoch 95/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.4146 - acc: 0.1154\n",
      "Epoch 96/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.4139 - acc: 0.1154\n",
      "Epoch 97/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.4131 - acc: 0.1154\n",
      "Epoch 98/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.4123 - acc: 0.1154\n",
      "Epoch 99/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.4115 - acc: 0.1154\n",
      "Epoch 100/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.4107 - acc: 0.1154\n",
      "Epoch 101/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4099 - acc: 0.1154\n",
      "Epoch 102/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.4092 - acc: 0.1154\n",
      "Epoch 103/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.4084 - acc: 0.1154\n",
      "Epoch 104/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.4076 - acc: 0.1154\n",
      "Epoch 105/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.4068 - acc: 0.1154\n",
      "Epoch 106/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.4060 - acc: 0.1154\n",
      "Epoch 107/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.4052 - acc: 0.1154\n",
      "Epoch 108/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.4045 - acc: 0.1154\n",
      "Epoch 109/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.4037 - acc: 0.1154\n",
      "Epoch 110/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.4029 - acc: 0.1154\n",
      "Epoch 111/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.4021 - acc: 0.1154\n",
      "Epoch 112/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.4013 - acc: 0.1154\n",
      "Epoch 113/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.4006 - acc: 0.1154\n",
      "Epoch 114/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3998 - acc: 0.1154\n",
      "Epoch 115/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3990 - acc: 0.1154\n",
      "Epoch 116/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3982 - acc: 0.1154\n",
      "Epoch 117/1000\n",
      "52/52 [==============================] - 0s 60us/sample - loss: 2.3974 - acc: 0.1154\n",
      "Epoch 118/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3967 - acc: 0.1154\n",
      "Epoch 119/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3959 - acc: 0.1154\n",
      "Epoch 120/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3951 - acc: 0.1154\n",
      "Epoch 121/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3943 - acc: 0.1154\n",
      "Epoch 122/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.3935 - acc: 0.1154\n",
      "Epoch 123/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.3927 - acc: 0.1154\n",
      "Epoch 124/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3920 - acc: 0.1154\n",
      "Epoch 125/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.3912 - acc: 0.1154\n",
      "Epoch 126/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3904 - acc: 0.1154\n",
      "Epoch 127/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3896 - acc: 0.1154\n",
      "Epoch 128/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3888 - acc: 0.1154\n",
      "Epoch 129/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.3880 - acc: 0.1154\n",
      "Epoch 130/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.3872 - acc: 0.1154\n",
      "Epoch 131/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.3865 - acc: 0.1154\n",
      "Epoch 132/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.3857 - acc: 0.1154\n",
      "Epoch 133/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3849 - acc: 0.1154\n",
      "Epoch 134/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3841 - acc: 0.1154\n",
      "Epoch 135/1000\n",
      "52/52 [==============================] - 0s 72us/sample - loss: 2.3833 - acc: 0.1154\n",
      "Epoch 136/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3825 - acc: 0.1154\n",
      "Epoch 137/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.3817 - acc: 0.1154\n",
      "Epoch 138/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.3810 - acc: 0.1154\n",
      "Epoch 139/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3802 - acc: 0.1154\n",
      "Epoch 140/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.3794 - acc: 0.1154\n",
      "Epoch 141/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3786 - acc: 0.1154\n",
      "Epoch 142/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.3778 - acc: 0.1154\n",
      "Epoch 143/1000\n",
      "52/52 [==============================] - 0s 58us/sample - loss: 2.3770 - acc: 0.1154\n",
      "Epoch 144/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.3762 - acc: 0.1154\n",
      "Epoch 145/1000\n",
      "52/52 [==============================] - 0s 75us/sample - loss: 2.3755 - acc: 0.1154\n",
      "Epoch 146/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.3747 - acc: 0.1154\n",
      "Epoch 147/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.3739 - acc: 0.1154\n",
      "Epoch 148/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.3731 - acc: 0.1154\n",
      "Epoch 149/1000\n",
      "52/52 [==============================] - 0s 64us/sample - loss: 2.3723 - acc: 0.1154\n",
      "Epoch 150/1000\n",
      "52/52 [==============================] - 0s 98us/sample - loss: 2.3715 - acc: 0.1154\n",
      "Epoch 151/1000\n",
      "52/52 [==============================] - 0s 67us/sample - loss: 2.3707 - acc: 0.1154\n",
      "Epoch 152/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3700 - acc: 0.1154\n",
      "Epoch 153/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.3692 - acc: 0.1154\n",
      "Epoch 154/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.3684 - acc: 0.1154\n",
      "Epoch 155/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.3676 - acc: 0.1731\n",
      "Epoch 156/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3668 - acc: 0.1731\n",
      "Epoch 157/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3660 - acc: 0.1731\n",
      "Epoch 158/1000\n",
      "52/52 [==============================] - 0s 76us/sample - loss: 2.3653 - acc: 0.1731\n",
      "Epoch 159/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3645 - acc: 0.1731\n",
      "Epoch 160/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.3637 - acc: 0.1731\n",
      "Epoch 161/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.3629 - acc: 0.1731\n",
      "Epoch 162/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3622 - acc: 0.1731\n",
      "Epoch 163/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3614 - acc: 0.1731\n",
      "Epoch 164/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.3606 - acc: 0.1731\n",
      "Epoch 165/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.3598 - acc: 0.1731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3591 - acc: 0.1731\n",
      "Epoch 167/1000\n",
      "52/52 [==============================] - 0s 60us/sample - loss: 2.3583 - acc: 0.1731\n",
      "Epoch 168/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.3575 - acc: 0.1731\n",
      "Epoch 169/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.3568 - acc: 0.1731\n",
      "Epoch 170/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3560 - acc: 0.1731\n",
      "Epoch 171/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3552 - acc: 0.1731\n",
      "Epoch 172/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3545 - acc: 0.1731\n",
      "Epoch 173/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.3537 - acc: 0.1731\n",
      "Epoch 174/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3530 - acc: 0.1731\n",
      "Epoch 175/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3522 - acc: 0.1731\n",
      "Epoch 176/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3514 - acc: 0.1731\n",
      "Epoch 177/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3507 - acc: 0.1731\n",
      "Epoch 178/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3499 - acc: 0.1731\n",
      "Epoch 179/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.3492 - acc: 0.1731\n",
      "Epoch 180/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3484 - acc: 0.1731\n",
      "Epoch 181/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3477 - acc: 0.1731\n",
      "Epoch 182/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.3470 - acc: 0.1731\n",
      "Epoch 183/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.3462 - acc: 0.1731\n",
      "Epoch 184/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3455 - acc: 0.1731\n",
      "Epoch 185/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.3448 - acc: 0.1731\n",
      "Epoch 186/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3440 - acc: 0.1731\n",
      "Epoch 187/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3433 - acc: 0.1731\n",
      "Epoch 188/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3426 - acc: 0.1731\n",
      "Epoch 189/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3419 - acc: 0.1731\n",
      "Epoch 190/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.3411 - acc: 0.1731\n",
      "Epoch 191/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.3404 - acc: 0.1731\n",
      "Epoch 192/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.3397 - acc: 0.1731\n",
      "Epoch 193/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3390 - acc: 0.1731\n",
      "Epoch 194/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.3383 - acc: 0.1731\n",
      "Epoch 195/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3376 - acc: 0.1731\n",
      "Epoch 196/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.3369 - acc: 0.1731\n",
      "Epoch 197/1000\n",
      "52/52 [==============================] - 0s 67us/sample - loss: 2.3362 - acc: 0.1731\n",
      "Epoch 198/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3355 - acc: 0.1731\n",
      "Epoch 199/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.3348 - acc: 0.1731\n",
      "Epoch 200/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3341 - acc: 0.1731\n",
      "Epoch 201/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3334 - acc: 0.1731\n",
      "Epoch 202/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3327 - acc: 0.1731\n",
      "Epoch 203/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.3320 - acc: 0.1731\n",
      "Epoch 204/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3314 - acc: 0.1731\n",
      "Epoch 205/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3307 - acc: 0.1731\n",
      "Epoch 206/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.3300 - acc: 0.1731\n",
      "Epoch 207/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3293 - acc: 0.1731\n",
      "Epoch 208/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.3287 - acc: 0.1731\n",
      "Epoch 209/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.3280 - acc: 0.1731\n",
      "Epoch 210/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.3274 - acc: 0.1731\n",
      "Epoch 211/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.3267 - acc: 0.1731\n",
      "Epoch 212/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3261 - acc: 0.1731\n",
      "Epoch 213/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.3254 - acc: 0.1731\n",
      "Epoch 214/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.3248 - acc: 0.1731\n",
      "Epoch 215/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3241 - acc: 0.1731\n",
      "Epoch 216/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.3235 - acc: 0.1731\n",
      "Epoch 217/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3228 - acc: 0.1731\n",
      "Epoch 218/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.3222 - acc: 0.1731\n",
      "Epoch 219/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.3216 - acc: 0.1731\n",
      "Epoch 220/1000\n",
      "52/52 [==============================] - 0s 87us/sample - loss: 2.3210 - acc: 0.1731\n",
      "Epoch 221/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.3203 - acc: 0.1731\n",
      "Epoch 222/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.3197 - acc: 0.1731\n",
      "Epoch 223/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.3191 - acc: 0.1731\n",
      "Epoch 224/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3185 - acc: 0.1731\n",
      "Epoch 225/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3179 - acc: 0.1731\n",
      "Epoch 226/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.3173 - acc: 0.1731\n",
      "Epoch 227/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.3167 - acc: 0.1731\n",
      "Epoch 228/1000\n",
      "52/52 [==============================] - 0s 67us/sample - loss: 2.3161 - acc: 0.1731\n",
      "Epoch 229/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3155 - acc: 0.1731\n",
      "Epoch 230/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3149 - acc: 0.1731\n",
      "Epoch 231/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.3143 - acc: 0.2115\n",
      "Epoch 232/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3137 - acc: 0.2115\n",
      "Epoch 233/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3131 - acc: 0.2115\n",
      "Epoch 234/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3126 - acc: 0.2115\n",
      "Epoch 235/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.3120 - acc: 0.2115\n",
      "Epoch 236/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.3114 - acc: 0.2115\n",
      "Epoch 237/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.3108 - acc: 0.2115\n",
      "Epoch 238/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3103 - acc: 0.2115\n",
      "Epoch 239/1000\n",
      "52/52 [==============================] - 0s 77us/sample - loss: 2.3097 - acc: 0.2115\n",
      "Epoch 240/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.3091 - acc: 0.2115\n",
      "Epoch 241/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.3086 - acc: 0.2115\n",
      "Epoch 242/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.3080 - acc: 0.2115\n",
      "Epoch 243/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.3075 - acc: 0.2115\n",
      "Epoch 244/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.3069 - acc: 0.2115\n",
      "Epoch 245/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.3064 - acc: 0.2115\n",
      "Epoch 246/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.3058 - acc: 0.2115\n",
      "Epoch 247/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.3053 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3048 - acc: 0.2115\n",
      "Epoch 249/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.3042 - acc: 0.2115\n",
      "Epoch 250/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3037 - acc: 0.2115\n",
      "Epoch 251/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3032 - acc: 0.2115\n",
      "Epoch 252/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.3026 - acc: 0.2115\n",
      "Epoch 253/1000\n",
      "52/52 [==============================] - 0s 68us/sample - loss: 2.3021 - acc: 0.2115\n",
      "Epoch 254/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3016 - acc: 0.2115\n",
      "Epoch 255/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.3011 - acc: 0.2115\n",
      "Epoch 256/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.3006 - acc: 0.2115\n",
      "Epoch 257/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.3001 - acc: 0.2115\n",
      "Epoch 258/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.2996 - acc: 0.2115\n",
      "Epoch 259/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2990 - acc: 0.2115\n",
      "Epoch 260/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.2985 - acc: 0.2115\n",
      "Epoch 261/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2980 - acc: 0.2115\n",
      "Epoch 262/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2975 - acc: 0.2115\n",
      "Epoch 263/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2970 - acc: 0.2115\n",
      "Epoch 264/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.2966 - acc: 0.2115\n",
      "Epoch 265/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.2961 - acc: 0.2115\n",
      "Epoch 266/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.2956 - acc: 0.2115\n",
      "Epoch 267/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.2951 - acc: 0.2115\n",
      "Epoch 268/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.2946 - acc: 0.2115\n",
      "Epoch 269/1000\n",
      "52/52 [==============================] - 0s 78us/sample - loss: 2.2941 - acc: 0.2115\n",
      "Epoch 270/1000\n",
      "52/52 [==============================] - 0s 79us/sample - loss: 2.2936 - acc: 0.2115\n",
      "Epoch 271/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2932 - acc: 0.2115\n",
      "Epoch 272/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2927 - acc: 0.2115\n",
      "Epoch 273/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.2922 - acc: 0.2115\n",
      "Epoch 274/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2918 - acc: 0.2115\n",
      "Epoch 275/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.2913 - acc: 0.2115\n",
      "Epoch 276/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2908 - acc: 0.2115\n",
      "Epoch 277/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2904 - acc: 0.2115\n",
      "Epoch 278/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2899 - acc: 0.2115\n",
      "Epoch 279/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2894 - acc: 0.2115\n",
      "Epoch 280/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2890 - acc: 0.2115\n",
      "Epoch 281/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2885 - acc: 0.2115\n",
      "Epoch 282/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.2881 - acc: 0.2115\n",
      "Epoch 283/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2876 - acc: 0.2115\n",
      "Epoch 284/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2872 - acc: 0.2115\n",
      "Epoch 285/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2867 - acc: 0.2115\n",
      "Epoch 286/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2863 - acc: 0.2115\n",
      "Epoch 287/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.2859 - acc: 0.2115\n",
      "Epoch 288/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2854 - acc: 0.2115\n",
      "Epoch 289/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.2850 - acc: 0.2115\n",
      "Epoch 290/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.2845 - acc: 0.2115\n",
      "Epoch 291/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2841 - acc: 0.2115\n",
      "Epoch 292/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.2837 - acc: 0.2115\n",
      "Epoch 293/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2832 - acc: 0.2115\n",
      "Epoch 294/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2828 - acc: 0.2115\n",
      "Epoch 295/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.2824 - acc: 0.2115\n",
      "Epoch 296/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2820 - acc: 0.2115\n",
      "Epoch 297/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.2815 - acc: 0.2115\n",
      "Epoch 298/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2811 - acc: 0.2115\n",
      "Epoch 299/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.2807 - acc: 0.2115\n",
      "Epoch 300/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2803 - acc: 0.2115\n",
      "Epoch 301/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2798 - acc: 0.2115\n",
      "Epoch 302/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2794 - acc: 0.2115\n",
      "Epoch 303/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2790 - acc: 0.2115\n",
      "Epoch 304/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2786 - acc: 0.2115\n",
      "Epoch 305/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2782 - acc: 0.2115\n",
      "Epoch 306/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2778 - acc: 0.2115\n",
      "Epoch 307/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2774 - acc: 0.2115\n",
      "Epoch 308/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2769 - acc: 0.2115\n",
      "Epoch 309/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2765 - acc: 0.2115\n",
      "Epoch 310/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2761 - acc: 0.2115\n",
      "Epoch 311/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2757 - acc: 0.2115\n",
      "Epoch 312/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2753 - acc: 0.2115\n",
      "Epoch 313/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2749 - acc: 0.2115\n",
      "Epoch 314/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2745 - acc: 0.2115\n",
      "Epoch 315/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2741 - acc: 0.2115\n",
      "Epoch 316/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2737 - acc: 0.2115\n",
      "Epoch 317/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.2733 - acc: 0.2115\n",
      "Epoch 318/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2729 - acc: 0.2115\n",
      "Epoch 319/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2725 - acc: 0.2115\n",
      "Epoch 320/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.2721 - acc: 0.2115\n",
      "Epoch 321/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2716 - acc: 0.2115\n",
      "Epoch 322/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2712 - acc: 0.2115\n",
      "Epoch 323/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.2708 - acc: 0.2115\n",
      "Epoch 324/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2704 - acc: 0.2115\n",
      "Epoch 325/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2700 - acc: 0.2115\n",
      "Epoch 326/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2696 - acc: 0.2115\n",
      "Epoch 327/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2692 - acc: 0.2115\n",
      "Epoch 328/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2688 - acc: 0.2115\n",
      "Epoch 329/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2684 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2680 - acc: 0.2115\n",
      "Epoch 331/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2676 - acc: 0.2115\n",
      "Epoch 332/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2672 - acc: 0.2115\n",
      "Epoch 333/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2668 - acc: 0.2115\n",
      "Epoch 334/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2664 - acc: 0.2115\n",
      "Epoch 335/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2659 - acc: 0.2115\n",
      "Epoch 336/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2655 - acc: 0.2115\n",
      "Epoch 337/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2651 - acc: 0.2115\n",
      "Epoch 338/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2647 - acc: 0.2115\n",
      "Epoch 339/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2643 - acc: 0.2115\n",
      "Epoch 340/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2639 - acc: 0.2115\n",
      "Epoch 341/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2635 - acc: 0.2115\n",
      "Epoch 342/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.2630 - acc: 0.2115\n",
      "Epoch 343/1000\n",
      "52/52 [==============================] - 0s 26us/sample - loss: 2.2626 - acc: 0.2115\n",
      "Epoch 344/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2622 - acc: 0.2115\n",
      "Epoch 345/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2618 - acc: 0.2115\n",
      "Epoch 346/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.2613 - acc: 0.2115\n",
      "Epoch 347/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.2609 - acc: 0.2115\n",
      "Epoch 348/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.2605 - acc: 0.2115\n",
      "Epoch 349/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2600 - acc: 0.2115\n",
      "Epoch 350/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2596 - acc: 0.2115\n",
      "Epoch 351/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.2592 - acc: 0.2115\n",
      "Epoch 352/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.2587 - acc: 0.2115\n",
      "Epoch 353/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2583 - acc: 0.2115\n",
      "Epoch 354/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2579 - acc: 0.2115\n",
      "Epoch 355/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2574 - acc: 0.2115\n",
      "Epoch 356/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2570 - acc: 0.2115\n",
      "Epoch 357/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.2565 - acc: 0.2115\n",
      "Epoch 358/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.2561 - acc: 0.2115\n",
      "Epoch 359/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2556 - acc: 0.2115\n",
      "Epoch 360/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.2551 - acc: 0.2115\n",
      "Epoch 361/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2547 - acc: 0.2115\n",
      "Epoch 362/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2542 - acc: 0.2115\n",
      "Epoch 363/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2538 - acc: 0.2115\n",
      "Epoch 364/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2533 - acc: 0.2115\n",
      "Epoch 365/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2528 - acc: 0.2115\n",
      "Epoch 366/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.2523 - acc: 0.2115\n",
      "Epoch 367/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2519 - acc: 0.2115\n",
      "Epoch 368/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.2514 - acc: 0.2115\n",
      "Epoch 369/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2509 - acc: 0.2115\n",
      "Epoch 370/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2504 - acc: 0.2115\n",
      "Epoch 371/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2499 - acc: 0.2115\n",
      "Epoch 372/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2494 - acc: 0.2115\n",
      "Epoch 373/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2489 - acc: 0.2115\n",
      "Epoch 374/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2484 - acc: 0.2115\n",
      "Epoch 375/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2479 - acc: 0.2115\n",
      "Epoch 376/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.2474 - acc: 0.2115\n",
      "Epoch 377/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2469 - acc: 0.2115\n",
      "Epoch 378/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2464 - acc: 0.2115\n",
      "Epoch 379/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.2458 - acc: 0.2115\n",
      "Epoch 380/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2453 - acc: 0.2115\n",
      "Epoch 381/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.2448 - acc: 0.2115\n",
      "Epoch 382/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.2443 - acc: 0.2115\n",
      "Epoch 383/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2437 - acc: 0.2115\n",
      "Epoch 384/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.2432 - acc: 0.2115\n",
      "Epoch 385/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2426 - acc: 0.2115\n",
      "Epoch 386/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2421 - acc: 0.2115\n",
      "Epoch 387/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2415 - acc: 0.2115\n",
      "Epoch 388/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2410 - acc: 0.2115\n",
      "Epoch 389/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2404 - acc: 0.2115\n",
      "Epoch 390/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2398 - acc: 0.2115\n",
      "Epoch 391/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2393 - acc: 0.2115\n",
      "Epoch 392/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2387 - acc: 0.2115\n",
      "Epoch 393/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2381 - acc: 0.2115\n",
      "Epoch 394/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2375 - acc: 0.2115\n",
      "Epoch 395/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.2370 - acc: 0.2115\n",
      "Epoch 396/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2364 - acc: 0.2115\n",
      "Epoch 397/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2358 - acc: 0.2115\n",
      "Epoch 398/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2352 - acc: 0.2115\n",
      "Epoch 399/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2346 - acc: 0.2115\n",
      "Epoch 400/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2340 - acc: 0.2115\n",
      "Epoch 401/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2334 - acc: 0.2115\n",
      "Epoch 402/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2327 - acc: 0.2115\n",
      "Epoch 403/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.2321 - acc: 0.2115\n",
      "Epoch 404/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2315 - acc: 0.2115\n",
      "Epoch 405/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2309 - acc: 0.2115\n",
      "Epoch 406/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2302 - acc: 0.2115\n",
      "Epoch 407/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2296 - acc: 0.2115\n",
      "Epoch 408/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2290 - acc: 0.2115\n",
      "Epoch 409/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2283 - acc: 0.2115\n",
      "Epoch 410/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.2277 - acc: 0.2115\n",
      "Epoch 411/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2270 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 412/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2264 - acc: 0.2115\n",
      "Epoch 413/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2257 - acc: 0.2115\n",
      "Epoch 414/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2250 - acc: 0.2115\n",
      "Epoch 415/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.2244 - acc: 0.2115\n",
      "Epoch 416/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2237 - acc: 0.2115\n",
      "Epoch 417/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.2230 - acc: 0.2115\n",
      "Epoch 418/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2223 - acc: 0.2115\n",
      "Epoch 419/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2217 - acc: 0.2115\n",
      "Epoch 420/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.2210 - acc: 0.2115\n",
      "Epoch 421/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2203 - acc: 0.2115\n",
      "Epoch 422/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.2196 - acc: 0.2115\n",
      "Epoch 423/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2189 - acc: 0.2115\n",
      "Epoch 424/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2182 - acc: 0.2115\n",
      "Epoch 425/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.2175 - acc: 0.2115\n",
      "Epoch 426/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.2168 - acc: 0.2115\n",
      "Epoch 427/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.2161 - acc: 0.2115\n",
      "Epoch 428/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.2154 - acc: 0.2115\n",
      "Epoch 429/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.2147 - acc: 0.2115\n",
      "Epoch 430/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2140 - acc: 0.2115\n",
      "Epoch 431/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.2133 - acc: 0.2115\n",
      "Epoch 432/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.2125 - acc: 0.2115\n",
      "Epoch 433/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2118 - acc: 0.2115\n",
      "Epoch 434/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2111 - acc: 0.2115\n",
      "Epoch 435/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.2104 - acc: 0.2115\n",
      "Epoch 436/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.2097 - acc: 0.2115\n",
      "Epoch 437/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.2089 - acc: 0.2115\n",
      "Epoch 438/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.2082 - acc: 0.2115\n",
      "Epoch 439/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.2075 - acc: 0.2115\n",
      "Epoch 440/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2067 - acc: 0.2115\n",
      "Epoch 441/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.2060 - acc: 0.2115\n",
      "Epoch 442/1000\n",
      "52/52 [==============================] - 0s 62us/sample - loss: 2.2053 - acc: 0.2115\n",
      "Epoch 443/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2045 - acc: 0.2115\n",
      "Epoch 444/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.2038 - acc: 0.2115\n",
      "Epoch 445/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.2031 - acc: 0.2115\n",
      "Epoch 446/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.2023 - acc: 0.2115\n",
      "Epoch 447/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.2016 - acc: 0.2115\n",
      "Epoch 448/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.2009 - acc: 0.2115\n",
      "Epoch 449/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.2001 - acc: 0.2115\n",
      "Epoch 450/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1994 - acc: 0.2115\n",
      "Epoch 451/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1986 - acc: 0.2115\n",
      "Epoch 452/1000\n",
      "52/52 [==============================] - 0s 26us/sample - loss: 2.1979 - acc: 0.2115\n",
      "Epoch 453/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1972 - acc: 0.2115\n",
      "Epoch 454/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1964 - acc: 0.2115\n",
      "Epoch 455/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1957 - acc: 0.2115\n",
      "Epoch 456/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1950 - acc: 0.2115\n",
      "Epoch 457/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1942 - acc: 0.2115\n",
      "Epoch 458/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1935 - acc: 0.2115\n",
      "Epoch 459/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1928 - acc: 0.2115\n",
      "Epoch 460/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.1920 - acc: 0.2115\n",
      "Epoch 461/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1913 - acc: 0.2115\n",
      "Epoch 462/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1906 - acc: 0.2115\n",
      "Epoch 463/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1898 - acc: 0.2115\n",
      "Epoch 464/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1891 - acc: 0.2115\n",
      "Epoch 465/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1884 - acc: 0.2115\n",
      "Epoch 466/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1877 - acc: 0.2115\n",
      "Epoch 467/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1869 - acc: 0.2115\n",
      "Epoch 468/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1862 - acc: 0.2115\n",
      "Epoch 469/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1855 - acc: 0.2115\n",
      "Epoch 470/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1848 - acc: 0.2115\n",
      "Epoch 471/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1841 - acc: 0.2115\n",
      "Epoch 472/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1834 - acc: 0.2115\n",
      "Epoch 473/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1827 - acc: 0.2115\n",
      "Epoch 474/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1819 - acc: 0.2115\n",
      "Epoch 475/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1812 - acc: 0.2115\n",
      "Epoch 476/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1805 - acc: 0.2115\n",
      "Epoch 477/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1798 - acc: 0.2115\n",
      "Epoch 478/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1792 - acc: 0.2115\n",
      "Epoch 479/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1785 - acc: 0.2115\n",
      "Epoch 480/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1778 - acc: 0.2115\n",
      "Epoch 481/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1771 - acc: 0.2115\n",
      "Epoch 482/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1764 - acc: 0.2115\n",
      "Epoch 483/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1757 - acc: 0.2115\n",
      "Epoch 484/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1751 - acc: 0.2115\n",
      "Epoch 485/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1744 - acc: 0.2115\n",
      "Epoch 486/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1737 - acc: 0.2115\n",
      "Epoch 487/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1731 - acc: 0.2115\n",
      "Epoch 488/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1724 - acc: 0.2115\n",
      "Epoch 489/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1717 - acc: 0.2115\n",
      "Epoch 490/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1711 - acc: 0.2115\n",
      "Epoch 491/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1704 - acc: 0.2115\n",
      "Epoch 492/1000\n",
      "52/52 [==============================] - 0s 62us/sample - loss: 2.1698 - acc: 0.2115\n",
      "Epoch 493/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1692 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1685 - acc: 0.2115\n",
      "Epoch 495/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1679 - acc: 0.2115\n",
      "Epoch 496/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.1673 - acc: 0.2115\n",
      "Epoch 497/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1666 - acc: 0.2115\n",
      "Epoch 498/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1660 - acc: 0.2115\n",
      "Epoch 499/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1654 - acc: 0.2115\n",
      "Epoch 500/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1648 - acc: 0.2115\n",
      "Epoch 501/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.1642 - acc: 0.2115\n",
      "Epoch 502/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1636 - acc: 0.2115\n",
      "Epoch 503/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1630 - acc: 0.2115\n",
      "Epoch 504/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1624 - acc: 0.2115\n",
      "Epoch 505/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1618 - acc: 0.2115\n",
      "Epoch 506/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1612 - acc: 0.2115\n",
      "Epoch 507/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1607 - acc: 0.2115\n",
      "Epoch 508/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1601 - acc: 0.2115\n",
      "Epoch 509/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1595 - acc: 0.2115\n",
      "Epoch 510/1000\n",
      "52/52 [==============================] - 0s 58us/sample - loss: 2.1590 - acc: 0.2115\n",
      "Epoch 511/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1584 - acc: 0.2115\n",
      "Epoch 512/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1578 - acc: 0.2115\n",
      "Epoch 513/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1573 - acc: 0.2115\n",
      "Epoch 514/1000\n",
      "52/52 [==============================] - 0s 61us/sample - loss: 2.1567 - acc: 0.2115\n",
      "Epoch 515/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1562 - acc: 0.2115\n",
      "Epoch 516/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1557 - acc: 0.2115\n",
      "Epoch 517/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1551 - acc: 0.2115\n",
      "Epoch 518/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1546 - acc: 0.2115\n",
      "Epoch 519/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1541 - acc: 0.2115\n",
      "Epoch 520/1000\n",
      "52/52 [==============================] - 0s 63us/sample - loss: 2.1536 - acc: 0.2115\n",
      "Epoch 521/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1531 - acc: 0.2115\n",
      "Epoch 522/1000\n",
      "52/52 [==============================] - 0s 64us/sample - loss: 2.1526 - acc: 0.2115\n",
      "Epoch 523/1000\n",
      "52/52 [==============================] - 0s 77us/sample - loss: 2.1521 - acc: 0.2115\n",
      "Epoch 524/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1516 - acc: 0.2115\n",
      "Epoch 525/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1511 - acc: 0.2115\n",
      "Epoch 526/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1506 - acc: 0.2115\n",
      "Epoch 527/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1501 - acc: 0.2115\n",
      "Epoch 528/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1496 - acc: 0.2115\n",
      "Epoch 529/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1491 - acc: 0.2115\n",
      "Epoch 530/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1487 - acc: 0.2115\n",
      "Epoch 531/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1482 - acc: 0.2115\n",
      "Epoch 532/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1477 - acc: 0.2115\n",
      "Epoch 533/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1473 - acc: 0.2115\n",
      "Epoch 534/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1468 - acc: 0.2115\n",
      "Epoch 535/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1464 - acc: 0.2115\n",
      "Epoch 536/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1460 - acc: 0.2115\n",
      "Epoch 537/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1455 - acc: 0.2115\n",
      "Epoch 538/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1451 - acc: 0.2115\n",
      "Epoch 539/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1447 - acc: 0.2115\n",
      "Epoch 540/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1442 - acc: 0.2115\n",
      "Epoch 541/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1438 - acc: 0.2115\n",
      "Epoch 542/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1434 - acc: 0.2115\n",
      "Epoch 543/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1430 - acc: 0.2115\n",
      "Epoch 544/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1426 - acc: 0.2115\n",
      "Epoch 545/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1422 - acc: 0.2115\n",
      "Epoch 546/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1418 - acc: 0.2115\n",
      "Epoch 547/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1414 - acc: 0.2115\n",
      "Epoch 548/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1410 - acc: 0.2115\n",
      "Epoch 549/1000\n",
      "52/52 [==============================] - 0s 64us/sample - loss: 2.1406 - acc: 0.2115\n",
      "Epoch 550/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1402 - acc: 0.2115\n",
      "Epoch 551/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1398 - acc: 0.2115\n",
      "Epoch 552/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1395 - acc: 0.2115\n",
      "Epoch 553/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1391 - acc: 0.2115\n",
      "Epoch 554/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1387 - acc: 0.2115\n",
      "Epoch 555/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1384 - acc: 0.2115\n",
      "Epoch 556/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1380 - acc: 0.2115\n",
      "Epoch 557/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1377 - acc: 0.2115\n",
      "Epoch 558/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1373 - acc: 0.2115\n",
      "Epoch 559/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1370 - acc: 0.2115\n",
      "Epoch 560/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1366 - acc: 0.2115\n",
      "Epoch 561/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1363 - acc: 0.2115\n",
      "Epoch 562/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1360 - acc: 0.2115\n",
      "Epoch 563/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1356 - acc: 0.2115\n",
      "Epoch 564/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1353 - acc: 0.2115\n",
      "Epoch 565/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1350 - acc: 0.2115\n",
      "Epoch 566/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1347 - acc: 0.2115\n",
      "Epoch 567/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1343 - acc: 0.2115\n",
      "Epoch 568/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1340 - acc: 0.2115\n",
      "Epoch 569/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1337 - acc: 0.2115\n",
      "Epoch 570/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1334 - acc: 0.2115\n",
      "Epoch 571/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1331 - acc: 0.2115\n",
      "Epoch 572/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1328 - acc: 0.2115\n",
      "Epoch 573/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1325 - acc: 0.2115\n",
      "Epoch 574/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1322 - acc: 0.2115\n",
      "Epoch 575/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1319 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 576/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1316 - acc: 0.2115\n",
      "Epoch 577/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1314 - acc: 0.2115\n",
      "Epoch 578/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1311 - acc: 0.2115\n",
      "Epoch 579/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1308 - acc: 0.2115\n",
      "Epoch 580/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1305 - acc: 0.2115\n",
      "Epoch 581/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1302 - acc: 0.2115\n",
      "Epoch 582/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1300 - acc: 0.2115\n",
      "Epoch 583/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1297 - acc: 0.2115\n",
      "Epoch 584/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1295 - acc: 0.2115\n",
      "Epoch 585/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1292 - acc: 0.2115\n",
      "Epoch 586/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1289 - acc: 0.2115\n",
      "Epoch 587/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1287 - acc: 0.2115\n",
      "Epoch 588/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1284 - acc: 0.2115\n",
      "Epoch 589/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1282 - acc: 0.2115\n",
      "Epoch 590/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1279 - acc: 0.2115\n",
      "Epoch 591/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.1277 - acc: 0.2115\n",
      "Epoch 592/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1274 - acc: 0.2115\n",
      "Epoch 593/1000\n",
      "52/52 [==============================] - 0s 62us/sample - loss: 2.1272 - acc: 0.2115\n",
      "Epoch 594/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1270 - acc: 0.2115\n",
      "Epoch 595/1000\n",
      "52/52 [==============================] - 0s 70us/sample - loss: 2.1267 - acc: 0.2115\n",
      "Epoch 596/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1265 - acc: 0.2115\n",
      "Epoch 597/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1263 - acc: 0.2115\n",
      "Epoch 598/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1260 - acc: 0.2115\n",
      "Epoch 599/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1258 - acc: 0.2115\n",
      "Epoch 600/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1256 - acc: 0.2115\n",
      "Epoch 601/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1254 - acc: 0.2115\n",
      "Epoch 602/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1252 - acc: 0.2115\n",
      "Epoch 603/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1250 - acc: 0.2115\n",
      "Epoch 604/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1247 - acc: 0.2115\n",
      "Epoch 605/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1245 - acc: 0.2115\n",
      "Epoch 606/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1243 - acc: 0.2115\n",
      "Epoch 607/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1241 - acc: 0.2115\n",
      "Epoch 608/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1239 - acc: 0.2115\n",
      "Epoch 609/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1237 - acc: 0.2115\n",
      "Epoch 610/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1235 - acc: 0.2115\n",
      "Epoch 611/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1233 - acc: 0.2115\n",
      "Epoch 612/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1231 - acc: 0.2115\n",
      "Epoch 613/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1229 - acc: 0.2115\n",
      "Epoch 614/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1227 - acc: 0.2115\n",
      "Epoch 615/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1225 - acc: 0.2115\n",
      "Epoch 616/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1223 - acc: 0.2115\n",
      "Epoch 617/1000\n",
      "52/52 [==============================] - 0s 58us/sample - loss: 2.1222 - acc: 0.2115\n",
      "Epoch 618/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1220 - acc: 0.2115\n",
      "Epoch 619/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1218 - acc: 0.2115\n",
      "Epoch 620/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1216 - acc: 0.2115\n",
      "Epoch 621/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1214 - acc: 0.2115\n",
      "Epoch 622/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1212 - acc: 0.2115\n",
      "Epoch 623/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1211 - acc: 0.2115\n",
      "Epoch 624/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1209 - acc: 0.2115\n",
      "Epoch 625/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1207 - acc: 0.2115\n",
      "Epoch 626/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1206 - acc: 0.2115\n",
      "Epoch 627/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1204 - acc: 0.2115\n",
      "Epoch 628/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1202 - acc: 0.2115\n",
      "Epoch 629/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1200 - acc: 0.2115\n",
      "Epoch 630/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1199 - acc: 0.2115\n",
      "Epoch 631/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1197 - acc: 0.2115\n",
      "Epoch 632/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1196 - acc: 0.2115\n",
      "Epoch 633/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1194 - acc: 0.2115\n",
      "Epoch 634/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1192 - acc: 0.2115\n",
      "Epoch 635/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1191 - acc: 0.2115\n",
      "Epoch 636/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1189 - acc: 0.2115\n",
      "Epoch 637/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1188 - acc: 0.2115\n",
      "Epoch 638/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1186 - acc: 0.2115\n",
      "Epoch 639/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1185 - acc: 0.2115\n",
      "Epoch 640/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1183 - acc: 0.2115\n",
      "Epoch 641/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1182 - acc: 0.2115\n",
      "Epoch 642/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1180 - acc: 0.2115\n",
      "Epoch 643/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1179 - acc: 0.2115\n",
      "Epoch 644/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1177 - acc: 0.2115\n",
      "Epoch 645/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1176 - acc: 0.2115\n",
      "Epoch 646/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1174 - acc: 0.2115\n",
      "Epoch 647/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1173 - acc: 0.2115\n",
      "Epoch 648/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1172 - acc: 0.2115\n",
      "Epoch 649/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1170 - acc: 0.2115\n",
      "Epoch 650/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1169 - acc: 0.2115\n",
      "Epoch 651/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1167 - acc: 0.2115\n",
      "Epoch 652/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1166 - acc: 0.2115\n",
      "Epoch 653/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1165 - acc: 0.2115\n",
      "Epoch 654/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1163 - acc: 0.2115\n",
      "Epoch 655/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1162 - acc: 0.2115\n",
      "Epoch 656/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1161 - acc: 0.2115\n",
      "Epoch 657/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1159 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1158 - acc: 0.2115\n",
      "Epoch 659/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1157 - acc: 0.2115\n",
      "Epoch 660/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1156 - acc: 0.2115\n",
      "Epoch 661/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1154 - acc: 0.2115\n",
      "Epoch 662/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1153 - acc: 0.2115\n",
      "Epoch 663/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1152 - acc: 0.2115\n",
      "Epoch 664/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1151 - acc: 0.2115\n",
      "Epoch 665/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1149 - acc: 0.2115\n",
      "Epoch 666/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1148 - acc: 0.2115\n",
      "Epoch 667/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1147 - acc: 0.2115\n",
      "Epoch 668/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1146 - acc: 0.2115\n",
      "Epoch 669/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1145 - acc: 0.2115\n",
      "Epoch 670/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1143 - acc: 0.2115\n",
      "Epoch 671/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1142 - acc: 0.2115\n",
      "Epoch 672/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1141 - acc: 0.2115\n",
      "Epoch 673/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1140 - acc: 0.2115\n",
      "Epoch 674/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1139 - acc: 0.2115\n",
      "Epoch 675/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1138 - acc: 0.2115\n",
      "Epoch 676/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1137 - acc: 0.2115\n",
      "Epoch 677/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1136 - acc: 0.2115\n",
      "Epoch 678/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1134 - acc: 0.2115\n",
      "Epoch 679/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1133 - acc: 0.2115\n",
      "Epoch 680/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1132 - acc: 0.2115\n",
      "Epoch 681/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1131 - acc: 0.2115\n",
      "Epoch 682/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1130 - acc: 0.2115\n",
      "Epoch 683/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1129 - acc: 0.2115\n",
      "Epoch 684/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1128 - acc: 0.2115\n",
      "Epoch 685/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.1127 - acc: 0.2115\n",
      "Epoch 686/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1126 - acc: 0.2115\n",
      "Epoch 687/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1125 - acc: 0.2115\n",
      "Epoch 688/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1124 - acc: 0.2115\n",
      "Epoch 689/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1123 - acc: 0.2115\n",
      "Epoch 690/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1122 - acc: 0.2115\n",
      "Epoch 691/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1121 - acc: 0.2115\n",
      "Epoch 692/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1120 - acc: 0.2115\n",
      "Epoch 693/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1119 - acc: 0.2115\n",
      "Epoch 694/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1118 - acc: 0.2115\n",
      "Epoch 695/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1117 - acc: 0.2115\n",
      "Epoch 696/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1116 - acc: 0.2115\n",
      "Epoch 697/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1115 - acc: 0.2115\n",
      "Epoch 698/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1114 - acc: 0.2115\n",
      "Epoch 699/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1113 - acc: 0.2115\n",
      "Epoch 700/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1112 - acc: 0.2115\n",
      "Epoch 701/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1111 - acc: 0.2115\n",
      "Epoch 702/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1111 - acc: 0.2115\n",
      "Epoch 703/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1110 - acc: 0.2115\n",
      "Epoch 704/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1109 - acc: 0.2115\n",
      "Epoch 705/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1108 - acc: 0.2115\n",
      "Epoch 706/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1107 - acc: 0.2115\n",
      "Epoch 707/1000\n",
      "52/52 [==============================] - 0s 88us/sample - loss: 2.1106 - acc: 0.2115\n",
      "Epoch 708/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.1105 - acc: 0.2115\n",
      "Epoch 709/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1104 - acc: 0.2115\n",
      "Epoch 710/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1104 - acc: 0.2115\n",
      "Epoch 711/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1103 - acc: 0.2115\n",
      "Epoch 712/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1102 - acc: 0.2115\n",
      "Epoch 713/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1101 - acc: 0.2115\n",
      "Epoch 714/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1100 - acc: 0.2115\n",
      "Epoch 715/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1099 - acc: 0.2115\n",
      "Epoch 716/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1098 - acc: 0.2115\n",
      "Epoch 717/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1098 - acc: 0.2115\n",
      "Epoch 718/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1097 - acc: 0.2115\n",
      "Epoch 719/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1096 - acc: 0.2115\n",
      "Epoch 720/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1095 - acc: 0.2115\n",
      "Epoch 721/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1094 - acc: 0.2115\n",
      "Epoch 722/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1094 - acc: 0.2115\n",
      "Epoch 723/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1093 - acc: 0.2115\n",
      "Epoch 724/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1092 - acc: 0.2115\n",
      "Epoch 725/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1091 - acc: 0.2115\n",
      "Epoch 726/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1091 - acc: 0.2115\n",
      "Epoch 727/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1090 - acc: 0.2115\n",
      "Epoch 728/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1089 - acc: 0.2115\n",
      "Epoch 729/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1088 - acc: 0.2115\n",
      "Epoch 730/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1088 - acc: 0.2115\n",
      "Epoch 731/1000\n",
      "52/52 [==============================] - 0s 70us/sample - loss: 2.1087 - acc: 0.2115\n",
      "Epoch 732/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1086 - acc: 0.2115\n",
      "Epoch 733/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1085 - acc: 0.2115\n",
      "Epoch 734/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1085 - acc: 0.2115\n",
      "Epoch 735/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1084 - acc: 0.2115\n",
      "Epoch 736/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1083 - acc: 0.2115\n",
      "Epoch 737/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1082 - acc: 0.2115\n",
      "Epoch 738/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1082 - acc: 0.2115\n",
      "Epoch 739/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1081 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1080 - acc: 0.2115\n",
      "Epoch 741/1000\n",
      "52/52 [==============================] - 0s 62us/sample - loss: 2.1080 - acc: 0.2115\n",
      "Epoch 742/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1079 - acc: 0.2115\n",
      "Epoch 743/1000\n",
      "52/52 [==============================] - 0s 68us/sample - loss: 2.1078 - acc: 0.2115\n",
      "Epoch 744/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1078 - acc: 0.2115\n",
      "Epoch 745/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1077 - acc: 0.2115\n",
      "Epoch 746/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1076 - acc: 0.2115\n",
      "Epoch 747/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1076 - acc: 0.2115\n",
      "Epoch 748/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1075 - acc: 0.2115\n",
      "Epoch 749/1000\n",
      "52/52 [==============================] - 0s 78us/sample - loss: 2.1074 - acc: 0.2115\n",
      "Epoch 750/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1074 - acc: 0.2115\n",
      "Epoch 751/1000\n",
      "52/52 [==============================] - 0s 60us/sample - loss: 2.1073 - acc: 0.2115\n",
      "Epoch 752/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1072 - acc: 0.2115\n",
      "Epoch 753/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1072 - acc: 0.2115\n",
      "Epoch 754/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1071 - acc: 0.2115\n",
      "Epoch 755/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1070 - acc: 0.2115\n",
      "Epoch 756/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1070 - acc: 0.2115\n",
      "Epoch 757/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1069 - acc: 0.2115\n",
      "Epoch 758/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1069 - acc: 0.2115\n",
      "Epoch 759/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1068 - acc: 0.2115\n",
      "Epoch 760/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1067 - acc: 0.2115\n",
      "Epoch 761/1000\n",
      "52/52 [==============================] - 0s 83us/sample - loss: 2.1067 - acc: 0.2115\n",
      "Epoch 762/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1066 - acc: 0.2115\n",
      "Epoch 763/1000\n",
      "52/52 [==============================] - 0s 55us/sample - loss: 2.1065 - acc: 0.2115\n",
      "Epoch 764/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1065 - acc: 0.2115\n",
      "Epoch 765/1000\n",
      "52/52 [==============================] - 0s 63us/sample - loss: 2.1064 - acc: 0.2115\n",
      "Epoch 766/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1064 - acc: 0.2115\n",
      "Epoch 767/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1063 - acc: 0.2115\n",
      "Epoch 768/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1063 - acc: 0.2115\n",
      "Epoch 769/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1062 - acc: 0.2115\n",
      "Epoch 770/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1061 - acc: 0.2115\n",
      "Epoch 771/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1061 - acc: 0.2115\n",
      "Epoch 772/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1060 - acc: 0.2115\n",
      "Epoch 773/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1060 - acc: 0.2115\n",
      "Epoch 774/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1059 - acc: 0.2115\n",
      "Epoch 775/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1059 - acc: 0.2115\n",
      "Epoch 776/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1058 - acc: 0.2115\n",
      "Epoch 777/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1057 - acc: 0.2115\n",
      "Epoch 778/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.1057 - acc: 0.2115\n",
      "Epoch 779/1000\n",
      "52/52 [==============================] - 0s 53us/sample - loss: 2.1056 - acc: 0.2115\n",
      "Epoch 780/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1056 - acc: 0.2115\n",
      "Epoch 781/1000\n",
      "52/52 [==============================] - 0s 52us/sample - loss: 2.1055 - acc: 0.2115\n",
      "Epoch 782/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1055 - acc: 0.2115\n",
      "Epoch 783/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1054 - acc: 0.2115\n",
      "Epoch 784/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1054 - acc: 0.2115\n",
      "Epoch 785/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1053 - acc: 0.2115\n",
      "Epoch 786/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1053 - acc: 0.2115\n",
      "Epoch 787/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.1052 - acc: 0.2115\n",
      "Epoch 788/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1052 - acc: 0.2115\n",
      "Epoch 789/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1051 - acc: 0.2115\n",
      "Epoch 790/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1051 - acc: 0.2115\n",
      "Epoch 791/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1050 - acc: 0.2115\n",
      "Epoch 792/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1050 - acc: 0.2115\n",
      "Epoch 793/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1049 - acc: 0.2115\n",
      "Epoch 794/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1049 - acc: 0.2115\n",
      "Epoch 795/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1048 - acc: 0.2115\n",
      "Epoch 796/1000\n",
      "52/52 [==============================] - 0s 26us/sample - loss: 2.1048 - acc: 0.2115\n",
      "Epoch 797/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1047 - acc: 0.2115\n",
      "Epoch 798/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1047 - acc: 0.2115\n",
      "Epoch 799/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1046 - acc: 0.2115\n",
      "Epoch 800/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1046 - acc: 0.2115\n",
      "Epoch 801/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1045 - acc: 0.2115\n",
      "Epoch 802/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1045 - acc: 0.2115\n",
      "Epoch 803/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1044 - acc: 0.2115\n",
      "Epoch 804/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1044 - acc: 0.2115\n",
      "Epoch 805/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1043 - acc: 0.2115\n",
      "Epoch 806/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1043 - acc: 0.2115\n",
      "Epoch 807/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1042 - acc: 0.2115\n",
      "Epoch 808/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1042 - acc: 0.2115\n",
      "Epoch 809/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1041 - acc: 0.2115\n",
      "Epoch 810/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1041 - acc: 0.2115\n",
      "Epoch 811/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1041 - acc: 0.2115\n",
      "Epoch 812/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1040 - acc: 0.2115\n",
      "Epoch 813/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1040 - acc: 0.2115\n",
      "Epoch 814/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1039 - acc: 0.2115\n",
      "Epoch 815/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1039 - acc: 0.2115\n",
      "Epoch 816/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1038 - acc: 0.2115\n",
      "Epoch 817/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1038 - acc: 0.2115\n",
      "Epoch 818/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1037 - acc: 0.2115\n",
      "Epoch 819/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1037 - acc: 0.2115\n",
      "Epoch 820/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1037 - acc: 0.2115\n",
      "Epoch 821/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1036 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 822/1000\n",
      "52/52 [==============================] - 0s 56us/sample - loss: 2.1036 - acc: 0.2115\n",
      "Epoch 823/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1035 - acc: 0.2115\n",
      "Epoch 824/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1035 - acc: 0.2115\n",
      "Epoch 825/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1035 - acc: 0.2115\n",
      "Epoch 826/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1034 - acc: 0.2115\n",
      "Epoch 827/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1034 - acc: 0.2115\n",
      "Epoch 828/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1033 - acc: 0.2115\n",
      "Epoch 829/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1033 - acc: 0.2115\n",
      "Epoch 830/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1033 - acc: 0.2115\n",
      "Epoch 831/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1032 - acc: 0.2115\n",
      "Epoch 832/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1032 - acc: 0.2115\n",
      "Epoch 833/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1031 - acc: 0.2115\n",
      "Epoch 834/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1031 - acc: 0.2115\n",
      "Epoch 835/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1031 - acc: 0.2115\n",
      "Epoch 836/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1030 - acc: 0.2115\n",
      "Epoch 837/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1030 - acc: 0.2115\n",
      "Epoch 838/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1029 - acc: 0.2115\n",
      "Epoch 839/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1029 - acc: 0.2115\n",
      "Epoch 840/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1029 - acc: 0.2115\n",
      "Epoch 841/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1028 - acc: 0.2115\n",
      "Epoch 842/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1028 - acc: 0.2115\n",
      "Epoch 843/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1027 - acc: 0.2115\n",
      "Epoch 844/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1027 - acc: 0.2115\n",
      "Epoch 845/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1027 - acc: 0.2115\n",
      "Epoch 846/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1026 - acc: 0.2115\n",
      "Epoch 847/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1026 - acc: 0.2115\n",
      "Epoch 848/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1026 - acc: 0.2115\n",
      "Epoch 849/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1025 - acc: 0.2115\n",
      "Epoch 850/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1025 - acc: 0.2115\n",
      "Epoch 851/1000\n",
      "52/52 [==============================] - 0s 49us/sample - loss: 2.1025 - acc: 0.2115\n",
      "Epoch 852/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.1024 - acc: 0.2115\n",
      "Epoch 853/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1024 - acc: 0.2115\n",
      "Epoch 854/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1023 - acc: 0.2115\n",
      "Epoch 855/1000\n",
      "52/52 [==============================] - 0s 44us/sample - loss: 2.1023 - acc: 0.2115\n",
      "Epoch 856/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1023 - acc: 0.2115\n",
      "Epoch 857/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1022 - acc: 0.2115\n",
      "Epoch 858/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1022 - acc: 0.2115\n",
      "Epoch 859/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1022 - acc: 0.2115\n",
      "Epoch 860/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1021 - acc: 0.2115\n",
      "Epoch 861/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1021 - acc: 0.2115\n",
      "Epoch 862/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1021 - acc: 0.2115\n",
      "Epoch 863/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1020 - acc: 0.2115\n",
      "Epoch 864/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1020 - acc: 0.2115\n",
      "Epoch 865/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1020 - acc: 0.2115\n",
      "Epoch 866/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1019 - acc: 0.2115\n",
      "Epoch 867/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1019 - acc: 0.2115\n",
      "Epoch 868/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1019 - acc: 0.2115\n",
      "Epoch 869/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1018 - acc: 0.2115\n",
      "Epoch 870/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1018 - acc: 0.2115\n",
      "Epoch 871/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1018 - acc: 0.2115\n",
      "Epoch 872/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1017 - acc: 0.2115\n",
      "Epoch 873/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1017 - acc: 0.2115\n",
      "Epoch 874/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1017 - acc: 0.2115\n",
      "Epoch 875/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1016 - acc: 0.2115\n",
      "Epoch 876/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1016 - acc: 0.2115\n",
      "Epoch 877/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.1016 - acc: 0.2115\n",
      "Epoch 878/1000\n",
      "52/52 [==============================] - 0s 88us/sample - loss: 2.1015 - acc: 0.2115\n",
      "Epoch 879/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1015 - acc: 0.2115\n",
      "Epoch 880/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1015 - acc: 0.2115\n",
      "Epoch 881/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1015 - acc: 0.2115\n",
      "Epoch 882/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1014 - acc: 0.2115\n",
      "Epoch 883/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1014 - acc: 0.2115\n",
      "Epoch 884/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.1014 - acc: 0.2115\n",
      "Epoch 885/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1013 - acc: 0.2115\n",
      "Epoch 886/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1013 - acc: 0.2115\n",
      "Epoch 887/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1013 - acc: 0.2115\n",
      "Epoch 888/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1012 - acc: 0.2115\n",
      "Epoch 889/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1012 - acc: 0.2115\n",
      "Epoch 890/1000\n",
      "52/52 [==============================] - 0s 48us/sample - loss: 2.1012 - acc: 0.2115\n",
      "Epoch 891/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1012 - acc: 0.2115\n",
      "Epoch 892/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1011 - acc: 0.2115\n",
      "Epoch 893/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1011 - acc: 0.2115\n",
      "Epoch 894/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1011 - acc: 0.2115\n",
      "Epoch 895/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1010 - acc: 0.2115\n",
      "Epoch 896/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.1010 - acc: 0.2115\n",
      "Epoch 897/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1010 - acc: 0.2115\n",
      "Epoch 898/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1010 - acc: 0.2115\n",
      "Epoch 899/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.1009 - acc: 0.2115\n",
      "Epoch 900/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1009 - acc: 0.2115\n",
      "Epoch 901/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1009 - acc: 0.2115\n",
      "Epoch 902/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1008 - acc: 0.2115\n",
      "Epoch 903/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1008 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 904/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1008 - acc: 0.2115\n",
      "Epoch 905/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.1008 - acc: 0.2115\n",
      "Epoch 906/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1007 - acc: 0.2115\n",
      "Epoch 907/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1007 - acc: 0.2115\n",
      "Epoch 908/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1007 - acc: 0.2115\n",
      "Epoch 909/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.1007 - acc: 0.2115\n",
      "Epoch 910/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.1006 - acc: 0.2115\n",
      "Epoch 911/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.1006 - acc: 0.2115\n",
      "Epoch 912/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1006 - acc: 0.2115\n",
      "Epoch 913/1000\n",
      "52/52 [==============================] - 0s 25us/sample - loss: 2.1006 - acc: 0.2115\n",
      "Epoch 914/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.1005 - acc: 0.2115\n",
      "Epoch 915/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.1005 - acc: 0.2115\n",
      "Epoch 916/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1005 - acc: 0.2115\n",
      "Epoch 917/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1005 - acc: 0.2115\n",
      "Epoch 918/1000\n",
      "52/52 [==============================] - 0s 57us/sample - loss: 2.1004 - acc: 0.2115\n",
      "Epoch 919/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1004 - acc: 0.2115\n",
      "Epoch 920/1000\n",
      "52/52 [==============================] - 0s 50us/sample - loss: 2.1004 - acc: 0.2115\n",
      "Epoch 921/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1003 - acc: 0.2115\n",
      "Epoch 922/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.1003 - acc: 0.2115\n",
      "Epoch 923/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1003 - acc: 0.2115\n",
      "Epoch 924/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1003 - acc: 0.2115\n",
      "Epoch 925/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.1003 - acc: 0.2115\n",
      "Epoch 926/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.1002 - acc: 0.2115\n",
      "Epoch 927/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1002 - acc: 0.2115\n",
      "Epoch 928/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.1002 - acc: 0.2115\n",
      "Epoch 929/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.1002 - acc: 0.2115\n",
      "Epoch 930/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1001 - acc: 0.2115\n",
      "Epoch 931/1000\n",
      "52/52 [==============================] - 0s 46us/sample - loss: 2.1001 - acc: 0.2115\n",
      "Epoch 932/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.1001 - acc: 0.2115\n",
      "Epoch 933/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1001 - acc: 0.2115\n",
      "Epoch 934/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.1000 - acc: 0.2115\n",
      "Epoch 935/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.1000 - acc: 0.2115\n",
      "Epoch 936/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.1000 - acc: 0.2115\n",
      "Epoch 937/1000\n",
      "52/52 [==============================] - 0s 45us/sample - loss: 2.1000 - acc: 0.2115\n",
      "Epoch 938/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0999 - acc: 0.2115\n",
      "Epoch 939/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.0999 - acc: 0.2115\n",
      "Epoch 940/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.0999 - acc: 0.2115\n",
      "Epoch 941/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.0999 - acc: 0.2115\n",
      "Epoch 942/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0999 - acc: 0.2115\n",
      "Epoch 943/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0998 - acc: 0.2115\n",
      "Epoch 944/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0998 - acc: 0.2115\n",
      "Epoch 945/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0998 - acc: 0.2115\n",
      "Epoch 946/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.0998 - acc: 0.2115\n",
      "Epoch 947/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.0997 - acc: 0.2115\n",
      "Epoch 948/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.0997 - acc: 0.2115\n",
      "Epoch 949/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.0997 - acc: 0.2115\n",
      "Epoch 950/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0997 - acc: 0.2115\n",
      "Epoch 951/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.0997 - acc: 0.2115\n",
      "Epoch 952/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0996 - acc: 0.2115\n",
      "Epoch 953/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.0996 - acc: 0.2115\n",
      "Epoch 954/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.0996 - acc: 0.2115\n",
      "Epoch 955/1000\n",
      "52/52 [==============================] - 0s 27us/sample - loss: 2.0996 - acc: 0.2115\n",
      "Epoch 956/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.0995 - acc: 0.2115\n",
      "Epoch 957/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.0995 - acc: 0.2115\n",
      "Epoch 958/1000\n",
      "52/52 [==============================] - 0s 32us/sample - loss: 2.0995 - acc: 0.2115\n",
      "Epoch 959/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0995 - acc: 0.2115\n",
      "Epoch 960/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0995 - acc: 0.2115\n",
      "Epoch 961/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0994 - acc: 0.2115\n",
      "Epoch 962/1000\n",
      "52/52 [==============================] - 0s 54us/sample - loss: 2.0994 - acc: 0.2115\n",
      "Epoch 963/1000\n",
      "52/52 [==============================] - 0s 41us/sample - loss: 2.0994 - acc: 0.2115\n",
      "Epoch 964/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0994 - acc: 0.2115\n",
      "Epoch 965/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0994 - acc: 0.2115\n",
      "Epoch 966/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.0993 - acc: 0.2115\n",
      "Epoch 967/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0993 - acc: 0.2115\n",
      "Epoch 968/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0993 - acc: 0.2115\n",
      "Epoch 969/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0993 - acc: 0.2115\n",
      "Epoch 970/1000\n",
      "52/52 [==============================] - 0s 42us/sample - loss: 2.0993 - acc: 0.2115\n",
      "Epoch 971/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.0992 - acc: 0.2115\n",
      "Epoch 972/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0992 - acc: 0.2115\n",
      "Epoch 973/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.0992 - acc: 0.2115\n",
      "Epoch 974/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.0992 - acc: 0.2115\n",
      "Epoch 975/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.0992 - acc: 0.2115\n",
      "Epoch 976/1000\n",
      "52/52 [==============================] - 0s 25us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 977/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 978/1000\n",
      "52/52 [==============================] - 0s 31us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 979/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 980/1000\n",
      "52/52 [==============================] - 0s 33us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 981/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.0991 - acc: 0.2115\n",
      "Epoch 982/1000\n",
      "52/52 [==============================] - 0s 35us/sample - loss: 2.0990 - acc: 0.2115\n",
      "Epoch 983/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.0990 - acc: 0.2115\n",
      "Epoch 984/1000\n",
      "52/52 [==============================] - 0s 37us/sample - loss: 2.0990 - acc: 0.2115\n",
      "Epoch 985/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0990 - acc: 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0990 - acc: 0.2115\n",
      "Epoch 987/1000\n",
      "52/52 [==============================] - 0s 34us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 988/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 989/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 990/1000\n",
      "52/52 [==============================] - 0s 43us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 991/1000\n",
      "52/52 [==============================] - 0s 47us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 992/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0989 - acc: 0.2115\n",
      "Epoch 993/1000\n",
      "52/52 [==============================] - 0s 30us/sample - loss: 2.0988 - acc: 0.2115\n",
      "Epoch 994/1000\n",
      "52/52 [==============================] - 0s 36us/sample - loss: 2.0988 - acc: 0.2115\n",
      "Epoch 995/1000\n",
      "52/52 [==============================] - 0s 39us/sample - loss: 2.0988 - acc: 0.2115\n",
      "Epoch 996/1000\n",
      "52/52 [==============================] - 0s 40us/sample - loss: 2.0988 - acc: 0.2115\n",
      "Epoch 997/1000\n",
      "52/52 [==============================] - 0s 29us/sample - loss: 2.0988 - acc: 0.2115\n",
      "Epoch 998/1000\n",
      "52/52 [==============================] - 0s 38us/sample - loss: 2.0987 - acc: 0.2115\n",
      "Epoch 999/1000\n",
      "52/52 [==============================] - 0s 51us/sample - loss: 2.0987 - acc: 0.2115\n",
      "Epoch 1000/1000\n",
      "52/52 [==============================] - 0s 28us/sample - loss: 2.0987 - acc: 0.2115\n"
     ]
    }
   ],
   "source": [
    "history = ANN.fit(inputVector, outputVector, epochs=1000, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseVector2 = ANN.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for w, v in zip(vocabulary, denseVector2):\n",
    "    plt.text(v[0], v[1], w)\n",
    "    \n",
    "plt.xlim(-1, 3)\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "    \n",
    "def angle(x1, x2):\n",
    "    return x1.dot(x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young 2.3008218 0.8703023\n",
      "strong 2.915856 0.0026436814\n",
      "boy 3.4228528 -0.99889237\n",
      "girl 4.114057 -0.78198373\n",
      "pretty 3.5204318 -0.9925607\n",
      "princess 2.733967 0.38300127\n",
      "wise 2.5185943 0.7819386\n",
      "king 3.0563755 -0.52967477\n",
      "prince 3.365548 -0.8165542\n",
      "man 2.98065 -0.19347857\n",
      "queen 1.5923182 0.82772464\n",
      "woman 2.3447592 0.883697\n"
     ]
    }
   ],
   "source": [
    "result = denseVector2\n",
    "\n",
    "# query = result[word2ind(\"queen\")]\n",
    "query = result[word2ind(\"queen\")] - result[word2ind(\"girl\")] + result[word2ind(\"king\")]\n",
    "\n",
    "for i in range(len(result)):\n",
    "    print(vocabulary[i], distance(query, result[i]), angle(query, result[i]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
