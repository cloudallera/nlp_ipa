{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#정보검색\" data-toc-modified-id=\"정보검색-1\">정보검색</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-1.1\">Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Web-Crawling-and-Basic-Text-Analysis(계속)\" data-toc-modified-id=\"Web-Crawling-and-Basic-Text-Analysis(계속)-1.1.1\">Web Crawling and Basic Text Analysis(계속)</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-does-it-work\" data-toc-modified-id=\"How-does-it-work-1.1.1.1\">How does it work</a></span></li><li><span><a href=\"#Zipf's-law\" data-toc-modified-id=\"Zipf's-law-1.1.1.2\">Zipf's law</a></span></li><li><span><a href=\"#Abstraction-of-search-engine-architecture(crawling-and-doc-ananlyzer-detail)\" data-toc-modified-id=\"Abstraction-of-search-engine-architecture(crawling-and-doc-ananlyzer-detail)-1.1.1.3\">Abstraction of search engine architecture(crawling and doc ananlyzer detail)</a></span></li><li><span><a href=\"#What-you-should-know\" data-toc-modified-id=\"What-you-should-know-1.1.1.4\">What you should know</a></span></li></ul></li><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-1.1.2\">Inverted Index</a></span></li></ul></li><li><span><a href=\"#색인기법(계속)\" data-toc-modified-id=\"색인기법(계속)-1.2\">색인기법(계속)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-text-indexing-:-Bag-of-Words-representation\" data-toc-modified-id=\"Full-text-indexing-:-Bag-of-Words-representation-1.2.1\">Full text indexing : Bag-of-Words representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words-representation-구현-절차\" data-toc-modified-id=\"Bag-of-Words-representation-구현-절차-1.2.1.1\">Bag-of-Words representation 구현 절차</a></span></li><li><span><a href=\"#lexicon-만들기-:-list\" data-toc-modified-id=\"lexicon-만들기-:-list-1.2.1.2\">lexicon 만들기 : list</a></span></li><li><span><a href=\"#lexicon-만들기(속도-개선)-:-list-=&gt;-set\" data-toc-modified-id=\"lexicon-만들기(속도-개선)-:-list-=>-set-1.2.1.3\">lexicon 만들기(속도 개선) : list =&gt; set</a></span></li><li><span><a href=\"#BOW-만들기-:-list\" data-toc-modified-id=\"BOW-만들기-:-list-1.2.1.4\">BOW 만들기 : list</a></span></li><li><span><a href=\"#BOW-속도-개선(1)-:-list-=&gt;-dict\" data-toc-modified-id=\"BOW-속도-개선(1)-:-list-=>-dict-1.2.1.5\">BOW 속도 개선(1) : list =&gt; dict</a></span></li><li><span><a href=\"#BOW-속도-개선(2)-:-dict-=&gt;-defaultdict,-lexicon-사전-비교-안하기\" data-toc-modified-id=\"BOW-속도-개선(2)-:-dict-=>-defaultdict,-lexicon-사전-비교-안하기-1.2.1.6\">BOW 속도 개선(2) : dict =&gt; defaultdict, lexicon 사전 비교 안하기</a></span></li></ul></li><li><span><a href=\"#DTM-=&gt;-TDM-변환\" data-toc-modified-id=\"DTM-=>-TDM-변환-1.2.2\">DTM =&gt; TDM 변환</a></span></li><li><span><a href=\"#서비스-구현-시-추가-고려사항-(Postings-list)\" data-toc-modified-id=\"서비스-구현-시-추가-고려사항-(Postings-list)-1.2.3\">서비스 구현 시 추가 고려사항 (Postings list)</a></span></li><li><span><a href=\"#Controlled-Vocabulary-Set-만들기\" data-toc-modified-id=\"Controlled-Vocabulary-Set-만들기-1.2.4\">Controlled Vocabulary Set 만들기</a></span><ul class=\"toc-item\"><li><span><a href=\"#뉴스기사에서-불필요한-단어-지우기\" data-toc-modified-id=\"뉴스기사에서-불필요한-단어-지우기-1.2.4.1\">뉴스기사에서 불필요한 단어 지우기</a></span></li></ul></li><li><span><a href=\"#Inverted-Index-(역문헌-구조)\" data-toc-modified-id=\"Inverted-Index-(역문헌-구조)-1.2.5\">Inverted Index (역문헌 구조)</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정보검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling and Basic Text Analysis(계속)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### How does it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- In pseudo code\n",
    "\n",
    "~~~\n",
    "Def Crawler(entry_point) {\n",
    "    URL_list = [entry_point]\n",
    "    while (len(URL_list)>0) {\n",
    "            URL = URL_list.pop();\n",
    "            if (isVisited(URL) or !isLegal(URL) or !checkRobotsTxt(URL))\n",
    "                  continue;\t\t\t\t\n",
    "            HTML = URL.open();\t\n",
    "            for (anchor in HTML.listOfAnchors()) {\n",
    "                   URL_list.append(anchor);\n",
    "             }\n",
    "             setVisited(URL);\n",
    "             insertToIndex(HTML);\n",
    "    }\n",
    "}\n",
    "~~~\n",
    "\n",
    "![pseudo](./images/pseudo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 영어 Brown Corpus의 경우, \"the\"가 7%, 3.5% 빈도로 나타남\n",
    "\n",
    "![zipfs_law](./images/zipfs_law2.png)\n",
    "\n",
    "- 고빈도와 저빈도를 제외하면 10% 정도 밖에 안됨. BOW의 90% space가 낭비되고 있음<br>\n",
    "   실제 단어가 나타나는 부분만 저장하는 구조로 변경 필요 => 해결책 : Inverted Document Index (Linked List) 사용\n",
    "\n",
    "\n",
    "- BOW에서 \"1\"이 연속되거나(고빈도), \"0\"이 연속되는(rare, 저빈도) 단어는 zipf's law에 따라 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Abstraction of search engine architecture(crawling and doc ananlyzer detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![abstraction_detail](./images/abstraction_detail.png)\n",
    "\n",
    "- 영어의 경우 Stemming을 하지 않음 : people, location and organization 등(공개된 자료가 없음)\n",
    "   -> Named entity recognition으로 대체됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### What you should know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Basic techniques for crawling : Visiting strategy(Breadth first, Depthfirst vs. Focused crawling), Avoid duplicate visit,<br>\n",
    "   Politeness policy, Re-visit policy, Analyze crawled web pages, HTML parsing(html.parser, jsoup/json)\n",
    "- Zipf’s law\n",
    "- Procedures for automatic text indexing : Tokenization, Normalization, Stemming, Stopwords\n",
    "- **Bag-of-Words document representation**\n",
    "\n",
    "![BOW](./images/BOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hash table, B-tree, trie, ...\n",
    "\n",
    "![inverted_index](./images/inverted_index.png)\n",
    "\n",
    "- D는 문서의 전체 단어 크기이지만, |L|은 zipf's law에 따라 전체의 10%만 사용함\n",
    "- python에서는 document의 index number를 pointer 처럼 사용하여 linked list를 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 색인기법(계속)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Full text indexing : Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Assumption: word is independent from each other\n",
    "- Pros: simple\n",
    "- Cons: grammar and order are missing\n",
    "- The most frequently used document representation : Image, speech, gene sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Bag-of-Words representation 구현 절차\n",
    "\n",
    "1. 크롤링\n",
    "1. collection => document 집합\n",
    "1. Word split => Tokenize, Preprocessing\n",
    "1. Lexicon Dictionary (|V|) => 전체 문서의 token 리스트\n",
    "1. 문서 표현 (by BOW) => Lexicon Dictionary의 token이 해당 문서에 있으면 1, 없으면 0으로 표현된 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### lexicon 만들기 : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "\n",
    "def getLexicon():    # list 사용\n",
    "    lexicon = list()\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "            if token not in lexicon:\n",
    "                lexicon.append(token)\n",
    "                \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getLexicon()   # 속도가 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### lexicon 만들기(속도 개선) : list => set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "\n",
    "def getLexicon():    # set 사용\n",
    "    lexicon = set()\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "                lexicon.add(token)\n",
    "                \n",
    "    return list(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.41 ms ± 470 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getLexicon()   # set을 사용해서 if문을 없앴기 때문에 속도가 개선됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2638"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = getLexicon()\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### BOW 만들기 : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 단어사전: [0:단어1, 1:단어2, 2:단어3, ...]\n",
    "# 문서:     0:  [0:0 or 1, ...] => 문서에 있으면 1, 없으면 0으로 표현\n",
    "#              1: \n",
    "# 문서목록 : [0:문서1, 1:문서2, ...]\n",
    "\n",
    "# 문서는 모두 서로 다르다고 가정 : 같은 문서가 있는 경우 set을 사용할 수 없음\n",
    "\n",
    "def getDocRepr(lexicon):\n",
    "    docList = list()    # 문서목록\n",
    "    docRepr = list()   # 문서표현 of BOW의 집합 => 문서 갯수 만큼\n",
    "    # 0: BOW\n",
    "    # 1: BOW\n",
    "    # 2: BOW\n",
    "    # ...\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        docList.append(docName)\n",
    "        docVector = list(0 for _ in range(len(lexicon)))   # 문서 1개에 대한 문서표현(BOW) = [0] * 단어의 갯수\n",
    "\n",
    "        for token in document.split():\n",
    "            if token in lexicon:\n",
    "                docVector[lexicon.index(token)] = 1   # 문서 사전에 있으면, BOW 표현을 1로 변경\n",
    "\n",
    "        docRepr.append(docVector)\n",
    "        \n",
    "    return docList, docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, '7.', 2638)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docList, docRepr = getDocRepr(lexicon)\n",
    "len(docList), len(docRepr), lexicon[2], len(docRepr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#docRepr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 ms ± 37.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocRepr(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### BOW 속도 개선(1) : list => dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 문제점 : 전체 리스트에서 0인 부분이 너무 많고, 문서가 많아질수록 더 심해짐\n",
    "- 개선방안 : BOW를 list가 아닌 dictionary 형태로 변경(token이 있는 부분만 저장) <br>\n",
    "   ==> lexicon dicitionary에서 비교하지 않고, token을 직접 key로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# BOW dictrionary 표현\n",
    "\n",
    "def getDocReprByDict(lexicon):\n",
    "    # docList = list()    # 문서목록 => 필요 없음\n",
    "    docRepr = dict()   # 문서표현 of BOW의 Dictionary\n",
    "    # key => 문서\n",
    "    # Value => BOW => list x, dict\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        docRepr[docName] = dict()\n",
    "\n",
    "        for token in document.split():\n",
    "            if token in lexicon:\n",
    "                docRepr[docName][lexicon.index(token)] = 1   # 문서 사전에 있으면, BOW 표현을 1로 변경\n",
    "\n",
    "        \n",
    "    return docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docRepr = getDocReprByDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, '1809896.txt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docRepr), list(docRepr.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '가.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docRepr['1809896.txt'][1328], lexicon[1328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 ms ± 33.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocReprByDict(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### BOW 속도 개선(2) : dict => defaultdict, lexicon 사전 비교 안하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# defaultdict로 변경\n",
    "from collections import defaultdict\n",
    "\n",
    "def getDocReprByDefaultDict(lexicon):\n",
    "    docRepr = defaultdict(lambda: defaultdict(int))   # 문서표현 of BOW의 Dictionary\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        for token in document.split():\n",
    "            docRepr[docName][token] = 1   # 문서 사전을 비교하지 않고, token을 key로 바로 사용 => BOW 표현을 1로 변경\n",
    "            # lexicon 사전을 만들 때 이미 해당 문서의 모든 token을 포함시겼기 때문에 비교하지 않고 token을 바로 key로 사용함\n",
    "        \n",
    "    return docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docRepr = getDocReprByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, '1809896.txt')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docRepr), list(docRepr.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '총')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docRepr['1809896.txt'][100], lexicon[100]    # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 ms ± 22.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocReprByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DTM => TDM 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 검색 => Input(키워드들의 집합 = Query) <br>\n",
    "   키워드들은 단어로 구성, 단어를 추출\n",
    "\n",
    "\n",
    "- Q = \\[\"국회\", \"의원\"]  이라면,\n",
    "\n",
    "\n",
    "- |Q| = q, |D| = d, |V| = v 라면,<br>\n",
    "   q**d, **v   ==> 너무 과다한 계산 (실행 불가)\n",
    "\n",
    "~~~\n",
    "for 단어 in Q:<br>\n",
    "    for 전체 문서를 대상:<br>\n",
    "            for 문서 내 단어 :<br>\n",
    "                if 단어 == 사전:<br>\n",
    "                    이때의 해당 문서가 검색 후보\n",
    "~~~\n",
    "\n",
    "---------\n",
    "- 이를 개선하기 위해, InvertedDocument (역문헌 구조)를 사용\n",
    "\n",
    "\n",
    "- key: 단어 in 사전,<br>\n",
    "   value: 어느 문서에서 나왔는지, 몇 번(, 어느 위치 등)\n",
    "\n",
    "~~~\n",
    "for 단어 in Q:\n",
    "    # for 전체 문서를 대상:  # 전체 문서를 대상으로 찾을 필요 없음\n",
    "    #        for 문서 내 단어 :\n",
    "    #            if 단어 == 사전:\n",
    "    단어 in InvertedDocument.keys():\n",
    "        이때의 해당 문서가 검색 후보\n",
    "~~~\n",
    "\n",
    "----------\n",
    "- **DTM(Document-Term Matrix)**\n",
    "\n",
    "| Doc  | W1 | W1 | W3 | W4 | W5 | ... | Wn |\n",
    "|------|----|----|----|----|----|---|---|\n",
    "|Doc1 | 0 | 1 | 0 | 1 | 0 | ... | 0 |\n",
    "|Doc2 |\n",
    "|... |\n",
    "|Doc10|\n",
    "\n",
    "- **DTM => TDM으로 변경** (리스트 x, 딕셔너리 o => 공간을 줄일려고)\n",
    "\n",
    "\n",
    "- **TDM(Term-Document Matrix)**\n",
    "\n",
    "|Word | D1 | D1 | ... | D10 |\n",
    "|------|---|---|---|---|\n",
    "|W1 | 0 |\n",
    "|W2 | 1 | \n",
    "|W3 | 0 |\n",
    "|W4 | 1 |\n",
    "|W5 | 0 |\n",
    "|...    |\n",
    "|Wn | 0 |\n",
    "\n",
    "----------\n",
    "- **Query**의 경우에도:<br>\n",
    "   문자들의 검색 결과를 one-hot 벡터로 합해서 표현하면 됨\n",
    "\n",
    "|Query | W1 | W1 | W3 | W4 | W5 | ... | Wn |\n",
    "|------|----|----|----|----|----|----|----|\n",
    "|Q1 | 0 | 1 | 0 | 1 | 0 | ... | 0 |\n",
    "|Q2 |\n",
    "|Q3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DTM을 TDM으로 변경\n",
    "def convertInvertedDocument(DTM):\n",
    "    TDM = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for fileName, termList in DTM.items():   # DTM[fileName][term]\n",
    "        for term, freq in termList.items():\n",
    "            TDM[term][fileName] = freq\n",
    "            \n",
    "    return TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TDM = convertInvertedDocument(docRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {'1809897.txt': 1, '1809898.txt': 1}),\n",
       " defaultdict(int, {'1809896.txt': 1}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean 검색\n",
    "TDM[\"국회\"], TDM[\"의원\"]  # Boolean Model 이라고 함 (&, |, not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'1809897.txt': 1, '1809898.txt': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM[\"국회\"] or TDM[\"의원\"]  # Boolean Model 이라고 함 (&, |, not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 서비스 구현 시 추가 고려사항 (Postings list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 실제 서비스로 구현하기 위해서는 POSTING 파일을 따로 만들어서 in-memory에 올려서 처리해야 함 (hash 파일 구조 DB)<br>\n",
    "   (python 코드는 구현은 쉽지만, 서비스 속도에 문제가 있음)\n",
    "\n",
    "\n",
    "- 단어 -> 포스트파일 위치\n",
    "\n",
    "    POSTING-구조체 (파일DB) <br>\n",
    "    위치:\\[어느문서:면번, 다음위치:...\\]\\[어느문서:면번, 다음위치:...\\]위치:\\[어느문서:면번, 다음위치:...\\]...\n",
    "\n",
    "\n",
    "- (inverted document) Postings list (Lucene)\n",
    "\n",
    "![posting](./images/posting_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Controlled Vocabulary Set 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- BOW 특성상 정확히 일치하는 단어만을 인식함 -> 어절만 적용 시 구두점까지 정확히 일치하는 경우만 인식됨. 띄어쓰기 안되어 있는 경우, 품사적인 중의성 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 어절, 특정 품사와 단어 길이 제한, Ngram(어절 or 음절) 등을 적용한 set() 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'NNG'), ('날씨', 'NNG'), ('는', 'JX'), ('어제', 'NNG'), ('날씨', 'NNG'), ('보다', 'JKM'), ('안', 'MAG'), ('좋', 'VA'), ('은', 'ETD'), ('거', 'NNB'), ('같', 'VA'), ('아요', 'EFN')]\n",
      "['오늘', '날씨', '어제', '거']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘 날씨는 어제 날씨 보다 안좋은거 같아요\"\n",
    "print(Kkma().pos(sentence))\n",
    "print(Kkma().nouns(sentence))   # 빈도에 대한 정보가 사라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘']\n",
      "['날씨']\n",
      "['어제']\n",
      "['날씨']\n",
      "[]\n",
      "['거']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for token in sentence.split():   # word_tokenize(sentence)\n",
    "    print(Kkma().nouns(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('거', 'NNB')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1].startswith(\"N\")]:   # 명사만\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('좋', 'VA')\n",
      "('같', 'VA')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"]]:  # 명사와 형용사(용언)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:  # 명사, 형용사, 길이 제한\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['보다', '안좋은거', '날씨는', '오늘', '같아요', '어제', '날씨']\n"
     ]
    }
   ],
   "source": [
    "lexicon = list(set(sentence.split()))\n",
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:\n",
    "    lexicon.append(token[0])\n",
    "\n",
    "print(list(set(lexicon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functions.nlp import ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘 날씨는', '날씨는 어제', '어제 날씨', '날씨 보다', '보다 안좋은거', '안좋은거 같아요']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.ngramEojeol(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ngram.ngramUmjeol(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  ['안좋은거', '날씨는', '오늘', '같아요', '어제', '날씨보다']\n",
      "2.  ['안좋은거', '날씨', '날씨는', '오늘', '같아요', '어제', '날씨보다']\n",
      "3.  ['안좋은거', '날씨는', '오늘', '같아요', '어제', '날씨보다', '오늘', '날씨', '어제', '날씨']\n",
      "4.  ['어제 날씨보다', '안좋은거', '날씨는 어제', '오늘 날씨는', '날씨보다 안좋은거', '안좋은거 같아요', '날씨', '날씨는', '오늘', '같아요', '어제', '날씨보다']\n",
      "5.  ['어제 날씨보다', '안좋은거', '날씨 날씨', '날씨는 어제', '오늘 날씨는', '날씨보다 안좋은거', '안좋은거 같아요', '날씨', '날씨는', '오늘', '같아요', '어제', '날씨보다']\n",
      "6.  ['아요', '오늘 날씨는', '어제', '보다', '어제 날씨보다', '씨보', '안좋은거', '날씨 날씨', '날씨', '날씨보다', '씨는', '날씨는 어제', '날씨보다 안좋은거', '은거', '날씨는', '같아', '안좋', '좋은', '안좋은거 같아요', '오늘', '같아요']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘 날씨는 어제 날씨보다 안좋은거 같아요\"\n",
    "\n",
    "lexicon = list(set(sentence.split()))\n",
    "print(\"1. \", lexicon)\n",
    "\n",
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:\n",
    "    lexicon.append(token[0])\n",
    "print(\"2. \", list(set(lexicon)))\n",
    "\n",
    "print(\"3. \", lexicon)\n",
    "\n",
    "lexicon.extend(ngram.ngramEojeol(sentence))\n",
    "print(\"4. \", list(set(lexicon)))\n",
    "\n",
    "lexicon.extend(ngram.ngramEojeol(\" \".join(token[0] for token in [token for word in Kkma().pos(sentence) if word[1] in  [\"NNG\", \"VA\"] and len(word[0]) > 1])))\n",
    "print(\"5. \", list(set(lexicon)))\n",
    "\n",
    "newLexicon = list()\n",
    "for word in lexicon:\n",
    "    if len(word.split()) == 1:\n",
    "        newLexicon.extend(ngram.ngramUmjeol(word))\n",
    "lexicon.extend(newLexicon)\n",
    "print(\"6. \", list(set(lexicon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 6번 lexicon의 경우 query에 어떤 형태의 단어가 입력되어도 찾아질 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 뉴스기사에서 불필요한 단어 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functions.naver import NewsScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "news = NewsScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus = news.get_article(\"정치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\"대구, 로봇산업 중심지로 발전할 역량 충분\"전국경제투어 일곱번째 순서로 대구광역시 방문문재인 대통령이 지난 1월 29일 서울 동대문 디자인플라자에서 열린 한국 전자·IT산업 융합 전시회를 방문해 네이버랩스의 지능형 로봇팔과 악수하고 있다. (청와대 제공) 2019.1.29/뉴스1(서울=뉴스1) 홍기삼 기자 = 문재인 대통령은 \"로봇산업을 통해 대구가 제조업의 중심지로 부활할 것\"이라고 말했다.문 대통령은 22일 오전 전국경제투어 일곱번째 순서로 대구광역시를 방문, 달성군 현대로보틱스에서 열린 \\'로봇산업 육성전략 보고회\\'에 참석해 이같이 밝혔다.문 대통령은 \"대구도 로봇산업을 통해 제조업의 중심지로 부활할 것\"이라며 \"대구의 꿈을 정부가 적극 지원하고 대한민국이 꿈꾸는 로봇산업의 미래가 대구에서 제일 먼저 펼쳐지도록 대구시민들께서 힘을 모아주시기 바란다\"라고 당부했다.문 대통령은 \"국내 유일의 로봇산업진흥기관 \\'한국로봇산업진흥원\\'이 대구에 있고 로봇산업클러스터가 조성돼 국내 1위의 로봇기업, 세계 3위의 글로벌 로봇기업을 포함해 수도권을 벗어나 로봇기업이 가장 많은 도시\"라며 \"대구가 로봇산업 중심지로 발전할 역량이 충분하다\"고 설명했다.문 대통령은 \"로봇이 발전할수록 사람의 역할이 커져야 한다\"며 \"대구에서 로봇을 도입한 뒤 기업 매출이 늘고 일자리가 만들어진 경우가 많다. 약 80여 대의 로봇을 도입해 프레스, 용접공정을 자동화했지만 오히려 생산기술과 개발, 연구 인력을 신규 채용한 기업도 있다\"고 소개했다.이를 위해 고위험, 고강도, 유해 작업환경에 로봇이 널리 활용되도록 하고 낮은 가격의 협동 로봇을 집중적으로 지원해 영세 중소기업의 생산성을 향상시키고 노동자가 좀 더 안전한 환경에서 일하며 삶의 질을 높일 수 있도록 하겠다고 전했다.문 대통령은 \"물류, 의료, 가사 로봇 같은 서비스 로봇을 통해 인간의 삶을 돕도록 하겠다\"라며 \"서비스 로봇이 상용화돼 의료와 재활, 돌봄과 재난대응을 비롯한 다양한 기능을 수행하게 되면 사회적 약자를 비롯한 누구나 안전하고 편리한 삶을 누리게 될 것\"이라고 전했다.아울러 \"정부는 \\'사람을 위한 로봇 산업\\'이라는 원칙 아래 2023년 로봇산업 글로벌 4대 강국을 목표로 삼았다\"라며 \"작지만 강한, 세계적인 스타 기업 20개를 만들어내 정부부터 로봇 보급과 확산의 마중물이 될 것\"이라고 다짐했다.문 대통령은 \"대구시가 미래 신성장산업으로 선정한 물, 의료, 에너지, 미래형자동차, 스마트시티 산업은 로봇 산업과 접목될 때 시너지가 더 커질 수 있다\"라며 \"대구가 대한민국 로봇산업의 심장으로 힘차게 뛸 때, 대구경제가 살아나고 대한민국 로봇산업도 한 차원 성장해 나갈 것\"이라고 맺었다.argus@news1.kr▶ [ 크립토허브 ]  [ 터닝포인트 2019 ] ▶ 네이버 메인에서 [뉴스1] 구독하기![© 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지]\\n\\n\\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def getFileList(base=\"./naver_news/2019-03-20_02-07/\", ext=\"txt\"):\n",
    "#     fileList = list()\n",
    "    \n",
    "#     for file in os.listdir(base):\n",
    "#         if file.split(\".\")[-1] == ext:\n",
    "#             fileList.append(\"{0}/{1}\".format(base, file))\n",
    "    \n",
    "#     return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def getContent(file):\n",
    "#     content = \"\"\n",
    "    \n",
    "#     with open(file, encoding=\"utf-8\") as f:\n",
    "#         content = f.read()\n",
    "    \n",
    "#     return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coupus = getContent(getFileList()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\n\\n\\n통계청, ‘2018년 한국의 사회지표’ 발표 게티이미지뱅크국민 중 결혼이 필요하다고 여기는 응답자 비율이 20년 만에 처음으로 50%를 밑돌았다.',\n",
       " '반면 국민 중 절반 이상은 남녀 동거에 찬성했다.',\n",
       " '저출산과 수명 연장에 따라 노인 인구가 유소년 인구보다 많아지는 현상은 더욱 심화되고 있다.',\n",
       " '◇미혼 여성 10명 중 2명만 ‘결혼 필요’ 통계청이 22일 발표한 ‘2018년 한국의 사회지표’에 따르면, 지난해 13세 이상 국민 중 ‘결혼은 해야 한다’고 생각하는 사람의 비율은 2년 전(51.9%)보다 3.8%포인트 하락한 48.1%를 기록했다.',\n",
       " '통계를 처음 작성한 1998년 당시 이 비율은 73.5%에 달했지만, 2014년(56.8%) 50%대에 진입한 뒤, 급기야 50%선마저 무너진 것이다.']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus =news.get_article(\"경제\")\n",
    "sent_tokenize(corpus)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n통계청, ‘2018년 한국의 사회지표’ 발표 게티이미지뱅크국민 중 결혼이 필요하다고 여기는 응답자 비율이 20년 만에 처음으로 50%를 밑돌았다. 반면 국민 중 절반 이상은 남녀 동거에 찬성했다. 저출산과 수명 연장에 따라 노인 인구가 유소년 인구보다 많아지는 현상은 더욱 심화되고 있다. ◇미혼 여성 10명 중 2명만 ‘결혼 필요’ 통계청이 22일 발표한 ‘2018년 한국의 사회지표’에 따르면, 지난해 13세 이상 국민 중 ‘결혼은 해야 한다’고 생각하는 사람의 비율은 2년 전(51.9 보다 3.8%포인트 하락한 48.1%를 기록했다. 통계를 처음 작성한 1998년 당시 이 비율은 73.5%에 달했지만, 2014년(56.8  50%대에 진입한 뒤, 급기야 50%선마저 무너진 것이다. 특히 미혼 남녀에서 하락 폭이 컸다. 미혼 남성 중 결혼을 해야 한다고 생각하는 사람의 비율은 2012년 60.4%→2014년 51.8%→2016년 42.9%에 이어, 지난해 36.3%까지 하락했다. 미혼 여성 또한 이 비율이 2년 전(31.0 보다 8.6%포인트 떨어진 22.4%를 기록했다. 미혼 남녀 10명 중 겨우 2~3명 남짓만이 결혼 필요성을 느끼고 있는 셈이다. 반면 남녀 비혼 동거에 대한 심리적 장벽은 낮아졌다. 지난해 동거에 ‘동의한다’고 답한 비율은 56.4%로 2년 전(48.0 보다 8.4%포인트나 높아지며, 처음으로 50%선을 넘었다. 특히 20대(74.4  30대(73.2  등 젊은 연령층일수록 비혼 동거를 긍정적으로 인식했다. [저작권 한국일보]최근 10년간 혼인 추이_김경진기자◇노인 비중, 갈수록 유소년 앞질러지난해 전체 인구(5,163만5,000명) 중 65세 이상 고령층이 차지하는 비중은 14.3%에 달했다. 반면 14세 이하 유소년 비중은 12.9%였다. 2017년 고령층 비중(13.8 이 유소년(13.1 을 처음으로 추월한 뒤, 격차가 확대된 것이다. 이 같은 현상은 계속 심화될 것으로 전망된다. 지난해 태어난 출생아 수는 32만6,900명으로 역대 최저치를 기록한 반면, 의료기술 발달 등에 따른 수명 연장으로 65세 이상 인구는 계속 늘고 있기 때문이다. 2017년 기준 우리 국민의 기대수명은 82.7년으로 10년 전인 2007년(79.2년)보다 3.5년 증가했다. 한편 2017년 기준 19세 이상 성인의 흡연율은 21.1%로, 전년보다 1.5%포인트 하락했다. 반면 같은 기간 고(高)위험 음주율(1회 평균 음주량이 남자 7잔 이상, 여자 5잔 이상이며 주 2회 이상 음주)은 증가(13.2→13.4 했다. 성인의 유산소 신체활동 실천율(주당 2시간 30분 이상 운동)은 46.4%로 전년보다 1.4%포인트 감소했다. 다만 체질량지수(BMI)가 25 이상인 비만 유병률은 34.8%로 0.7%포인트 줄었다. 세종=박준석 기자 pjs@hankookilbo.com▶한국일보 [페이스북] [카카오 친구맺기]▶네이버 채널에서 한국일보를 구독하세요!\\n\\n\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern = re.compile(r\"[/]{2,}\")   # \"/\"가 2번 이상 반복되는 경우\n",
    "pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "pattern.findall(corpus)\n",
    "pattern.sub(\" \", corpus)    # %) 처럼 다른 pucctuation이 연속되는 경우에도 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n\\n\\n\\n\\n\\n', '\\n\\n\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 통계청, ‘2018년 한국의 사회지표’ 발표 게티이미지뱅크국민 중 결혼이 필요하다고 여기는 응답자 비율이 20년 만에 처음으로 50%를 밑돌았다. 반면 국민 중 절반 이상은 남녀 동거에 찬성했다. 저출산과 수명 연장에 따라 노인 인구가 유소년 인구보다 많아지는 현상은 더욱 심화되고 있다. ◇미혼 여성 10명 중 2명만 ‘결혼 필요’ 통계청이 22일 발표한 ‘2018년 한국의 사회지표’에 따르면, 지난해 13세 이상 국민 중 ‘결혼은 해야 한다’고 생각하는 사람의 비율은 2년 전(51.9%)보다 3.8%포인트 하락한 48.1%를 기록했다. 통계를 처음 작성한 1998년 당시 이 비율은 73.5%에 달했지만, 2014년(56.8%) 50%대에 진입한 뒤, 급기야 50%선마저 무너진 것이다. 특히 미혼 남녀에서 하락 폭이 컸다. 미혼 남성 중 결혼을 해야 한다고 생각하는 사람의 비율은 2012년 60.4%→2014년 51.8%→2016년 42.9%에 이어, 지난해 36.3%까지 하락했다. 미혼 여성 또한 이 비율이 2년 전(31.0%)보다 8.6%포인트 떨어진 22.4%를 기록했다. 미혼 남녀 10명 중 겨우 2~3명 남짓만이 결혼 필요성을 느끼고 있는 셈이다. 반면 남녀 비혼 동거에 대한 심리적 장벽은 낮아졌다. 지난해 동거에 ‘동의한다’고 답한 비율은 56.4%로 2년 전(48.0%)보다 8.4%포인트나 높아지며, 처음으로 50%선을 넘었다. 특히 20대(74.4%) 30대(73.2%) 등 젊은 연령층일수록 비혼 동거를 긍정적으로 인식했다. [저작권 한국일보]최근 10년간 혼인 추이_김경진기자◇노인 비중, 갈수록 유소년 앞질러지난해 전체 인구(5,163만5,000명) 중 65세 이상 고령층이 차지하는 비중은 14.3%에 달했다. 반면 14세 이하 유소년 비중은 12.9%였다. 2017년 고령층 비중(13.8%)이 유소년(13.1%)을 처음으로 추월한 뒤, 격차가 확대된 것이다. 이 같은 현상은 계속 심화될 것으로 전망된다. 지난해 태어난 출생아 수는 32만6,900명으로 역대 최저치를 기록한 반면, 의료기술 발달 등에 따른 수명 연장으로 65세 이상 인구는 계속 늘고 있기 때문이다. 2017년 기준 우리 국민의 기대수명은 82.7년으로 10년 전인 2007년(79.2년)보다 3.5년 증가했다. 한편 2017년 기준 19세 이상 성인의 흡연율은 21.1%로, 전년보다 1.5%포인트 하락했다. 반면 같은 기간 고(高)위험 음주율(1회 평균 음주량이 남자 7잔 이상, 여자 5잔 이상이며 주 2회 이상 음주)은 증가(13.2→13.4%)했다. 성인의 유산소 신체활동 실천율(주당 2시간 30분 이상 운동)은 46.4%로 전년보다 1.4%포인트 감소했다. 다만 체질량지수(BMI)가 25 이상인 비만 유병률은 34.8%로 0.7%포인트 줄었다. 세종=박준석 기자 pjs@hankookilbo.com▶한국일보 [페이스북] [카카오 친구맺기]▶네이버 채널에서 한국일보를 구독하세요! '"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# white space가 2번 이상 나타나는 경우\n",
    "pattern = re.compile(r\"\\s{2,}\")\n",
    "print(pattern.findall(corpus))\n",
    "pattern.sub(\" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hankookilbo']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern = re.compile(r\"\\b(.+)\\b\")\n",
    "pattern = re.compile(r\"[A-Za-z-_]{8,}\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pjs@hankookilbo.com', '.com')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이메일 제거\n",
    "pattern = re.compile(r\"(\\w+@[a-zA-Z0-9\\-\\_]{3,}(.[a-zA-Z]{2,})+)\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BMI', 'MI'), ('pjs@hankookilbo.com', '.com')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url 제거\n",
    "pattern = re.compile(r\"([a-zA-Z]{1,}(.[a-zA-Z]{1,})+)\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 한글 이외에 모두 제거\n",
    "pattern = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣]+)\")\n",
    "#pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getPatternList():\n",
    "    patternList = {}\n",
    "    patternList[\"Korean\"] = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣]+)\")\n",
    "    patternList[\"Email\"] = re.compile(r\"(\\w+@[a-zA-Z0-9\\-\\_]{3,}(.[a-zA-Z]{2,})+)\")\n",
    "    patternList[\"Whitespace\"] = re.compile(r\"\\s{2,}\")\n",
    "    patternList[\"Punctuation\"] =  re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "    return patternList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus = news.get_article(\"정치\")\n",
    "corpus = getPatternList()[\"Korean\"].sub(\" \", corpus)\n",
    "corpus = getPatternList()[\"Punctuation\"].sub(\" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kma = Kkma()\n",
    "\n",
    "dictTerm = list()\n",
    "dictPOS = list()\n",
    "dictNoun = list()\n",
    "dictNgram = list()\n",
    "\n",
    "for sentence in sent_tokenize(corpus):\n",
    "    for token in sentence.split():\n",
    "        if len(token) > 1:\n",
    "            dictTerm.append(token)\n",
    "            dictPOS.extend([morpheme for morpheme in kma.morphs(token) if len(morpheme) > 1])\n",
    "            dictNoun.extend([noun for noun in kma.nouns(token) if len(noun) > 1])\n",
    "            dictNgram.extend(ngram.ngramUmjeol(token))\n",
    "        \n",
    "dictTerm = list(set(dictTerm))\n",
    "dictPOS = list(set(dictPOS))\n",
    "dictNoun = list(set(dictNoun))\n",
    "dictNgram = list(set(dictNgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 197, 176, 414)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictTerm), len(dictPOS), len(dictNoun), len(dictNgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(dictTerm + dictPOS + dictNoun+ dictNgram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index (역문헌 구조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
