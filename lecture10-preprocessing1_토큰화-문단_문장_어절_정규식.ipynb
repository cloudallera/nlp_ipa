{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NLP-개요\" data-toc-modified-id=\"NLP-개요-1\">NLP 개요</a></span><ul class=\"toc-item\"><li><span><a href=\"#자연어-분석-:-기반-기술\" data-toc-modified-id=\"자연어-분석-:-기반-기술-1.1\">자연어 분석 : 기반 기술</a></span></li><li><span><a href=\"#응용-기술-:-모델을-접목한-도메인-Specific-기술\" data-toc-modified-id=\"응용-기술-:-모델을-접목한-도메인-Specific-기술-1.2\">응용 기술 : 모델을 접목한 도메인 Specific 기술</a></span></li></ul></li><li><span><a href=\"#KoNLPy,-NLTK\" data-toc-modified-id=\"KoNLPy,-NLTK-2\">KoNLPy, NLTK</a></span><ul class=\"toc-item\"><li><span><a href=\"#KoNLPy-패키지-설치\" data-toc-modified-id=\"KoNLPy-패키지-설치-2.1\">KoNLPy 패키지 설치</a></span></li><li><span><a href=\"#KoNLPy-import,-POS-tagging\" data-toc-modified-id=\"KoNLPy-import,-POS-tagging-2.2\">KoNLPy import, POS tagging</a></span></li><li><span><a href=\"#KoNLPy-법률/의안-코퍼스-사용\" data-toc-modified-id=\"KoNLPy-법률/의안-코퍼스-사용-2.3\">KoNLPy 법률/의안 코퍼스 사용</a></span></li></ul></li><li><span><a href=\"#NLTK-패키지\" data-toc-modified-id=\"NLTK-패키지-3\">NLTK 패키지</a></span><ul class=\"toc-item\"><li><span><a href=\"#nltk-install,-import\" data-toc-modified-id=\"nltk-install,-import-3.1\">nltk install, import</a></span></li><li><span><a href=\"#쿠퍼스-다운로드\" data-toc-modified-id=\"쿠퍼스-다운로드-3.2\">쿠퍼스 다운로드</a></span></li><li><span><a href=\"#다운로드-한-쿠퍼스-사용\" data-toc-modified-id=\"다운로드-한-쿠퍼스-사용-3.3\">다운로드 한 쿠퍼스 사용</a></span><ul class=\"toc-item\"><li><span><a href=\"#Penn-Treebank-Tag-Set-:-영미권에서-기본적으로-사용하는-쿠퍼스\" data-toc-modified-id=\"Penn-Treebank-Tag-Set-:-영미권에서-기본적으로-사용하는-쿠퍼스-3.3.1\">Penn Treebank Tag Set : 영미권에서 기본적으로 사용하는 쿠퍼스</a></span></li></ul></li><li><span><a href=\"#punkt-다운로드-(sentence-tokenizer)\" data-toc-modified-id=\"punkt-다운로드-(sentence-tokenizer)-3.4\">punkt 다운로드 (sentence tokenizer)</a></span></li><li><span><a href=\"#sent_tokenize,-word_tokenize\" data-toc-modified-id=\"sent_tokenize,-word_tokenize-3.5\">sent_tokenize, word_tokenize</a></span></li><li><span><a href=\"#TweetTokenizer-:-감정을-표현하는-이모티콘을-구분-가능함\" data-toc-modified-id=\"TweetTokenizer-:-감정을-표현하는-이모티콘을-구분-가능함-3.6\">TweetTokenizer : 감정을 표현하는 이모티콘을 구분 가능함</a></span></li><li><span><a href=\"#정규식으로-tokenize-:-regexp_tokenize\" data-toc-modified-id=\"정규식으로-tokenize-:-regexp_tokenize-3.7\">정규식으로 tokenize : regexp_tokenize</a></span><ul class=\"toc-item\"><li><span><a href=\"#한글-정규식-tokenize\" data-toc-modified-id=\"한글-정규식-tokenize-3.7.1\">한글 정규식 tokenize</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 분석 : 기반 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **형태소 분석** : 토큰 분리, 어간 추출, 품사 부착, 색인, 벡터화\n",
    "    - 입력된 문장을 형태소 단위로 분류하고 품사를 부착\n",
    "- **구문 분석** : 문장 경계 인식, 구문분석, 공기어, 개체명 사전 구축(PLOT, 수치, 외국어 한글 표기), 개체명 인식\n",
    "    - 주어, 목적어, 서술어와 같은 구문단위를 찾음\n",
    "- **의미 분석** : 대용어 해소(대명사, 두문자어, 약어, 수치), 의미 중의성 해결(동명이인 ,이명이인)\n",
    "    - 문장이 의미적으로 올바른 문장인지를 판단\n",
    "- **담화 분석**(담론 분석) : 분류, 군집, 중복, 요약, 가중치, 순위화, 토픽 모델링, 이슈 트래킹, 평판분석, 감성분석, 복합논증분석\n",
    "    - 대화 흐름상 어떤 의미를 가지는지를 찾음: 문맥 구조 분석(문장들의 연관관계), 의도분석( )\n",
    "- 중의성 해소\n",
    "\n",
    "형태소 분석, 구문 분석 분야는 어느 정도 기술이 성숙되어 있으나, 의미 분석, 담화 분석 등은 아직도 연구가 진행중인 분야임\n",
    "\n",
    "- 담화 : 문장이 연속되어 이루어지는 말의 단위, 문단\n",
    "- 문장 : 완결된 내용을 나타내는 최소 단위\n",
    "- 어절 : 문장을 구성하는 단위 (주어, 서술어, 관형어, ...) --> 띄어쓰기 단위\n",
    "- 단어 : 어절을 구성하는 요소 --> 형태소의 조합으로 구성되어 있음 (관형사, 명사, 조사, 동사) --> 품사를 붙일 수 있는 단위\n",
    "- 형태소 : (형태론적으로, 문법적으로) 의미를 가진 문법의 최소 단위(어근 분리) --> 형식 형태소(분석에 불필요)와 의존 형태소(분석에 사용)로 구분\n",
    "\n",
    "**형태소와 단어** : 단어는 뜻을 지니고 홀로 쓰일 수 있는 말, 형태소는 문법상 뜻을 가진 가장 작은 말\n",
    "    - 자립성에 따라 : 자립 형태소, 의존 형태소\n",
    "    - 의미에 따라 : 실질 형태소, 형식 형태소\n",
    "    - 단어 형성 방법에 따른 분류\n",
    "        - 단일어 : 어근\n",
    "        - 복합어 : 합성어(어근 + 어근), 파생어( 접두사 + 어근, 어근 + 접미사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 응용 기술 : 모델을 접목한 도메인 Specific 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검색\n",
    "- 온라인 광고\n",
    "- 자동번역\n",
    "- 감정분석\n",
    "- 음성인식\n",
    "- 맞춤법검사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy, NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy 패키지 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KoNLPy : 한국어 정보처리를 위한 파이썬 패키지 --> Corpus, Morpheme Alalyzer, POS Tagging, ... \n",
    "    - https://konlpy-ko.readthedocs.io/ko/v0.4.3/\n",
    "    - JDK 1.8 이상, JPype1 0.5.7 이상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- konlpy 설치\n",
    "\n",
    "~~~\n",
    "$ pip install konlpy     # Python 2.x\n",
    "$ pip3 install konlpy    # Python 3.x\n",
    "~~~\n",
    "    \n",
    "- MAC에 conda 환경에서 설치 시, 아래와 같은 conda  환경에서는 install 에러가 났으나,\n",
    "\n",
    "```\n",
    "$ conda activate nlp      # python3.6.1\n",
    "$ pip install konlpy\n",
    "```\n",
    "\n",
    "- 아래의 conda 환경에서는 정상적으로 install 되었음\n",
    "\n",
    "```\n",
    "$ conda activate base    # python3.7.1\n",
    "$ pip install konlpy\n",
    "```\n",
    "\n",
    "- python 3.6.7에서 install error 메시지\n",
    "\n",
    "```  \n",
    "Failed building wheel for JPype1\n",
    "Running setup.py clean for JPype1\n",
    "Failed to build JPype1\n",
    "Installing collected packages: JPype1\n",
    "...\n",
    "...\n",
    "Command \"/Users/alex/anaconda3/envs/nlp/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/k4/8cb3q8ws6mb4d1tjs8y85tj80000gn/T/pip-install-m1n7e9_x/JPype1/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/k4/8cb3q8ws6mb4d1tjs8y85tj80000gn/T/pip-record-thqtqp1b/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/k4/8cb3q8ws6mb4d1tjs8y85tj80000gn/T/pip-install-m1n7e9_x/JPype1/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy import, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('학교', 'NNG'),\n",
       " ('종', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('땡땡', 'MAG'),\n",
       " ('ㅌ', 'UN'),\n",
       " ('땡', 'MAG')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma = Kkma()\n",
    "ma.pos(\"학교종이 땡땡ㅌ땡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy 법률/의안 코퍼스 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw    # 범률 코퍼스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitution.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolaw.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill    # 법률 의안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1809896.txt',\n",
       " '1809897.txt',\n",
       " '1809895.txt',\n",
       " '1809894.txt',\n",
       " '1809890.txt',\n",
       " '1809891.txt',\n",
       " '1809893.txt',\n",
       " '1809892.txt',\n",
       " '1809899.txt',\n",
       " '1809898.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobill.fileids()    # 숫자는 의안번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = kolaw.open(kolaw.fileids()[0]).read()  # 말뭉치가 들어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4178"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.split())   # 어절이 4,178개 있음  --> 너무 많아서 사용하기 전에 줄이는 작업을 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.splitlines())   # 개행문자 단위로 문장이 356개가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분석을 위한 전문적이 툴이 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다국어를 지원하지만, 한국어는 지원 안됨\n",
    "- 문장 분석 등 일부 기능은 사용할 수 있음\n",
    "- 50개 이상의 corpora와 lexical resources를 쉽게 사용할 수 있는 interface가 있음\n",
    "- classification, tokenization, stemming, tagging, parsing, and semantic reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk install, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    # conda에 설치되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쿠퍼스 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk.download() : 매개변수를 지정하지 않으면, 모듈을 선택할 수 있는 윈도우가 뜨고, 선택해서 설치 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"brown\")\n",
    "#nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 설치 지원 윈도우\n",
    "\n",
    "![nltk](./images/nltk_package.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다운로드 한 쿠퍼스 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = brown.open(brown.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gutenberg.open(gutenberg.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penn Treebank Tag Set : 영미권에서 기본적으로 사용하는 쿠퍼스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( (S \n",
      "    (NP-SBJ \n",
      "      (NP (NNP Pierre) (NNP Vinken) )\n",
      "      (, ,) \n",
      "      (ADJP \n",
      "        (NP (CD 61) (NNS years) )\n",
      "        (JJ old) )\n",
      "      (, ,) )\n",
      "    (VP (MD will) \n",
      "      (VP (VB join) \n",
      "        (NP (DT the) (NN board) )\n",
      "        (PP-CLR (IN as) \n",
      "          (NP (DT a) (JJ nonexecutive) (NN director) ))\n",
      "        (NP-TMP (NNP Nov.) (CD 29) )))\n",
      "    (. .) ))\n",
      "( (S \n",
      "    (NP-SBJ (NNP Mr.) (NNP Vinken) )\n",
      "    (VP (VBZ is) \n",
      "      (NP-PRD \n",
      "        (NP (NN chairman) )\n",
      "        (PP (IN of) \n",
      "          (NP \n",
      "            (NP (NNP Elsevier) (NNP N.V.) )\n",
      "            (, ,) \n",
      "            (NP (DT the) (NNP Dutch) (VBG publishing) (NN group) )))))\n",
      "    (. .) ))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(treebank.open(treebank.fileids()[0]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punkt 다운로드 (sentence tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"punkt\")    # 한국어는 포함되어 있지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import punkt   # 텍스트를 sentence list로 분리해주는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
    "do not mark sentence boundaries.  And sometimes sentences\n",
    "can start with non-capitalized words.  i is a good variable\n",
    "name.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "-----\n",
      "And sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "-----\n",
      "i is a good variable\n",
      "name.\n"
     ]
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')   # pre-trained model (sentence detector)\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr.\n",
      "-----\n",
      "Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.\n",
      "-----\n",
      "And sometimes sentences\n",
      "can start with non-capitalized words.\n",
      "-----\n",
      "i is a good variable\n",
      "name.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "punkt_tokenizer = PunktSentenceTokenizer()\n",
    "print(\"\\n-----\\n\".join(punkt_tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokenize(g)\n",
    "# sent        # sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16823"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.splitlines())    # 라인 단위 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7493"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)    # 문장 단위 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'was',\n",
       " 'the',\n",
       " 'youngest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'daughters',\n",
       " 'of',\n",
       " 'a',\n",
       " 'most',\n",
       " 'affectionate',\n",
       " ',',\n",
       " 'indulgent',\n",
       " 'father',\n",
       " ';',\n",
       " 'and',\n",
       " 'had',\n",
       " ',',\n",
       " 'in',\n",
       " 'consequence',\n",
       " 'of',\n",
       " 'her',\n",
       " 'sister',\n",
       " \"'s\",\n",
       " 'marriage',\n",
       " ',',\n",
       " 'been',\n",
       " 'mistress',\n",
       " 'of',\n",
       " 'his',\n",
       " 'house',\n",
       " 'from',\n",
       " 'a',\n",
       " 'very',\n",
       " 'early',\n",
       " 'period',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sent[1])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '?', 'Hello', 'world', '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hello world? Hello world.\", preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘미세먼지보통이다.', '불금을', '즐기자', '!']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"오늘미세먼지보통이다. 불금을 즐기자!\", preserve_line=True)   # 공백으로 구분되지 않은 단어는 분리하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_tokenize는 한글에서는 사용할 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world?', 'Hello world.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Hello world? Hello world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘미세먼지보통이다.', '불금을 즐기자!']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"오늘미세먼지보통이다. 불금을 즐기자!\")    # 문장 구분자는 한글에서도 인식이됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sent_tokenize는 한글에서도 사용 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer : 감정을 표현하는 이모티콘을 구분 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘미세먼지보통이다.', '불금을 즐기자....................:)   ;()']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"오늘미세먼지보통이다. 불금을 즐기자....................:)   ;()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘미세먼지보통이다', '.', '불금을', '즐기자', '...', ':)', ';(', ')']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TweetTokenizer().tokenize(\"오늘미세먼지보통이다. 불금을 즐기자....................:)   ;()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘미세먼지보통이다',\n",
       " '.',\n",
       " '불금을',\n",
       " '즐기자',\n",
       " '...',\n",
       " '...',\n",
       " '...',\n",
       " '...',\n",
       " ':',\n",
       " ')',\n",
       " ';',\n",
       " '(',\n",
       " ')']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"오늘미세먼지보통이다. 불금을 즐기자............:)   ;()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizer를 사용 목적에 따라 적절히 선택해서 사용해야 함. 상황에 따라 성능이 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규식으로 tokenize : regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'unite',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'of',\n",
       " 'existence',\n",
       " 'and',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty',\n",
       " 'one',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'with',\n",
       " 'very',\n",
       " 'little',\n",
       " 'to',\n",
       " 'distress',\n",
       " 'or',\n",
       " 'vex',\n",
       " 'her']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(sent[0], \"[A-za-z]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글 정규식 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '미세먼지', '보통이다', '불금을', '즐기자']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"([가-힣]+)\"\n",
    "regexp_tokenize(\"오늘 미세먼지 보통이다. 불금을 즐기자....................:)   ;(\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '미세', '먼지', '먼지', '미세', '보통이다', '불금을', '즐기자']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(\"오늘 미세 먼지 먼지 미세 보통이다. 불금을 즐기자....................:)...;(...\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
