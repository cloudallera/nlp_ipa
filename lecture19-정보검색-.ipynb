{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#정보검색\" data-toc-modified-id=\"정보검색-1\">정보검색</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-1.1\">Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-1.1.1\">Inverted Index</a></span></li><li><span><a href=\"#Retrieval-Models-:-Vector-Space-Model\" data-toc-modified-id=\"Retrieval-Models-:-Vector-Space-Model-1.1.2\">Retrieval Models : Vector Space Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Relevance-=-Similarity\" data-toc-modified-id=\"Relevance-=-Similarity-1.1.2.1\">Relevance = Similarity</a></span></li><li><span><a href=\"#Vector-Space-Model\" data-toc-modified-id=\"Vector-Space-Model-1.1.2.2\">Vector Space Model</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.1.2.3\">TF-IDF</a></span></li></ul></li></ul></li><li><span><a href=\"#색인기법(계속)\" data-toc-modified-id=\"색인기법(계속)-1.2\">색인기법(계속)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Full-text-indexing-:-Bag-of-Words-representation\" data-toc-modified-id=\"Full-text-indexing-:-Bag-of-Words-representation-1.2.1\">Full text indexing : Bag-of-Words representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words-representation-구현-절차\" data-toc-modified-id=\"Bag-of-Words-representation-구현-절차-1.2.1.1\">Bag-of-Words representation 구현 절차</a></span></li><li><span><a href=\"#lexicon-만들기-:-list\" data-toc-modified-id=\"lexicon-만들기-:-list-1.2.1.2\">lexicon 만들기 : list</a></span></li><li><span><a href=\"#lexicon-만들기(속도-개선)-:-list-=&gt;-set\" data-toc-modified-id=\"lexicon-만들기(속도-개선)-:-list-=>-set-1.2.1.3\">lexicon 만들기(속도 개선) : list =&gt; set</a></span></li><li><span><a href=\"#BOW-만들기-:-list\" data-toc-modified-id=\"BOW-만들기-:-list-1.2.1.4\">BOW 만들기 : list</a></span></li><li><span><a href=\"#BOW-속도-개선(1)-:-list-=&gt;-dict\" data-toc-modified-id=\"BOW-속도-개선(1)-:-list-=>-dict-1.2.1.5\">BOW 속도 개선(1) : list =&gt; dict</a></span></li><li><span><a href=\"#BOW-속도-개선(2)-:-dict-=&gt;-defaultdict,-lexicon-사전-비교-안하기\" data-toc-modified-id=\"BOW-속도-개선(2)-:-dict-=>-defaultdict,-lexicon-사전-비교-안하기-1.2.1.6\">BOW 속도 개선(2) : dict =&gt; defaultdict, lexicon 사전 비교 안하기</a></span></li></ul></li><li><span><a href=\"#DTM-=&gt;-TDM-변환\" data-toc-modified-id=\"DTM-=>-TDM-변환-1.2.2\">DTM =&gt; TDM 변환</a></span></li><li><span><a href=\"#Controlled-Vocabulary-Set-만들기\" data-toc-modified-id=\"Controlled-Vocabulary-Set-만들기-1.2.3\">Controlled Vocabulary Set 만들기</a></span><ul class=\"toc-item\"><li><span><a href=\"#뉴스기사에서-불필요한-단어-지우기\" data-toc-modified-id=\"뉴스기사에서-불필요한-단어-지우기-1.2.3.1\">뉴스기사에서 불필요한 단어 지우기</a></span></li></ul></li><li><span><a href=\"#Inverted-Index-(역문헌-구조)\" data-toc-modified-id=\"Inverted-Index-(역문헌-구조)-1.2.4\">Inverted Index (역문헌 구조)</a></span></li></ul></li><li><span><a href=\"#Retrieval-Models\" data-toc-modified-id=\"Retrieval-Models-1.3\">Retrieval Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vector-Space-Model-:-TF-IDF\" data-toc-modified-id=\"Vector-Space-Model-:-TF-IDF-1.3.1\">Vector Space Model : TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF\" data-toc-modified-id=\"TF-1.3.1.1\">TF</a></span></li><li><span><a href=\"#IDF\" data-toc-modified-id=\"IDF-1.3.1.2\">IDF</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.3.1.3\">TF-IDF</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정보검색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Hash table, B-tree, trie, ...\n",
    "\n",
    "![inverted_index](./images/inverted_index.png)\n",
    "\n",
    "- D는 문서의 전체 단어 크기이지만, |L|은 zipf's law에 따라 전체의 10%만 사용함\n",
    "- python에서는 document의 index number를 pointer 처럼 사용하여 linked list를 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![inverted_index_construction](./images/inverted_index_construction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Models : Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance = Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Represent both doc and query by concept vectors  => concept는 Lexicon의 Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikipedia.org/wiki/Tf–idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two basic heuristics : TF & IDF\n",
    "- TF (Term Frequency) = Within-doc-frequency => 하나의 도큐먼트 내에서 중요도\n",
    "- IDF (Inverse Document Frequency) : Collection(도큐먼트들의 모음) 내에서 중요도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF는 단순히 나타난 횟수 만으로 weight를 산정하는 것은 바람직하지 않음\n",
    "- TF Normalization : Document의 길이로 나타난 횟수를 정규화\n",
    "- **Sublinear TF scaling**\n",
    "\n",
    "![Sublinear_TF_scaling](./images/Sublinear_TF_scaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Maximum TF scaling**\n",
    "\n",
    "![Maximum_TF_scaling](./images/Maximum_TF_scaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IDF는 rare terms에 higher weights를 할당\n",
    "\n",
    "![idf](./images/idf.png)\n",
    "\n",
    "- TF-IDF\n",
    "\n",
    "![tf_idf](./images/TF_IDF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TF_weight.png](./images/TF_weight.png)\n",
    "\n",
    "![IDF_weight.png](./images/IDF_weight.png)\n",
    "\n",
    "![TF_IDF_weight.png](./images/TF_IDF_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 색인기법(계속)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full text indexing : Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumption: word is independent from each other\n",
    "- Pros: simple\n",
    "- Cons: grammar and order are missing\n",
    "- The most frequently used document representation : Image, speech, gene sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words representation 구현 절차\n",
    "\n",
    "1. 크롤링\n",
    "1. collection => document 집합\n",
    "1. Word split => Tokenize, Preprocessing\n",
    "1. Lexicon Dictionary (|V|) => 전체 문서의 token 리스트\n",
    "1. 문서 표현 (by BOW) => Lexicon Dictionary의 token이 해당 문서에 있으면 1, 없으면 0으로 표현된 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lexicon 만들기 : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "\n",
    "def getLexicon():    # list 사용\n",
    "    lexicon = list()\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "            if token not in lexicon:\n",
    "                lexicon.append(token)\n",
    "                \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 ms ± 12.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getLexicon()   # 속도가 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lexicon 만들기(속도 개선) : list => set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "\n",
    "def getLexicon():    # set 사용\n",
    "    lexicon = set()\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "                lexicon.add(token)\n",
    "                \n",
    "    return list(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 ms ± 13.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getLexicon()   # set을 사용해서 if문을 없앴기 때문에 속도가 개선됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2638"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = getLexicon()\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW 만들기 : list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전: [0:단어1, 1:단어2, 2:단어3, ...]\n",
    "# 문서:     0:  [0:0 or 1, ...] => 문서에 있으면 1, 없으면 0으로 표현\n",
    "#              1: \n",
    "# 문서목록 : [0:문서1, 1:문서2, ...]\n",
    "\n",
    "# 문서는 모두 서로 다르다고 가정 : 같은 문서가 있는 경우 set을 사용할 수 없음\n",
    "\n",
    "def getDocRepr(lexicon):\n",
    "    docList = list()    # 문서목록\n",
    "    docRepr = list()   # 문서표현 of BOW의 집합 => 문서 갯수 만큼\n",
    "    # 0: BOW\n",
    "    # 1: BOW\n",
    "    # 2: BOW\n",
    "    # ...\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        docList.append(docName)\n",
    "        docVector = list(0 for _ in range(len(lexicon)))   # 문서 1개에 대한 문서표현(BOW) = [0] * 단어의 갯수\n",
    "\n",
    "        for token in document.split():\n",
    "            if token in lexicon:\n",
    "                docVector[lexicon.index(token)] = 1   # 문서 사전에 있으면, BOW 표현을 1로 변경\n",
    "\n",
    "        docRepr.append(docVector)\n",
    "        \n",
    "    return docList, docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, '최소규모의', 2638)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docList, docRepr = getDocRepr(lexicon)\n",
    "len(docList), len(docRepr), lexicon[2], len(docRepr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docRepr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 ms ± 30.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocRepr(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW 속도 개선(1) : list => dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제점 : 전체 리스트에서 0인 부분이 너무 많고, 문서가 많아질수록 더 심해짐\n",
    "- 개선방안 : BOW를 list가 아닌 dictionary 형태로 변경(token이 있는 부분만 저장) <br>\n",
    "   ==> lexicon dicitionary에서 비교하지 않고, token을 직접 key로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW dictrionary 표현\n",
    "\n",
    "def getDocReprByDict(lexicon):\n",
    "    # docList = list()    # 문서목록 => 필요 없음\n",
    "    docRepr = dict()   # 문서표현 of BOW의 Dictionary\n",
    "    # key => 문서\n",
    "    # Value => BOW => list x, dict\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        docRepr[docName] = dict()\n",
    "\n",
    "        for token in document.split():\n",
    "            if token in lexicon:\n",
    "                docRepr[docName][lexicon.index(token)] = 1   # 문서 사전에 있으면, BOW 표현을 1로 변경\n",
    "\n",
    "        \n",
    "    return docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docRepr = getDocReprByDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, '1809896.txt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docRepr), list(docRepr.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#docRepr['1809896.txt'][1861], lexicon[1861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 ms ± 4.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocReprByDict(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW 속도 개선(2) : dict => defaultdict, lexicon 사전 비교 안하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdict로 변경\n",
    "from collections import defaultdict\n",
    "\n",
    "def getDocReprByDefaultDict(lexicon):\n",
    "    docRepr = defaultdict(lambda: defaultdict(int))   # 문서표현 of BOW의 Dictionary\n",
    "\n",
    "    for docName in kobill.fileids():\n",
    "        document = kobill.open(docName).read()\n",
    "        \n",
    "        for token in document.split():\n",
    "            docRepr[docName][token] = 1   # 문서 사전을 비교하지 않고, token을 key로 바로 사용 => BOW 표현을 1로 변경\n",
    "            # lexicon 사전을 만들 때 이미 해당 문서의 모든 token을 포함시겼기 때문에 비교하지 않고 token을 바로 key로 사용함\n",
    "        \n",
    "    return docRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docRepr = getDocReprByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, '1809896.txt')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docRepr), list(docRepr.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docRepr['1809896.txt'][100], lexicon[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 ms ± 562 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getDocReprByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM => TDM 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검색 => Input(키워드들의 집합 = Query) <br>\n",
    "   키워드들은 단어로 구성, 단어를 추출\n",
    "\n",
    "\n",
    "- Q = \\[\"국회\", \"의원\"]  이라면,\n",
    "\n",
    "\n",
    "- |Q| = q, |D| = d, |V| = v 라면,<br>\n",
    "   q**d, **v   ==> 너무 과다한 계산 (실행 불가)\n",
    "\n",
    "~~~\n",
    "for 단어 in Q:<br>\n",
    "    for 전체 문서를 대상:<br>\n",
    "            for 문서 내 단어 :<br>\n",
    "                if 단어 == 사전:<br>\n",
    "                    이때의 해당 문서가 검색 후보\n",
    "~~~\n",
    "\n",
    "---------\n",
    "- 이를 개선하기 위해, InvertedDocument (역문헌 구조)를 사용\n",
    "\n",
    "\n",
    "- key: 단어 in 사전,<br>\n",
    "   value: 어느 문서에서 나왔는지, 몇 번(, 어느 위치 등)\n",
    "\n",
    "~~~\n",
    "for 단어 in Q:\n",
    "    # for 전체 문서를 대상:  # 전체 문서를 대상으로 찾을 필요 없음\n",
    "    #        for 문서 내 단어 :\n",
    "    #            if 단어 == 사전:\n",
    "    단어 in InvertedDocument.keys():\n",
    "        이때의 해당 문서가 검색 후보\n",
    "~~~\n",
    "\n",
    "----------\n",
    "- **DTM(Document-Term Matrix)**\n",
    "\n",
    "| Doc  | W1 | W1 | W3 | W4 | W5 | ... | Wn |\n",
    "|------|----|----|----|----|----|---|---|\n",
    "|Doc1 | 0 | 1 | 0 | 1 | 0 | ... | 0 |\n",
    "|Doc2 |\n",
    "|... |\n",
    "|Doc10|\n",
    "\n",
    "- **DTM => TDM으로 변경** (리스트 x, 딕셔너리 o => 공간을 줄일려고)\n",
    "\n",
    "\n",
    "- **TDM(Term-Document Matrix)**\n",
    "\n",
    "|Word | D1 | D1 | ... | D10 |\n",
    "|------|---|---|---|---|\n",
    "|W1 | 0 |\n",
    "|W2 | 1 | \n",
    "|W3 | 0 |\n",
    "|W4 | 1 |\n",
    "|W5 | 0 |\n",
    "|...    |\n",
    "|Wn | 0 |\n",
    "\n",
    "----------\n",
    "- **Query**의 경우에도:<br>\n",
    "   문자들의 검색 결과를 one-hot 벡터로 합해서 표현하면 됨\n",
    "\n",
    "|Query | W1 | W1 | W3 | W4 | W5 | ... | Wn |\n",
    "|------|----|----|----|----|----|----|----|\n",
    "|Q1 | 0 | 1 | 0 | 1 | 0 | ... | 0 |\n",
    "|Q2 |\n",
    "|Q3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM을 TDM으로 변경\n",
    "def convertInvertedDocument(DTM):\n",
    "    TDM = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for fileName, termList in DTM.items():   # DTM[fileName][term]\n",
    "        for term, freq in termList.items():\n",
    "            TDM[term][fileName] = freq\n",
    "            \n",
    "    return TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM = convertInvertedDocument(docRepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {'1809897.txt': 1, '1809898.txt': 1}),\n",
       " defaultdict(int, {'1809896.txt': 1}))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean 검색\n",
    "TDM[\"국회\"], TDM[\"의원\"]  # Boolean Model 이라고 함 (&, |, not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'1809897.txt': 1, '1809898.txt': 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM[\"국회\"] or TDM[\"의원\"]  # Boolean Model 이라고 함 (&, |, not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled Vocabulary Set 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BOW 특성상 정확히 일치하는 단어만을 인식함 -> 어절만 적용 시 구두점까지 정확히 일치하는 경우만 인식됨. 띄어쓰기 안되어 있는 경우, 품사적인 중의성 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어절, 특정 품사와 단어 길이 제한, Ngram(어절 or 음절) 등을 적용한 set() 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'NNG'), ('날씨', 'NNG'), ('는', 'JX'), ('어제', 'NNG'), ('날씨', 'NNG'), ('보다', 'JKM'), ('안', 'MAG'), ('좋', 'VA'), ('은', 'ETD'), ('거', 'NNB'), ('같', 'VA'), ('아요', 'EFN')]\n",
      "['오늘', '날씨', '어제', '거']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘 날씨는 어제 날씨 보다 안좋은거 같아요\"\n",
    "print(Kkma().pos(sentence))\n",
    "print(Kkma().nouns(sentence))   # 빈도에 대한 정보가 사라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘']\n",
      "['날씨']\n",
      "['어제']\n",
      "['날씨']\n",
      "[]\n",
      "['거']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for token in sentence.split():   # word_tokenize(sentence)\n",
    "    print(Kkma().nouns(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('거', 'NNB')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1].startswith(\"N\")]:   # 명사만\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('좋', 'VA')\n",
      "('같', 'VA')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"]]:  # 명사와 형용사(용언)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', 'NNG')\n",
      "('날씨', 'NNG')\n",
      "('어제', 'NNG')\n",
      "('날씨', 'NNG')\n"
     ]
    }
   ],
   "source": [
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:  # 명사, 형용사, 길이 제한\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날씨', '보다', '오늘', '안좋은거', '날씨는', '어제', '같아요']\n"
     ]
    }
   ],
   "source": [
    "lexicon = list(set(sentence.split()))\n",
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:\n",
    "    lexicon.append(token[0])\n",
    "\n",
    "print(list(set(lexicon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.nlp import ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘 날씨는', '날씨는 어제', '어제 날씨', '날씨 보다', '보다 안좋은거', '안좋은거 같아요']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.ngramEojeol(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram.ngramUmjeol(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  ['날씨보다', '오늘', '안좋은거', '날씨는', '어제', '같아요']\n",
      "2.  ['날씨', '날씨보다', '오늘', '날씨는', '안좋은거', '어제', '같아요']\n",
      "3.  ['날씨보다', '오늘', '안좋은거', '날씨는', '어제', '같아요', '오늘', '날씨', '어제', '날씨']\n",
      "4.  ['날씨', '날씨보다', '오늘', '날씨는', '안좋은거', '어제', '오늘 날씨는', '안좋은거 같아요', '날씨보다 안좋은거', '같아요', '어제 날씨보다', '날씨는 어제']\n",
      "5.  ['날씨', '날씨보다', '오늘', '날씨는', '안좋은거', '날씨 날씨', '어제', '오늘 날씨는', '안좋은거 같아요', '날씨보다 안좋은거', '같아요', '어제 날씨보다', '날씨는 어제']\n",
      "6.  ['씨보', '어제', '오늘 날씨는', '날씨보다', '오늘', '좋은', '씨는', '날씨는', '날씨 날씨', '안좋은거 같아요', '아요', '날씨', '날씨는 어제', '보다', '안좋은거', '은거', '안좋', '같아', '날씨보다 안좋은거', '같아요', '어제 날씨보다']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘 날씨는 어제 날씨보다 안좋은거 같아요\"\n",
    "\n",
    "lexicon = list(set(sentence.split()))\n",
    "print(\"1. \", lexicon)\n",
    "\n",
    "for token in [word for word in Kkma().pos(sentence) if word[1] in [\"NNG\", \"VA\"] and len(word[0]) > 1]:\n",
    "    lexicon.append(token[0])\n",
    "print(\"2. \", list(set(lexicon)))\n",
    "\n",
    "print(\"3. \", lexicon)\n",
    "\n",
    "lexicon.extend(ngram.ngramEojeol(sentence))\n",
    "print(\"4. \", list(set(lexicon)))\n",
    "\n",
    "lexicon.extend(ngram.ngramEojeol(\" \".join(token[0] for token in [token for word in Kkma().pos(sentence) if word[1] in  [\"NNG\", \"VA\"] and len(word[0]) > 1])))\n",
    "print(\"5. \", list(set(lexicon)))\n",
    "\n",
    "newLexicon = list()\n",
    "for word in lexicon:\n",
    "    if len(word.split()) == 1:\n",
    "        newLexicon.extend(ngram.ngramUmjeol(word))\n",
    "lexicon.extend(newLexicon)\n",
    "print(\"6. \", list(set(lexicon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6번 lexicon의 경우 query에 어떤 형태의 단어가 입력되어도 찾아질 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뉴스기사에서 불필요한 단어 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.naver import NewsScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = NewsScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = news.get_article(\"정치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n野 \"자료없는 청문회 의미 없어…후보자가 방해까지\"與 \"유방암 수술병원이 왜 궁금한가…정치적 망신주기\"이종배 의원을 비롯한 자유한국당 의원들이 27일 서울 여의도 국회 산업통상자원중소벤처기업위원회에서 열린 박영선 중소벤처기업부장관 후보자 인사청문회에서 박영선 후보자의 자료제출을 요구하는 문구를 노트북에 붙여 놓고 있다. 2019.3.27/뉴스1 © News1 이종덕 기자(서울=뉴스1) 김성은 기자,최동현 기자,김세현 기자 = 27일 열린 박영선 중소벤처기업부 장관 후보자에 대한 국회 산업통상자원중소벤처기업위원회(산자위)의 인사청문회에서 여야는 박 후보자가 미제출한 자료를 둘러싸고 날선 공방을 벌였다. 산자위 소속 한국당 의원들은 회의장에서 \\'박영선 자료제출 거부! 국민들은 박영선 거부!\\'라고 쓰인 종이를 책상 위 컴퓨터에 붙여두고 발언했다.한국당은 \"깜깜이 청문회\"라며 강하게 비판했지만 민주당은 \"정치적 망신주기\"라며 반발했다. 산자위 한국당 간사인 이종배 의원은 \"자료 없이 인사청문회를 하는 것은 아무런 의미가 없다는 뜻에서 인사청문회 연기요청을 했지만, 여당 측에서 받아들일수 없다고 했다\"며 \"이제까지 청문회 중에서 이렇게 자료없이 \\'깜깜이 청문회\\'를 한 경우는 없다\"고 비판했다. 이어 \"후보자가 국무위원으로 자격이 있는지 자질을 갖추고 도덕성과 청렴성을 갖췄는지 보기 위해선 제출 자료를 토대로 따져야 한다. 자료도 제출하지 않고 야당 청문위원들의 입을 막으려는 자세는 있을 수 없다\"며 \"어떻게 인사청문회 위원에게 겁박을 하나, 청문위원 입에 재갈을 물리려 하나\"라고 반발했다.이에 산자위 민주당 간사인 홍의락 의원은 \"출생기록부라든지 혼인관계증명서라든지 이런 것들은 너무 개인적인 것들에 대한 자료라고 생각되는 부분이 있다 \"이런 부분에 대한 자료들에 대해선 철회해 주시든가 하시고, 필요한 자료는 강력하게 요구해 주시면 여당에서도 협조해서 청문회가 무사히 잘 끝날 수 있도록 노력하겠다\"고 답했다.민주당 이훈 의원은 \"자료제출이 거부된 것을 보니 후보자도 차마 인간적으로 감내하기 어려운 부분이 있다. 유방암 수술 받은 병원이 왜 궁금한 것인가\"라며 \"설사 수술을 했든 안 했든 이런 자료 제출 요구하면서 이거 제출하지 않았다고 정회를 요구하고.청문회를 거부하나\"라고 반문했다. 이어 \"중소기업을 살리고 벤처기업을살리고 제대로 역할을 할 수 있는지, 정책 역량을 갖췄는지, (검증하는) 이런 자리다\"라며 \"(야당이) 이런 자료 제출을 요구하면서 청문회를 정치적으로 끌고 가고 있다\"고 지적했다.같은당 박범계 의원도 한국당을 겨냥해 \"후보자의 결혼증명서를 내라고 하는 것은 어떤 불순한 상상을 하기에 내라고 하는 것인가\"라며 \"정치적 망신주기에 다를 바 아니라는 생각이 든다\"고 했다.이용주 민주평화당 의원도 \"관음증을 충족하기 위해 후보자의 개인적인 사생활에 대한 자료를 요구해선 안 된다고 본다\"며 \"국회라고 하더라도 그럴 자격이 없다\"고 비판했다.이에 이철규 한국당 의원은 \"극히 일부분을 가지고 마치 국회의원들이 무자비하게 망신주기를 했다고 표현한 것은 유감\"이라며 \"후보자께서 자료를 제출을 하지않는 것은 물론이고 적극적으로 자료 제출을 방해까지 했다\"고 반박했다.이어 \"국세청에서 적어도 거부할 수 없는 자료를 요구했는데 이철규 의원실에 요구 자료를 제출하지 말아달라는 압력까지 행사했다는 이야기를 듣고 참으로 실망을 금치 못했다\"고 주장했다. 이언주 바른미래당 의원도 \"저는 주로 정책자료를 요청했지만 정책자료에 대해서도 제대로 협조가 안됐다\"며 \"열흘 전에 자료를 요청햇는데, 이건 무시를 하는 것인지, 거짓말을 하는건지 이해할 수가 없다\"고 반박했다.이에 박 후보자는 \"의원님들께서 모두 2252건의 자료를 요구하셨다. 그중에서 제가 자료 제출을 안 한 것이 145건이다\"며 \"인사청문팀에서 이언주 의원님께 보내는 이메일 주소에 오타가 있었다\"고 해명했다.sekim@news1.kr▶ [ 크립토허브 ]  [ 터닝포인트 2019 ] ▶ 네이버 메인에서 [뉴스1] 구독하기![© 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지]\\n\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def getFileList(base=\"./naver_news/2019-03-20_02-07/\", ext=\"txt\"):\n",
    "#     fileList = list()\n",
    "    \n",
    "#     for file in os.listdir(base):\n",
    "#         if file.split(\".\")[-1] == ext:\n",
    "#             fileList.append(\"{0}/{1}\".format(base, file))\n",
    "    \n",
    "#     return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getContent(file):\n",
    "#     content = \"\"\n",
    "    \n",
    "#     with open(file, encoding=\"utf-8\") as f:\n",
    "#         content = f.read()\n",
    "    \n",
    "#     return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coupus = getContent(getFileList()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t6월 IATA 총회 등 국제행사 차질 우려…\\'조원태 체제\\' 가시화 전망\"투명한 경영으로 오히려 신뢰도 높아질 것\" 기대도(서울=연합뉴스) 김동규 기자 = \"64%는 확보했는데…, 2.6%가 부족했다.',\n",
       " '\"[그래픽] 조양호, 대한항공 경영권 박탈…반대 36%로 \\'연임안 부결\\'    20년간 대한항공을 이끌던 조양호(70) 한진그룹 회장이 27일 주주총회에서 경영권 수성에 실패하면서 대한항공은 충격에 빠졌다.',\n",
       " '대한항공은 주총 직후 공식적인 입장을 내지 않았지만, 내부에서는 \"설마 했던 일이 현실이 됐다\"며 당황한 모습이 역력하다.',\n",
       " '이날 대한항공 주총에 상정된 조양호 회장의 사내이사 연임안은 찬성 64.09%, 반대 35.91%로 부결됐다.대한항공 주총…항의하는 반대표 주주(서울=연합뉴스) 홍해인 기자 = 27일 오전 서울 강서구 대한항공 본사에서 열린 정기 주주총회에서 1호 의안이 통과하자 반대표 주주들이 항의하고 있다.',\n",
       " '2019.3.27    대한항공 정관에 따르면 주총 참석 주주 3분의 2 이상 찬성표를 얻어야 사내이사직 수성이 가능한데, 지분 2.6%가 부족했다.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus =news.get_article(\"경제\")\n",
    "sent_tokenize(corpus)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t6월 IATA 총회 등 국제행사 차질 우려…\\'조원태 체제\\' 가시화 전망\"투명한 경영으로 오히려 신뢰도 높아질 것\" 기대도(서울=연합뉴스) 김동규 기자 = \"64%는 확보했는데…, 2.6%가 부족했다 그래픽] 조양호, 대한항공 경영권 박탈…반대 36%로 \\'연임안 부결\\'    20년간 대한항공을 이끌던 조양호(70) 한진그룹 회장이 27일 주주총회에서 경영권 수성에 실패하면서 대한항공은 충격에 빠졌다.    대한항공은 주총 직후 공식적인 입장을 내지 않았지만, 내부에서는 \"설마 했던 일이 현실이 됐다\"며 당황한 모습이 역력하다.    이날 대한항공 주총에 상정된 조양호 회장의 사내이사 연임안은 찬성 64.09  반대 35.91%로 부결됐다.대한항공 주총…항의하는 반대표 주주(서울=연합뉴스) 홍해인 기자 = 27일 오전 서울 강서구 대한항공 본사에서 열린 정기 주주총회에서 1호 의안이 통과하자 반대표 주주들이 항의하고 있다. 2019.3.27    대한항공 정관에 따르면 주총 참석 주주 3분의 2 이상 찬성표를 얻어야 사내이사직 수성이 가능한데, 지분 2.6%가 부족했다.    대한항공은 조 회장이 자발적인 결단이 아니라 주주들의 결정에 의해 내몰리듯 사내이사와 대표이사에서 물러나게 됐다는 사실에 충격을 받은 모습이다.    내부에서는 조 회장이 대표이사직을 잃게 되면서 경영에 차질을 빚을 것을 우려하고 있다.당장 오는 6월 대한항공 주관으로 서울에서 처음 열리는 국제항공운송협회(IATA) 연차총회 개최가 걱정이다.    IATA는 현재 전 세계 120개국 287개 민간 항공사들이 회원으로 가입한 항공 관련 국제 협력 기구다.    총회 의장은 주관항공사 최고경영자(CEO)가 맡는 관례에 따라 조 회장이 의장 자리에 앉아야 하지만, 이를 어떻게 처리해야 할지도 고민이다.    조 회장은 1999년 아버지 고 조중훈 회장으로부터 대한항공 최고경영자(CEO) 자리를 물려받은 뒤 지난 20년 동안 대한항공을 이끌어왔다.    여러 부침이 있었지만, 대한항공을 세계적인 항공사로 성장시키는데 조 회장의 기여가 있었다는 것은 업계도 인정하는 사실이다.    여러 논란에도 불구하고 대한항공으로선 20년간 항공사 경영을 통해 쌓은 조 회장의 전문가적 식견과 인적 네트워크 등 리더십을 잃게 됐다.     다만, 조 회장 장남인 조원태 사장이 여전히 대표이사로 경영에 참여하고 있고, 조 회장도 주식 지분을 여전히 보유하고 있어 조 회장의 영향력이 완전히 배제됐다고 보기는 어렵다.    사내 일각에서는 조 회장이 경영에서 물러나는 것이 오히려 대한항공의 경영 투명성을 강화해 회사에 도움이 될 것이라는 말도 나온다.    실제로 이날 조 회장의 대한항공 사내이사 연임안 부결 소식이 알려지자 대한항공과 지주사인 한진칼 등 계열사 주가가 일제히 강세를 보였다.    대한항공 관계자는 \"오늘 주총 결정에 따라 조 회장의 거취와 대한항공 경영 등 관련 사항을 절차를 밟아 논의할 계획\"이라고 말했다.    dkkim@yna.co.kr▶네이버 홈에서 [연합뉴스] 채널 구독하기▶뭐 하고 놀까? #흥  ▶쇼미더뉴스! 오늘 많이 본 뉴스영상\\n\\n\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern = re.compile(r\"[/]{2,}\")   # \"/\"가 2번 이상 반복되는 경우\n",
    "pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "pattern.findall(corpus)\n",
    "pattern.sub(\" \", corpus)    # %) 처럼 다른 pucctuation이 연속되는 경우에도 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t', '    ', '    ', '    ', '    ', '    ', '    ', '    ', '    ', '    ', '    ', '    ', '     ', '    ', '    ', '    ', '    ', '  ', '\\n\\n\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 6월 IATA 총회 등 국제행사 차질 우려…\\'조원태 체제\\' 가시화 전망\"투명한 경영으로 오히려 신뢰도 높아질 것\" 기대도(서울=연합뉴스) 김동규 기자 = \"64%는 확보했는데…, 2.6%가 부족했다.\"[그래픽] 조양호, 대한항공 경영권 박탈…반대 36%로 \\'연임안 부결\\' 20년간 대한항공을 이끌던 조양호(70) 한진그룹 회장이 27일 주주총회에서 경영권 수성에 실패하면서 대한항공은 충격에 빠졌다. 대한항공은 주총 직후 공식적인 입장을 내지 않았지만, 내부에서는 \"설마 했던 일이 현실이 됐다\"며 당황한 모습이 역력하다. 이날 대한항공 주총에 상정된 조양호 회장의 사내이사 연임안은 찬성 64.09%, 반대 35.91%로 부결됐다.대한항공 주총…항의하는 반대표 주주(서울=연합뉴스) 홍해인 기자 = 27일 오전 서울 강서구 대한항공 본사에서 열린 정기 주주총회에서 1호 의안이 통과하자 반대표 주주들이 항의하고 있다. 2019.3.27 대한항공 정관에 따르면 주총 참석 주주 3분의 2 이상 찬성표를 얻어야 사내이사직 수성이 가능한데, 지분 2.6%가 부족했다. 대한항공은 조 회장이 자발적인 결단이 아니라 주주들의 결정에 의해 내몰리듯 사내이사와 대표이사에서 물러나게 됐다는 사실에 충격을 받은 모습이다. 내부에서는 조 회장이 대표이사직을 잃게 되면서 경영에 차질을 빚을 것을 우려하고 있다.당장 오는 6월 대한항공 주관으로 서울에서 처음 열리는 국제항공운송협회(IATA) 연차총회 개최가 걱정이다. IATA는 현재 전 세계 120개국 287개 민간 항공사들이 회원으로 가입한 항공 관련 국제 협력 기구다. 총회 의장은 주관항공사 최고경영자(CEO)가 맡는 관례에 따라 조 회장이 의장 자리에 앉아야 하지만, 이를 어떻게 처리해야 할지도 고민이다. 조 회장은 1999년 아버지 고 조중훈 회장으로부터 대한항공 최고경영자(CEO) 자리를 물려받은 뒤 지난 20년 동안 대한항공을 이끌어왔다. 여러 부침이 있었지만, 대한항공을 세계적인 항공사로 성장시키는데 조 회장의 기여가 있었다는 것은 업계도 인정하는 사실이다. 여러 논란에도 불구하고 대한항공으로선 20년간 항공사 경영을 통해 쌓은 조 회장의 전문가적 식견과 인적 네트워크 등 리더십을 잃게 됐다. 다만, 조 회장 장남인 조원태 사장이 여전히 대표이사로 경영에 참여하고 있고, 조 회장도 주식 지분을 여전히 보유하고 있어 조 회장의 영향력이 완전히 배제됐다고 보기는 어렵다. 사내 일각에서는 조 회장이 경영에서 물러나는 것이 오히려 대한항공의 경영 투명성을 강화해 회사에 도움이 될 것이라는 말도 나온다. 실제로 이날 조 회장의 대한항공 사내이사 연임안 부결 소식이 알려지자 대한항공과 지주사인 한진칼 등 계열사 주가가 일제히 강세를 보였다. 대한항공 관계자는 \"오늘 주총 결정에 따라 조 회장의 거취와 대한항공 경영 등 관련 사항을 절차를 밟아 논의할 계획\"이라고 말했다. dkkim@yna.co.kr▶네이버 홈에서 [연합뉴스] 채널 구독하기▶뭐 하고 놀까? #흥 ▶쇼미더뉴스! 오늘 많이 본 뉴스영상 '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# white space가 2번 이상 나타나는 경우\n",
    "pattern = re.compile(r\"\\s{2,}\")\n",
    "print(pattern.findall(corpus))\n",
    "pattern.sub(\" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern = re.compile(r\"\\b(.+)\\b\")\n",
    "pattern = re.compile(r\"[A-Za-z-_]{8,}\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dkkim@yna.co.kr', '.kr')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이메일 제거\n",
    "pattern = re.compile(r\"(\\w+@[a-zA-Z0-9\\-\\_]{3,}(.[a-zA-Z]{2,})+)\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IATA', 'TA'),\n",
       " ('IATA', 'TA'),\n",
       " ('IATA', 'TA'),\n",
       " ('CEO', 'EO'),\n",
       " ('CEO', 'EO'),\n",
       " ('dkkim@yna.co.kr', '.kr')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url 제거\n",
    "pattern = re.compile(r\"([a-zA-Z]{1,}(.[a-zA-Z]{1,})+)\")\n",
    "pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 이외에 모두 제거\n",
    "pattern = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣]+)\")\n",
    "#pattern.findall(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatternList():\n",
    "    patternList = {}\n",
    "    patternList[\"Korean\"] = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣]+)\")\n",
    "    patternList[\"Email\"] = re.compile(r\"(\\w+@[a-zA-Z0-9\\-\\_]{3,}(.[a-zA-Z]{2,})+)\")\n",
    "    patternList[\"Whitespace\"] = re.compile(r\"\\s{2,}\")\n",
    "    patternList[\"Punctuation\"] =  re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "    return patternList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = news.get_article(\"정치\")\n",
    "corpus = getPatternList()[\"Korean\"].sub(\" \", corpus)\n",
    "corpus = getPatternList()[\"Punctuation\"].sub(\" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kma = Kkma()\n",
    "\n",
    "dictTerm = list()\n",
    "dictPOS = list()\n",
    "dictNoun = list()\n",
    "dictNgram = list()\n",
    "\n",
    "for sentence in sent_tokenize(corpus):\n",
    "    for token in sentence.split():\n",
    "        if len(token) > 1:\n",
    "            dictTerm.append(token)\n",
    "            dictPOS.extend([morpheme for morpheme in kma.morphs(token) if len(morpheme) > 1])\n",
    "            dictNoun.extend([noun for noun in kma.nouns(token) if len(noun) > 1])\n",
    "            dictNgram.extend(ngram.ngramUmjeol(token))\n",
    "        \n",
    "dictTerm = list(set(dictTerm))\n",
    "dictPOS = list(set(dictPOS))\n",
    "dictNoun = list(set(dictNoun))\n",
    "dictNgram = list(set(dictNgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 220, 179, 519)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictTerm), len(dictPOS), len(dictNoun), len(dictNgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(dictTerm + dictPOS + dictNoun+ dictNgram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index (역문헌 구조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    (\"Document1\", \"This is a sample\"),\n",
    "    (\"Document2\", \"This is another sample\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory (Hash Key 값)\n",
    "# {단어1:포스팅위치, 단어2:포스팅위치, ...}   --> 동일한 단어가 여러 번 나온 경우 마지막 나온 globalPosting 위치\n",
    "globalLexicon = dict()\n",
    "# [0:문서1, 1:문서2, ...]\n",
    "globalDocument = list()\n",
    "\n",
    "# Disk\n",
    "# [0:(단어 idx, 문서 idx, 빈도, 다음주소), 1:(단어 idx, 문서 idx, 빈도, 다음주소),, ...]  --> 맨 마지막 postingData에는 다음주소 부분에 \"-1\" 값 할당\n",
    "globalPosting = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 1, 'a': 1, 'sample': 1}\n",
      "{'this': 1, 'is': 1, 'another': 1, 'sample': 1}\n",
      "['Document1', 'Document2']\n"
     ]
    }
   ],
   "source": [
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용으로 docIdx를 만든다. (Key, Document 이름은 절대로 겹치지 않는다는 가정)\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # 로컬 영역\n",
    "    # 파일 새로 생성\n",
    "    # {단어 idx : 빈도, 단어 idx: 빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    # fp -> struct(단어, 빈도) : localPosting\n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in globalLexicon.keys():\n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting)    # fseek\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, -1)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx    # globalPosting 위치(ptr:idx)\n",
    "        else:\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)    # fseek\n",
    "            beforeIdx = globalLexicon[indexTerm]\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, beforeIdx)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx    # globalPosting 위치(ptr:idx)\n",
    "        \n",
    "    print(localPosting)\n",
    "print(globalDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 4, 'is': 5, 'a': 2, 'sample': 7, 'another': 6},\n",
       " ['Document1', 'Document2'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalLexicon, globalDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, -1),\n",
       " (1, 0, 1, -1),\n",
       " (2, 0, 1, -1),\n",
       " (3, 0, 1, -1),\n",
       " (0, 1, 1, 0),\n",
       " (1, 1, 1, 1),\n",
       " (4, 1, 1, -1),\n",
       " (3, 1, 1, 3)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "    Document2    /    1    /    0\n",
      "    Document1    /    1    /    -1\n",
      "is\n",
      "    Document2    /    1    /    1\n",
      "    Document1    /    1    /    -1\n",
      "a\n",
      "    Document1    /    1    /    -1\n",
      "sample\n",
      "    Document2    /    1    /    3\n",
      "    Document1    /    1    /    -1\n",
      "another\n",
      "    Document2    /    1    /    -1\n"
     ]
    }
   ],
   "source": [
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    # indexTerm:단어, postingIdx:위치, ...\n",
    "    print(indexTerm)\n",
    "    \n",
    "    while True:    # Posting Nexting: -1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        print(\"    {0}    /    {1}    /    {2}\".format(globalDocument[postingData[1]], postingData[2], postingData[3]))\n",
    "        postingIdx = postingData[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalLexicon[\"sample\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 1, -1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalPosting[globalLexicon[\"sample\"]][3]]   # 다음 주소가 \"-1\" 일때 까지 반복해서 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Model : TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    (\"Document1\", \"This is a sample\"),\n",
    "    (\"Document2\", \"This is another sample\"),\n",
    "    (\"Document3\", \"This is not sample\")\n",
    "]\n",
    "\n",
    "query = \"this is sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def normTF(freq, totalCount):\n",
    "    return freq / totalCount\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1+log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def maxTF(freq, maxFreq, alpha=0.5):    # alpha(0<alpha<1)와 1 사이의 값으로 정규화 됨 (double normalization K)\n",
    "    return alpha + (1-alpha) * (freq / maxFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1\n",
      "Importance(Weight) in Document\n",
      "1. this rawTF:1\n",
      "2. this normTF:0.25\n",
      "3. this logTF:1.0\n",
      "4. this maxTF:1.0\n",
      "\n",
      "1. is rawTF:1\n",
      "2. is normTF:0.25\n",
      "3. is logTF:1.0\n",
      "4. is maxTF:1.0\n",
      "\n",
      "1. a rawTF:1\n",
      "2. a normTF:0.25\n",
      "3. a logTF:1.0\n",
      "4. a maxTF:1.0\n",
      "\n",
      "1. sample rawTF:1\n",
      "2. sample normTF:0.25\n",
      "3. sample logTF:1.0\n",
      "4. sample maxTF:1.0\n",
      "\n",
      "{'this': 1, 'is': 1, 'a': 1, 'sample': 1}\n",
      "\n",
      "Document2\n",
      "Importance(Weight) in Document\n",
      "1. this rawTF:1\n",
      "2. this normTF:0.25\n",
      "3. this logTF:1.0\n",
      "4. this maxTF:1.0\n",
      "\n",
      "1. is rawTF:1\n",
      "2. is normTF:0.25\n",
      "3. is logTF:1.0\n",
      "4. is maxTF:1.0\n",
      "\n",
      "1. another rawTF:1\n",
      "2. another normTF:0.25\n",
      "3. another logTF:1.0\n",
      "4. another maxTF:1.0\n",
      "\n",
      "1. sample rawTF:1\n",
      "2. sample normTF:0.25\n",
      "3. sample logTF:1.0\n",
      "4. sample maxTF:1.0\n",
      "\n",
      "{'this': 1, 'is': 1, 'another': 1, 'sample': 1}\n",
      "\n",
      "Document3\n",
      "Importance(Weight) in Document\n",
      "1. this rawTF:1\n",
      "2. this normTF:0.25\n",
      "3. this logTF:1.0\n",
      "4. this maxTF:1.0\n",
      "\n",
      "1. is rawTF:1\n",
      "2. is normTF:0.25\n",
      "3. is logTF:1.0\n",
      "4. is maxTF:1.0\n",
      "\n",
      "1. not rawTF:1\n",
      "2. not normTF:0.25\n",
      "3. not logTF:1.0\n",
      "4. not maxTF:1.0\n",
      "\n",
      "1. sample rawTF:1\n",
      "2. sample normTF:0.25\n",
      "3. sample logTF:1.0\n",
      "4. sample maxTF:1.0\n",
      "\n",
      "{'this': 1, 'is': 1, 'not': 1, 'sample': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    localPosting = dict()\n",
    "    maxCount = 0\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        maxCount += 1\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    print(docName)\n",
    "    alpha = 0.5\n",
    "    maxFreq = max(localPosting.values())\n",
    "\n",
    "    print(\"Importance(Weight) in Document\")\n",
    "    for term, freq in localPosting.items():\n",
    "        print(\"1. {0} rawTF:{1}\".format(term, rawTF(freq)))\n",
    "        print(\"2. {0} normTF:{1}\".format(term, normTF(freq, maxCount)))\n",
    "        print(\"3. {0} logTF:{1}\".format(term, logTF(freq)))\n",
    "        print(\"4. {0} maxTF:{1}\".format(term, maxTF(freq, maxFreq, alpha)))\n",
    "        print()\n",
    "    \n",
    "    print(localPosting)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawIDF(df, N):    # 일반적인 IDF\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothigIDF(df, N):    # the, a, 불용어 등 문제\n",
    "    return log10((N+1)/df)\n",
    "\n",
    "def probabilityIDF(df, N):\n",
    "    return log10((N-df+1)/df)    # N-df가 0이 되는 것을 방지하기 위해 1을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance(Weight) in Collection\n",
      "this\n",
      "1. rawIDF:0.0\n",
      "2. smoothingIDF:0.12493873660829993\n",
      "3. probabilityIDF:-0.47712125471966244\n",
      "\n",
      "is\n",
      "1. rawIDF:0.0\n",
      "2. smoothingIDF:0.12493873660829993\n",
      "3. probabilityIDF:-0.47712125471966244\n",
      "\n",
      "a\n",
      "1. rawIDF:0.47712125471966244\n",
      "2. smoothingIDF:0.6020599913279624\n",
      "3. probabilityIDF:0.47712125471966244\n",
      "\n",
      "sample\n",
      "1. rawIDF:0.0\n",
      "2. smoothingIDF:0.12493873660829993\n",
      "3. probabilityIDF:-0.47712125471966244\n",
      "\n",
      "another\n",
      "1. rawIDF:0.47712125471966244\n",
      "2. smoothingIDF:0.6020599913279624\n",
      "3. probabilityIDF:0.47712125471966244\n",
      "\n",
      "not\n",
      "1. rawIDF:0.47712125471966244\n",
      "2. smoothingIDF:0.6020599913279624\n",
      "3. probabilityIDF:0.47712125471966244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = len(collection)\n",
    "\n",
    "print(\"Importance(Weight) in Collection\")\n",
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    # indexTerm:단어, postingIdx:위치, ...\n",
    "    print(indexTerm)\n",
    "    \n",
    "    df = 0\n",
    "    while True:    # Posting Nexting: -1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[postingIdx]\n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    print(\"1. rawIDF:{0}\".format(rawIDF(df, N)))\n",
    "    print(\"2. smoothingIDF:{0}\".format(smoothigIDF(df, N)))\n",
    "    print(\"3. probabilityIDF:{0}\".format(probabilityIDF(df, N)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inverted Index(색인)와 TF-IDF 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    localPosting = dict()\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    maxFreq = max(localPosting.values())\n",
    "    \n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in globalLexicon.keys():\n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting)\n",
    "            postingData = [lexiconIdx, docIdx, maxTF(0, termFreq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx\n",
    "        else:\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[indexTerm]\n",
    "            postingData = [lexiconIdx, docIdx, maxTF(0, termFreq, maxFreq), beforeIdx]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx\n",
    "        \n",
    "    #print(localPosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n",
       " [1, 0, 1.0, -1],\n",
       " [2, 0, 1.0, -1],\n",
       " [3, 0, 1.0, -1],\n",
       " [0, 1, 1.0, 0],\n",
       " [1, 1, 1.0, 1],\n",
       " [4, 1, 1.0, -1],\n",
       " [3, 1, 1.0, 3],\n",
       " [0, 2, 1.0, 4],\n",
       " [1, 2, 1.0, 5],\n",
       " [5, 2, 1.0, -1],\n",
       " [3, 2, 1.0, 7]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this / IDF-0.0\n",
      "    Documents:Document3 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document2 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document1 / TF:1.0 / TF-IDF:0.0\n",
      "is / IDF-0.0\n",
      "    Documents:Document3 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document2 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document1 / TF:1.0 / TF-IDF:0.0\n",
      "a / IDF-0.47712125471966244\n",
      "    Documents:Document1 / TF:1.0 / TF-IDF:0.47712125471966244\n",
      "sample / IDF-0.0\n",
      "    Documents:Document3 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document2 / TF:1.0 / TF-IDF:0.0\n",
      "    Documents:Document1 / TF:1.0 / TF-IDF:0.0\n",
      "another / IDF-0.47712125471966244\n",
      "    Documents:Document2 / TF:1.0 / TF-IDF:0.47712125471966244\n",
      "not / IDF-0.47712125471966244\n",
      "    Documents:Document3 / TF:1.0 / TF-IDF:0.47712125471966244\n"
     ]
    }
   ],
   "source": [
    "N = len(globalDocument)\n",
    "\n",
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    df = 0\n",
    "    oldPostingIdx = postingIdx\n",
    "    \n",
    "    while True:    # Posting Nexting: -1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[postingIdx]\n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    postingIdx = oldPostingIdx\n",
    "    idf = rawIDF(df, N)\n",
    "    #print(indexTerm, str(df))\n",
    "    \n",
    "    print(\"{0} / IDF-{1}\".format(indexTerm, idf))\n",
    "\n",
    "    while True:\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "        \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        TF = postingData[2]\n",
    "        globalPosting[postingIdx][2] = globalPosting[postingIdx][2] * idf\n",
    "        print(\"    Documents:{0} / TF:{1} / TF-IDF:{2}\".format(globalDocument[postingData[1]], \n",
    "                                                               TF, globalPosting[postingIdx][2]))\n",
    "        \n",
    "        postingIdx = postingData[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
