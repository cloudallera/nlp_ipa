{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#토픽-분석(모델링)-:-Latent-Dirichlet-allocation\" data-toc-modified-id=\"토픽-분석(모델링)-:-Latent-Dirichlet-allocation-1\">토픽 분석(모델링) : Latent Dirichlet allocation</a></span><ul class=\"toc-item\"><li><span><a href=\"#LDA-개요(wikipedia)\" data-toc-modified-id=\"LDA-개요(wikipedia)-1.1\">LDA 개요(wikipedia)</a></span></li><li><span><a href=\"#LDA-개요(ratsgo-blog)\" data-toc-modified-id=\"LDA-개요(ratsgo-blog)-1.2\">LDA 개요(ratsgo blog)</a></span><ul class=\"toc-item\"><li><span><a href=\"#LDA-모델-아키텍처\" data-toc-modified-id=\"LDA-모델-아키텍처-1.2.1\">LDA 모델 아키텍처</a></span></li></ul></li><li><span><a href=\"#LDA와-깁스-샘플링\" data-toc-modified-id=\"LDA와-깁스-샘플링-1.3\">LDA와 깁스 샘플링</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 분석(모델링) : Latent Dirichlet allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 개요(wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ko.wikipedia.org/wiki/잠재_디리클레_할당\n",
    "\n",
    "\n",
    "- 자연어 처리에서 잠재 디리클레 할당(Latent Dirichlet allocation, LDA)은 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 서술하는 대한 **확률적 토픽 모델 기법** 중 하나이다. **미리 알고 있는 주제별 단어수 분포를 바탕으로**, 주어진 문서에서 발견된 단어수 분포를 분석함으로써 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측할 수 있다.\n",
    "\n",
    "\n",
    "- LDA는 이산 자료들에 대한 확률적 생성 모형이다. 문자 기반의 자료들에 대해 쓰일 수 있으며 사진 등의 다른 이산 자료들에 대해서도 쓰일 수 있다. 기존의 정보 검색분야에서 LDA와 유사하게 문헌 내의 잠재적인 의미구조를 파악하려는 시도들은 계속 이루어져 왔다. TF-IDF를 필두로 하여 잠재 의미 분석(Latent semantic indexing, LSI), 확률 잠재 의미 분석(Probabilistic latent semantic analysis, pLSA)등을 거쳐 LDA로 도달하게 되었고, 이는 토픽 모델링이라 불리는 분야를 탄생시켰다. 확률 잠재 의미 분석은 확률 잠재 의미 인덱싱(probabilistic latent semantic indexing, pLSI) 라고도 한다.\n",
    "\n",
    "\n",
    "- LDA에는 몇 가지 가정이 있는데 그 중 중요한 것은 **단어의 교환성(exchangeability)** 이다. 이는 '**단어 주머니(bag of words)**'라고 표현하기도 한다. 교환성은 단어들의 순서는 상관하지 않고 오로지 단어들의 유무만이 중요하다는 가정이다. 예를 들어, 'Apple is red'와 'Red is apple' 간에 차이가 없다고 생각하는 것이다. 단어의 순서를 무시할 경우 문헌은 단순히 그 안에 포함되는 **단어들의 빈도수만을 가지고 표현**이 가능하게 된다. 이 가정을 기반으로 단어와 문서들의 교환성을 포함하는 혼합 모형을 제시한 것이 바로 LDA이다. 하지만 단순히 단어 하나를 단위로 생각하는 것이 아니라 특정 단어들의 묶음을 한 단위로 생각하는 방식(n-gram)으로 LDA의 교환성 가정을 확장시킬 수도 있다.\n",
    "\n",
    "\n",
    "- LDA는 문헌의 주제를 찾기 위한 방법으로 고안되었지만, 이미지, 소리 등 텍스트 처리 이외의 다양한 분야에 쓰일 수 있고 이산 자료들, 즉 불연속적인 자료들뿐만 아니라 연속적인 자료들에 대해서 적용 할 수 있고 또한 다항 분포가 아닌 자료들에 대해서도 적용 할 수 있는 가능성이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 개요(ratsgo blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA란 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지에 대한 확률모형입니다. \n",
    "\n",
    "\n",
    "- LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해 냅니다.\n",
    "\n",
    "\n",
    "- LDA의 개략적인 도식은 다음과 같습니다.\n",
    "\n",
    "![LDA](./images/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic은 단어들의 cluster이며, 각 cluster(topic) 내에는 단어들이 어떻게 분포되어 있는지(확률)를 내포하고 있다. <br>\n",
    "   리디클레 분포로 부터 이 확률들이 산출된다. ($\\alpha$는 리디클레 파라미터, $\\beta$는 topic hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 모델 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- D는 말뭉치 전체 문서 개수, K는 전체 토픽 수(하이퍼 파라메터), N은 d번째 문서의 단어 수를 의미합니다. \n",
    "\n",
    "\n",
    "- 네모칸은 해당 횟수만큼 반복하라는 의미이며 동그라미는 변수를 가리킵니다. 화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당하는 변수입니다.\n",
    "\n",
    "\n",
    "- $\\large \\alpha \\rightarrow \\theta_d$ : M개의 문서에 대한 문서-토픽 분포 (per-document topic proportions)\n",
    "\n",
    "\n",
    "- $\\large \\beta \\rightarrow \\phi_k$ : N개의 word에 대한 word-topic 분포 (per-corpus topic distributions)\n",
    "\n",
    "\n",
    "우리가 관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 $w_{d,n}$이 유일합니다(음영 표시).\n",
    "\n",
    "이 정보만을 가지고 하이퍼파라메터(사용자 지정) α, β를 제외한 모든 잠재 변수를 추정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Latent_Dirichlet_allocation](./images/Latent_Dirichlet_allocation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA와 깁스 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 유도 과정은 평활화(smoothed)된 LDA에 대해  $\\varphi$와 $\\theta$를 소거하는 붕괴 기브스 표집을 사용한다. \n",
    "\n",
    "유도 과정을 간략화하기 위해 모든 문서의 길이가 $N$으로 같다고 가정한다. 물론 아래의 유도는 문서의 길이가 서로 달라도 유효하다.\n",
    "\n",
    "모형에 대한 전체 결합 분포는 다음과 같다. (토픽, 문서, 단어 전체에 대해 확율을 누적)\n",
    "\n",
    "$P(\\mathbf{w}, \\mathbf{z}, \\theta,\n",
    "\\mathbf{\\varphi} | \\alpha,\\beta) = \\prod_{i=1}^k\n",
    "P(\\varphi_i | \\beta) \\prod_{d=1}^M P(\\theta_d | \\alpha) \\prod_{n=1}^N\n",
    "P(z_{dn} | \\theta_d)P(w_{dn} | \\varphi_{z_{dn}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\varphi$와 $\\theta$를 모두 더하여 주변 분포를 구할 수 있다. ($\\varphi$와 $\\theta$의 확률을 무시하고 계산할 수 있도록 함)\n",
    "\n",
    "$\\Large {\\begin{aligned}&P({\\mathbf  {z}},{\\mathbf  {w}}|\\alpha ,\\beta )=\\int _{{\\theta }}\\int _{{{\\mathbf  {\\varphi }}}}P({\\mathbf  {w}},{\\mathbf  {z}},\\theta ,{\\mathbf  {\\varphi }}|\\alpha ,\\beta )\\,d{\\mathbf  {\\varphi }}\\,d\\theta \\\\=&\\int _{{{\\mathbf  {\\varphi }}}}\\prod _{{i=1}}^{k}P(\\varphi _{i}|\\beta )\\prod _{{d=1}}^{M}\\prod _{{n=1}}^{N}P(w_{{dn}}|\\varphi _{{z_{{dn}}}})\\,d{\\mathbf  {\\varphi }} \\, \\times \\, \\int _{{\\theta }}\\prod _{{d=1}}^{M}P(\\theta _{d}|\\alpha )\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})\\,d\\theta .\\end{aligned}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "모든 $\\theta$는 $\\varphi$에 대해 독립적이다. 따라서 $\\theta$와 $\\varphi$를 분리할 수 있다. \n",
    "\n",
    "우선 $\\theta$가 포함된 식을 정리하면,\n",
    "\n",
    "$\\int _{{\\theta }}\\prod _{{d=1}}^{M}P(\\theta _{d}|\\alpha )\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})d\\theta =\\prod _{{d=1}}^{M}\\int _{{\\theta _{d}}}P(\\theta _{d}|\\alpha )\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})\\,d\\theta _{d}$\n",
    "\n",
    "\n",
    "이 때 각각의 $\\theta _{d}$에 대해 식을 풀어 쓰면 다음과 같다.\n",
    "\n",
    "${\\begin{aligned}&\\int _{{\\theta _{d}}}P(\\theta _{d}|\\alpha )\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})\\,d\\theta _{d}=&\\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{\\alpha _{i}-1}}\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})\\,d\\theta _{d}.\\end{aligned}}$\n",
    "\n",
    "이제 $\\large c_{{dv}}^{i}$를 $d$번째 문서의 단어 중 단어집의 $v$번째 단어이면서 $i$번째 주제에 포함된 것의 개수라 정의하자. <br>\n",
    "그러면 $c$는 3차원 행렬로 정의될 수 있다. 이 때 값이 정확하게 정해지지 않은 성분은 $(\\cdot )$으로 표기하기로 한다. <br>\n",
    "가령 $\\large c_{{d,(\\cdot )}}^{i}$은 $d$번째 문서의 단어 중 $i$번째 주제에 포함된 것의 개수를 나타낸다. \n",
    "\n",
    "$c$을 이용하면 위의 식의 마지막 항은 다음과 같이 다시 쓰일 수 있다. (모든 문서의 토픽을 찾아 곱하는 과정을 단순화 할 수 있다.)\n",
    "\n",
    "$\\Large \\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})=\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}}}$\n",
    "\n",
    "마찬가지로 식 전체를 다시 쓰면,\n",
    "\n",
    "${\\begin{aligned}&\\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{\\alpha _{i}-1}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}}}\\,d\\theta _{d}\\\\=&\\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}+\\alpha _{i}-1}}\\,d\\theta _{d}.\\end{aligned}}$\n",
    "\n",
    "위 식에서 적분하는 식은 디리클레 분포 확률 밀도 함수와 같다. 따라서 확률 밀도 함수의 정의에 의해 다음을 만족한다.\n",
    "\n",
    "$\\Large \\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}+\\alpha _{i}-1}}\\,d\\theta _{d}=1$\n",
    "\n",
    "따라서 처음의 주변 분포에서 $\\theta$가 포함된 식은 다음과 같이 정리된다.\n",
    "\n",
    "${\\begin{aligned}&\\int _{{\\theta _{d}}}P(\\theta _{d}|\\alpha )\\prod _{{n=1}}^{N}P(z_{{dn}}|\\theta _{d})\\,d\\theta _{d}\\\\&=\\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}+\\alpha _{i}-1}}\\,d\\theta _{d}\\\\=&{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}{\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}}\\int _{{\\theta _{d}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}}\\prod _{{i=1}}^{k}\\theta _{{di}}^{{c_{{d,(\\cdot )}}^{i}+\\alpha _{i}-1}}\\,d\\theta _{d}\\\\=&{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}{\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}} . \\end{aligned}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "이제 주변 분포에서 ${\\mathbf  {\\varphi }}$가 포함된 식을 정리하자. 이 식은 $\\theta$ 과 포함된 식과 유사한 방법에 의해 정리될 수 있다.\n",
    "\n",
    "${\\begin{aligned}&\\int _{{{\\mathbf  {\\varphi }}}}\\prod _{{i=1}}^{k}P(\\varphi _{i}|\\beta )\\prod _{{d=1}}^{M}\\prod _{{n=1}}^{N}P(w_{{dn}}|\\varphi _{{z_{{dn}}}})\\,d{\\mathbf  {\\varphi }}\\\\=&\\prod _{{i=1}}^{k}\\int _{{\\varphi _{i}}}P(\\varphi _{i}|\\beta )\\prod _{{d=1}}^{M}\\prod _{{n=1}}^{N}P(w_{{dn}}|\\varphi _{{z_{{dn}}}})\\,d\\varphi _{i}\\\\=&\\prod _{{i=1}}^{k}\\int _{{\\varphi _{i}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}\\beta _{r}{\\bigr )}}{\\prod _{{r=1}}^{V}\\Gamma (\\beta _{r})}}\\prod _{{r=1}}^{V}\\varphi _{{ir}}^{{\\beta _{r}-1}}\\prod _{{r=1}}^{V}\\varphi _{{ir}}^{{c_{{(\\cdot ),r}}^{i}}}\\,d\\varphi _{i}\\\\=&\\prod _{{i=1}}^{k}\\int _{{\\varphi _{i}}}{\\frac  {\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}\\beta _{r}{\\bigr )}}{\\prod _{{r=1}}^{V}\\Gamma (\\beta _{r})}}\\prod _{{r=1}}^{V}\\varphi _{{ir}}^{{c_{{(\\cdot ),r}}^{i}+\\beta _{r}-1}}\\,d\\varphi _{i}\\\\=&\\prod _{{i=1}}^{k}{\\frac  {\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}\\beta _{r}{\\bigr )}}{\\prod _{{r=1}}^{V}\\Gamma (\\beta _{r})}}{\\frac  {\\prod _{{r=1}}^{V}\\Gamma (c_{{(\\cdot ),r}}^{i}+\\beta _{r})}{\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}c_{{(\\cdot ),r}}^{i}+\\beta _{r}{\\bigr )}}}.\\end{aligned}}$\n",
    "\n",
    "- $V$는 전체 문서에 있는 unique 한 word의 수 (Vocabulary 수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "위의 결과들을 종합하여 $P({\\mathbf  {z}},{\\mathbf  {w}}|\\alpha ,\\beta )$를 정리하면 다음과 같다.\n",
    "\n",
    "$\\Large {\\begin{aligned}&P({\\mathbf  {z}},{\\mathbf  {w}}|\\alpha ,\\beta )\\\\=&\\prod _{{d=1}}^{M}{\\frac  {\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}\\alpha _{i}{\\bigr )}}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}{\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}}\\times \\prod _{{i=1}}^{k}{\\frac  {\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}\\beta _{r}{\\bigr )}}{\\prod _{{r=1}}^{V}\\Gamma (\\beta _{r})}}{\\frac  {\\prod _{{r=1}}^{V}\\Gamma (c_{{(\\cdot ),r}}^{i}+\\beta _{r})}{\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}c_{{(\\cdot ),r}}^{i}+\\beta _{r}{\\bigr )}}} \\end{aligned}}$\n",
    "\n",
    "기브스 표집의 목표는 $P({\\mathbf  {z}}|{\\mathbf  {w}},\\alpha ,\\beta)$의 분포를 근사(approximate)하는 것이다. 이를 위해 다음과 같은 식을 사용하도록 한다.\n",
    "\n",
    "$\\large P(z_{{pq}}|{\\mathbf  {z_{{-pq}}}},{\\mathbf  {w}},\\alpha ,\\beta )={\\frac  {P(z_{{pq}},{\\mathbf  {z_{{-pq}}}},{\\mathbf  {w}},\\alpha ,\\beta )}{P({\\mathbf  {z_{{-pq}}}},{\\mathbf  {w}},\\alpha ,\\beta )}}$\n",
    "\n",
    "여기서부터 $z_{{pq}}$를 나타낼 때 $p$번째 문서의 $q$번째 단어는 단어집에서 $v$의 인덱스를 가진다고 가정한다. <br>\n",
    "또한 ${\\mathbf  {z_{{-pq}}}}$는 $z_{{pq}}$를 제외한 모든 $z$를 의미한다. \n",
    "\n",
    "이 때 기브스 표집은 $z_{{pq}}$만을 고려하기 때문에 위의 식의 정확한 값은 필요가 없으며, <br>\n",
    "모든 $z$에 대한 $z_{{pq}}$의 비율만 구하면 된다. 따라서 위의 식을 다음과 같이 정리할 수 있다.\n",
    "\n",
    "${\\begin{aligned}&P(z_{{pq}}=a|{\\mathbf  {z_{{-pq}}}},{\\mathbf  {w}},\\alpha ,\\beta )\\\\\\propto &P(z_{{pq}}=a,{\\mathbf  {z_{{-pq}}}},{\\mathbf  {w}}|\\alpha ,\\beta )\\\\=&\\left({\\frac  {\\Gamma \\left(\\sum _{{i=1}}^{k}\\alpha _{i}\\right)}{\\prod _{{i=1}}^{k}\\Gamma (\\alpha _{i})}}\\right)^{M}\\prod _{{j\\neq m}}{\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{d,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{d,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}}\\\\&\\times \\left({\\frac  {\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}\\beta _{r}{\\bigr )}}{\\prod _{{r=1}}^{V}\\Gamma (\\beta _{r})}}\\right)^{k}\\prod _{{i=1}}^{k}\\prod _{{r\\neq v}}\\Gamma (c_{{(\\cdot ),r}}^{i}+\\beta _{r})\\\\&\\times {\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{p,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{p,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}}\\prod _{{i=1}}^{k}{\\frac  {\\Gamma (c_{{(\\cdot ),v}}^{i}+\\beta _{v})}{\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}c_{{(\\cdot ),r}}^{i}+\\beta _{r}{\\bigr )}}}\\\\\\propto &{\\frac  {\\prod _{{i=1}}^{k}\\Gamma (c_{{p,(\\cdot )}}^{i}+\\alpha _{i})}{\\Gamma {\\bigl (}\\sum _{{i=1}}^{k}c_{{p,(\\cdot )}}^{i}+\\alpha _{i}{\\bigr )}}}\\prod _{{i=1}}^{k}{\\frac  {\\Gamma (c_{{(\\cdot ),v}}^{i}+\\beta _{v})}{\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}c_{{(\\cdot ),r}}^{i}+\\beta _{r}{\\bigr )}}}\\\\\\propto \\LARGE &\\prod _{{i=1}}^{k}\\Gamma (c_{{p,(\\cdot )}}^{i}+\\alpha _{i})\\prod _{{i=1}}^{k}{\\frac  {\\Gamma (c_{{(\\cdot ),v}}^{i}+\\beta _{v})}{\\Gamma {\\bigl (}\\sum _{{r=1}}^{V}c_{{(\\cdot ),r}}^{i}+\\beta _{r}{\\bigr )}}} . \\end{aligned}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![lda_gibbs_sampling.png](./images/lda_gibbs_sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
